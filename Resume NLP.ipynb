{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "20002b02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy\n",
    "import fitz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "244ff8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extracting files from directory into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "d6eca090",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [\"\"]*50\n",
    "count  = 0\n",
    "for dirname, _, filenames in os.walk(r'C:\\Users\\kshit\\Documents\\NLP\\Resumes'):\n",
    "    for filename in filenames:\n",
    "            fname = os.path.join(dirname, filename)\n",
    "            doc= fitz.open(fname)\n",
    "            a[count]=\"\"\n",
    "            for page in doc:\n",
    "              a[count] = a[count] + str(page.get_text())\n",
    "            count+=1           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "84a869be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CHARU BANSAL \\n \\n \\n \\n \\n \\n \\n \\n            5th Year Undergraduate \\nMb: +91-7080024445                                          Department of Mathematics & Scientific Computing, with Minor in Machine Learning and Applications \\nEmail: charub@iitk.ac.in \\n \\n \\n \\n \\n \\n             \\n    Indian Institute of Technology Kanpur \\nTechnical Skills \\n\\uf0b7 Programming languages  \\nC/C++, HTML/CSS, R, Python, MATLAB \\n\\uf0b7 Operating System  \\nWindows, Linux \\nEducational Qualifications \\n \\nYear \\nDegree/Board \\nInstitute/School \\nCGPA/% \\n2018-2019 \\nM.S. (MTH) \\nIIT Kanpur \\n8.0* \\n2014-2018 \\nB.S. (MTH) \\nIIT Kanpur \\n6.9 \\n2014 \\nAISSCE (CBSE XIIth Board) \\nJai Academy, Jhansi \\n92.8 \\n2012 \\nAISSE (CBSE Xth Board) \\nJai Academy, Jhansi \\n10.0 \\n*CGPA at the end of 8th Semester \\nExperience \\nSummer Intern @Accenture Digital (Project on Fraud Detection in Procurement Analysis) \\n \\n \\n      \\n          [May’18-Jul’18] \\n\\uf0b7 Performed Outlier Detection on Procurement Data using unsupervised k-NN density approach, Local Outlier Factor, k means Clustering and one \\nclass SVM in R. \\n\\uf0b7 Built a User Interface in R Shiny, to integrate all the processes for the user. The platform provides the user with the option of selecting and filtering \\ndata, choosing the model and its hyperparameters, and view interactive plots of the required variables. \\n\\uf0b7 Tested the program on six specific use cases of fraud, for comparing models and analyzing their effectiveness. \\nProbabilistic Machine Learning Project on ‘Zero-Shot Learning’ \\n \\n \\n \\n \\n \\n    \\n        [Aug’17-Nov’17] \\nProject under Prof. Piyush Rai, Computer Science Department, IIT Kanpur \\n\\uf0b7 Predicted the distribution parameters of unseen classes using regression on distribution parameters of seen classes. \\n\\uf0b7 Employed and compared Multivariate Regression Tree and Gaussian Process Regression for prediction. \\n\\uf0b7 Attempted Generalised Zero Shot Learning by sampling data from the predicted distribution of unseen classes. \\nNatural Language Processing Project on ‘Sentiment Analysis on Tweets’ \\n \\n \\n \\n \\n      \\n          [Jan’18-Apr’18] \\nProject under Prof. Harish Karnick, Computer Science Department, IIT Kanpur \\n\\uf0b7 Analyzed and classified sentiments of tweets into positive and negative. \\n\\uf0b7 Used the BBoW, tf, tfidf, Word2Vec, & GLoVE representations for feature extraction, after their tokenization and pre-processing. \\n\\uf0b7 Classified the tweets using Naïve Bayes, Logistic Regression, SVM, Feed Forward Neural Net (Multi-Layer Perceptron) and LSTM. \\nTime Series Analysis Project on ‘GDP Forecasting’  \\n \\n \\n \\n \\n \\n \\n \\n        [Aug’17-Nov’17] \\nProject under Prof. Amit Mitra, Mathematics and Statistics Department, IIT Kanpur \\n\\uf0b7 Analyzed for trend, seasonality, stationarity, and predicted GDP based on its time series, in R \\n\\uf0b7 Implemented the ARIMA model, Holt-Winters Smoothing, Augmented Dickey-Fuller, KPSS, & Ljung Box tests for further analysis. \\nRegression Analysis Project on ‘Statistical Modelling of Housing Prices’ \\n \\n \\n \\n \\n       \\n          [Jan’17-Apr’17] \\nProject under Prof. Sharmishtha Mitra, Mathematics and Statistics Department, IIT Kanpur \\n\\uf0b7 Designed a consistent Linear Multiple Regression Model to predict Housing Prices; employing a series of steps; fitting the usual Ordinary Least \\nSquares model, residual analysis, checking multicollinearity and variable selection; for the process. \\nStatistical Simulation and Data Analysis Project on ‘Identifying Authenticity of Currency Notes’  \\n \\n                            [Jan’18-Apr’18] \\nProject under Prof. Debasis Kundu, Mathematics and Statistics Department, IIT Kanpur \\n\\uf0b7 Modelled the Swiss bank data set on real and fake currencies with a two component Gaussian Mixture Model using Expectation Maximization \\nalgorithm, employed AIC/BIC scores to decide the shape and correlation structure of the clusters. \\n\\uf0b7 Checked and dismissed the requirement of k means parameter initialization and soft clustering classification. \\nMachine Learning Project on ‘E-mail Spam Filtering’ \\n \\n \\n \\n \\n \\n \\n \\n        [Aug’16-Nov’16] \\nProject under Prof. Piyush Rai, Computer Science Department, IIT Kanpur \\n\\uf0b7 Explored different classifiers for spam filtering and compared the results obtained to get a fair and comparative idea about the accuracy of various \\nlearning algorithms. \\n \\nScholastic Achievements \\n\\uf0b7 Secured AIR-1496 in JEE Advanced-2014 out of the top 150,000 applicants selected in JEE Mains 2014. \\n\\uf0b7 Secured AIR- 838 in GATE-2018 in Mathematics. \\n\\uf0b7 Selected among the top 1500 students qualified for the final interview round of NTSE 2010, out of the 300,000 applicants. \\n \\nRelevant Courses  \\n● Data Structure and Algorithm \\n● Machine Learning Techniques \\n● Probabilistic Machine Learning \\n  ● Regression Analysis \\n● Applied Stochastic Processes \\n● Applied Game Theory \\n \\n● Probability and Statistics \\n \\n  ● Inference \\n● Time Series Analysis \\n \\n● Natural Language Processing \\n● Statistical Simulation & Data Analysis   ● Data Mining* \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n*On-going Courses \\nPositions of Responsibility \\n● President, BloodConnect, Kanpur \\n \\n● Coordinator, Fine Arts Club, IITK \\n \\n● Student coordinator, Raktarpan, NSS \\n \\nExtra-Curricular Activities \\n\\uf0b7 Mentored 4 underprivileged students of class IX under the program NSS. \\n\\uf0b7 Was part of team of 12 members, who organized and went for a week-long trek to the Chandrashila peak (Uttarakhand) and back. \\n\\uf0b7 Arranged donation for underprivileged kids as social initiative by Inter IIT Sports Meet’16. \\n\\uf0b7 Performed and mentored Sand Art performance during various institute functions like Freshers’ night and Suicide Prevention Day. \\n',\n",
       " 'Ganti Sri Naga Sai Ajay Kamal\\najayganti3@gmail.com | +91 958-111-3007 | D.No 16-5-5, Sri Sai Surya Apartments, T-4, Sri Ram Nagar,\\nRajahmundry, Andhra Pradesh - 533105 | linkedin.com/in/ganti-ajay/ | github.com/ajayganti3\\nSUMMARY\\n• Experienced IT professional with 2.4 years of overall experience\\n• Experience in Web Application Development, Machine Learning and writing automation scripts for automating web\\napplications\\nEXPERIENCE\\nAssociate, Cognizant Technology Solutions\\nSep 2018 - Present\\n• Working for CVS client in developing and automating applications.\\n• Experience in developing Machine Learning model and creating API for the same to make it integrable with the\\nASP.NET web application\\n• Experience in developing Web Application with ASP.NET\\n• Experienced in each stage of software development life cycle Designing, Development, Testing and Documentation\\n• Experience in writing automation scripts for automating web application using selenium framework\\n• Able to deliver projects within deadlines with high quality\\n• Quick Learner, Good Team Player and have the ability to work independently\\nPROJECTS\\nNLP Classiﬁcation - CVS\\n• Classiﬁed contents / comments into speciﬁc categories using supervised machine learning techniques\\n• Used pre-processing techniques to remove unwanted combinations / words from data. Stop word removal, word tok-\\nenization and lemmatization we used to avoid data redundancy.\\n• Developed an API using Flask and integrated it with the existing web application\\n• Technology: Python, NLTK, NLP Preprocessing, Sci-Kit Learn, XGBoost, Flask\\nCassava Leaf Disease Classiﬁcation - Kaggle\\n• Developed a Deep Learning model (CNN) using Tensorﬂow and Keras for classiﬁying the images.\\n• Performed Cross Validation and achieved accuracy of 88.2%\\n• Technology: Deep Learning, Python, Tensorﬂow, Keras\\nMicrosoft Malware Detection\\n• Analysed and performed Feature Engineering on ASM ﬁles and byte ﬁles for identifying diﬀerent type of malware.\\n• Applied diﬀerent types of Machine Learning algorithms along with Grid Search and Random Search for hyperparameter\\noptimization and achieved log loss of 0.012\\n• Technology: Python, Sci-Kit Learn, XGBoost\\nScania Trucks APS Failure Prediction\\n• Performed Exploratory Data Analysis, Visualization, Feature Engineeting techniques\\n• Experimented with diﬀerent types of Machine Learning Classiﬁcation algorithms like Decision Tree, Random Forest,\\nSVM, Gradient Boost and Ensemble models like Stacking Classiﬁer on the data.\\n• Technology: Python, Sci-Kit Learn, XGBoost, SVM, Random Forest\\nEDUCATION\\nBachelor of Technology, Electronics and Communication\\nSep 2014 - Apr 2018\\nMVGR College of Engineering\\nPercentage: 79.9\\nSKILLS\\nProgramming Languages: Python, C#, Java\\nDatabase: MS SQL Server, My SQL\\nProject Management: TFS, GIT\\nFrameworks and Libraries: Pandas, Numpy, Sci-kit Learn, Tensorﬂow, Keras, Flask, Streamlit and PyTorch\\nOperating Systems: Windows, Linux\\nCERTIFICATIONS\\n• Applied AI Course\\n• Data Analysis, Visualization and Machine Learning from IBM Cognitive Class\\nPUBLICATIONS\\n• Published a paper in IEEE with title \"Interfacing of Matlab with arduino for face detection and Tracking Algorithm\\nusing serial communication\" with DOI: 10.1109/ICICI.2017.8365276\\n',\n",
       " 'Ajinkya Takawale\\n1-5-10 506, Sailor Komatsugawa, Edogawa-ku, Tokyo 132-0034\\n� ajinkya.takawale97@gmail.com\\n� ajinkyaT\\n� ajinkyaT\\nEducation\\n○ Indian Institute of Technology (IIT) Kharagpur\\n2015-2019\\nB.Tech. (Hons.), Department of Mechanical Engineering\\nThesis on Machine Learning applications in rural healthcare\\nTechnical Skills\\n○ Languages: Python, JavaScript, Java\\n○ Machine Learning: Scikit-learn, PyTorch, Tensorﬂow, Pandas, Numpy\\n○ Web Stack: React JS, Spring Boot, MySQL, Docker\\n○ AWS Services: Lambda, ECS, ECR, Batch, Step Functions, CloudWatch\\nWork Experience\\nMachine Learning Engineer - BizReach\\nTokyo, Japan\\nRecommendation Engines | ML infrastructure\\nOct 2019–Present\\n○ Identiﬁed target users for premium membership using BigQuery action logs and LightGBM. Improved f1 score by 15%\\n○ Designed and developed chatbot for internal use using Dialogﬂow and backend in python and hosted on GCP\\n○ Created an API endpoint using trained Deep Learning PyTorch model using SageMaker and AI-Sagify\\n○ Used interleaving as a better alternative to AB testing using AWS Batch, Step Functions, ECR\\n○ Created Spot instance management script using Step Functions for daily Sagemaker Jobs reducing cost up to 90%\\nInternships\\nEngineering Intern - Airbus India\\nBangalore, India\\nNatural Language Processing | Chatbot Development\\nMay 2018–July 2018\\n○ Developed an FAQ answering chatbot using open source chatbot framework to address customer care requirements\\n○ Worked on NLP components as intent classiﬁcation, named entity recognition and dialogue ﬂow management\\n○ Added a GUI dashboard for training data visualization and easy addition, editing and labelling of training data\\nMachine Learning Intern - Rorodata\\nHyderabad, India\\nComputer Vision | Deep Learning\\nDec 2017–Jan 2018\\n○ Developed Deep Learning models in the ﬁeld of Biomedical Image diagnosis such as segmentation and classiﬁcation\\n○ Implemented paper, \"Learning to Count Objects in Images\" by VGG group to count number of cells in an image by CNN\\n○ Utilized Gradient-weighted Class Activation Mapping (Grad-CAM) to highlight regions in the image for a prediction\\nSoftware Engineering Intern - Harbinger Systems\\nPune, India\\nNatural Language Processing | Software Development\\nJune 2017–July 2017\\n○ Studied Natural Language Processing and Information Retrieval methods for retrieving similar documents\\n○ Transformed text into tf-idf matrix followed by using cosine similarity to measure similarity of the matching documents\\n○ Surveyed the then existing chatbot as a service alternatives in the market for the task\\nIndustrial Training - Tata Motors\\nPune, India\\nAssembly Line Planning | Fish-bone Analysis | Engine Manufacturing\\nMay 2017–June 2017\\n○ Studied square pattern engine assembly line with the aim of reducing the number of engine rework done\\n○ Assisted in gathering data regarding cycle time, number of engine failures at cold and hot bed testing\\n○ Increased eﬃciency by 10% by analyzing and tracking key issues causing engine failure at testing\\nProjects\\nRunBook - Social media website | BizReach Training\\n○ Developed a complete website with front-end in ReactJS and MVC back-end in Spring framework and deployed on AWS\\nGoogle Summer of Code 2018 | Open Source Development | Audio Visual Speech Recognition\\n○ Developed a part of speech recognition system using both audio and lip video in scenario’s where audio is corrupted\\nSmart India Hackathon 2018 | Indian Space Research Organization (ISRO) | Deep Learning and Computer Vision\\n○ Led a team to ﬁnals to develop Content-Based Image Retrieval System (CBIR) for satellite images using Deep Learning\\nUltimate Student Hunt 2017 | Machine Learning Competition | Time Series Forecasting\\n○ Ranked 2nd in a competition to predict the footfall in the future given only limited past hourly timestamp data\\nExtra Curricular\\n• Qualiﬁed Indian National Chemistry Olympiad (INChO) ranked within top 1% out of 30,000 in the country\\n• Ministry of Science and Technology INSPIRE Scholar • Kishore Vaigyanik Protsahana Yojana (KVPY) Scholar\\n',\n",
       " 'Anant Mundra \\nPre-final year student \\nB.Tech (information technology (8.23 cgpa) \\nIndian Institute of Information Technology, UNA (IIIT Una) India\\nEDUCATION \\nSENIOR SECONDARY SCHOOL – [2016-2018] \\nMaheshwari Public School, Kota (79.8%, CBSE) \\nSECONDARY SCHOOL – [2014-2016] \\nThe Aditya Birla Public School, Chittorgarh (9.6 cgpa , CBSE) \\nResearch Intern at ALPHABETA INC  - A deep financial technology firm using  \\nVisualization, AI, DLT and Edge Computing.  \\nJune 2020-Aug 2020 (Certificate)  \\n• Worked on a set of problems faced by migrant labour in India during CoVID-19 lockdowns. \\nMy role was to build data driven models for most effective ways to: \\n• Disburse govt help to migrant labours, and  \\n• Bring them back to their home states, safely. \\n• Worked on to build a data driven model for govt incentives in commercial lending to MSME \\nstarting with stressed sectors facing CoVID-19 induced problems. \\n• Designing an appropriate model on raising capital for OLA keeping in mind the CoVID-19 \\nimpact on business. \\nMachine Learning – Teaching Assistant at ALPHABETA INC on Cocalc .  \\nSept 2020-Nov 2020 \\n• Tasked to build evaluation model including assignments, interactive quizzes for FinTech \\ncourses, to be delivered to Engineering and Management students at a leading university. \\n• Updated the CoCalc environment with relevant processes for evaluation \\nLEARNING & ACHIEVEMENT: \\n• Part of technical team at ALPHABETA, I worked on building multiple machine learning \\nmodels, doing various data analysis.\\nLANGUAGES\\nENGLISH\\nHINDI\\nCOURSES COMPLETED \\n• Joy Of Computing Using Python (NPTEL) \\nAchieved Silver Medal with 87.5% \\n• Complete Data Science Boot-Camp (UDEMY) \\nComplete Data Science Training: Mathematics, Statistics, Python, Advanced Statistics in Python,  Machine \\nand Deep Learning. \\n• Introduction to Investing and Portfolio Management (ALPHABETA) \\nBasics of stock market, looking at risk-return relationship, exploring asset classes, passive and active \\ninvesting using ETFs, and Index Funds.\\nACTIVITIES & AWARDS\\n• PARTICIPATED IN THE IIIT-JABALPUR \\n(GUSTO’20) SPORTS-FIESTA TABLE-\\nTENNIS TOURNAMENT. \\n• PARTICIPATED IN THE CBSE CLUSTER \\nWEST ZONE HANDBALL TOURNAMENT. \\n• IN MY FREE TIME I TAKE OUT TIME FOR \\nSPORTS (SWIMMING) AND ENJOY \\nWATCHING THRILLER/CRIME SERIALS.\\nSKILLS SUMMARY\\nCOMPETITIVE PROGRAMMER\\nPYTHON PROGRAMMING \\nLANGUAGE\\nDATA SCIENCE   \\nPYTHON | TABLEAU  \\nHTML/CSS\\nSUPERVISED ML\\nUNSUPERVISED ML\\nDEEP LEARNING\\nC/C++ LANGUAGE\\nanantmundra11@gmail.com\\n+91 63781.33616\\nCONTACT\\nWORK EXPERIENCE  \\nResearch Intern at  IIT MANDI \\nJan 2021 – ongoing (6 months) \\nROLE:  \\n•\\nPart of a team, working on the growth of plants. Building data driven models to \\nanalyse different factors affecting the growth.  \\n•\\nBuilding various regression models for weather forecasting. \\n•\\nUni-variate & Multi-variate time series analysis. (Prophet, ARIMA, LSTM) \\n•\\nCorrelating local weather data to the global dataset, with the actuals.\\n',\n",
       " 'ANURAG GUPTA\\nData Scientist\\nData scientist with experience in solving many real-world business problems across diﬀerent domains such as Retail, Banking and Financial sectors by\\nexecuting data-driven solutions.\\nanuragiitr9ag@gmail.com\\n8979546574\\nGurgaon, India\\nlinkedin.com/in/anuraggupta22\\nWORK EXPERIENCE\\nAssociate, Analytics\\nZenon\\n01/2019 - Present, \\nGurgaon, India\\nSenior Business Analyst, Analytics (01/2019 - 12/2020)\\nFor a Fortune 500 BFSI Firm, developed and implemented Machine Learning-\\nbased models such as XGBoost, GBM, and Ensembling techniques to enable the\\ndelivery of eﬀective and eﬃcient campaigns that enable the client to target the\\nappropriate audience within an actionable time frame.\\nIndustry Classiﬁcation Model and Market Basket Analysis for a leading ﬁnancial\\nservices company, Leveraged Natural Language Processing(NLP) assisted model in\\nclassifying target companies which were a critical input for the sales & marketing\\nteam to develop tailored strategies for each industry.\\nWorked on-site and closely with the client leadership teams apart from model\\ndevelopment by providing a commercial and economic interpretation of model\\ninsights that contributes to the overall strategic decision-making process.\\nWorked on developing the Economic Stress Score product for Businesses and\\nCustomers to predict the potential risk in the coming future.\\nInsights Library Project for a Healthcare Firm, Modularized capability to extract\\ninsights and present relevant, and personalized insights to end-user using\\nAdvanced ML.\\nAnalytics Specialist\\nOpera Solutions\\n09/2018 - 01/2019, \\nGurgaon, India\\nGross Sales Model for Prospect Targeting(Fortune 500 BFSI Firm) (09/2018 -\\n11/2018)\\nData Scientist\\nImpact Analytics Pvt. Ltd\\n06/2017 - 08/2018, \\nBangalore, India\\nPromo Eﬀectiveness for Large Scale Oﬃce Supplier (08/2017 - 11/2017)\\nMeasured eﬀectiveness of historical promotions against the baseline to identify\\ntoxic promotions and predicted their performance for the future by\\nDeveloping multi-linear regression to predict baseline sales, Various indirect\\neﬀects such as aﬃnity, halo, cannibalization, and pull-forward were calculated to\\ncapture promotion impact in a holistic manner\\nKey-value categories and Key-value items analysis to improve price perception\\nfor Fast Fashion Retailer(12/2017 – 05/2018) - Performance metrics and statistical\\nmethods used to identify KVCs and KVIs. Created a price elasticity model and\\nperformed aﬃnity analysis to be used as the key metric.\\nStrong understanding of advanced tableau features including calculated ﬁelds,\\nparameters, table calculations, join, dashboard action buttons, context ﬁlters and\\ndata blending. Worked on Promotion Management tool for promo optimization for\\nRetail client\\nEDUCATION\\nIDD B. Tech. (Chemical Engineering) and M. Tech.\\n(Hydrocarbon Engineering)\\nIIT Roorkee\\n2012 - 2017, \\nRoorkee, India\\nSKILLS\\nMachine Learning, Deep Learning, Statistics ,NLP,\\nData Visualization\\nPython, Tensorﬂow, Keras, R, SQL, GoogleBigQuery,\\nHive, Tableau\\nNeural Networks/CNN/RNN/Image Classiﬁcation\\nRegression,Decision Trees/Bagging/Boosting\\nPERSONAL PROJECTS\\nDeep Neural Network using Tensorﬂow\\n1. Build the 4-layer deep neural net to achieve the accuracy of 80%\\non the test dataset in classifying the cat images.2. Build a deep\\nneural network model to recognize numbers from 0 to 5 in sign\\nlanguage on the SIGNS dataset with 80% accuracy.\\nCNN Applications using Keras\\n1. Emotion Detection :Detect if someone\\'s emotion is classiﬁed as\\n\"happy\" or \"not happy.\"| 2. Objection detection using YOLO\\nalogorithm\\nTransfer Learning Applications using FaceNet\\n1.Build a face recognition system | Implement the neural style\\ntransfer algorithm andgnerate dnovel artistic images\\nCERTIFICATES & EXTRA\\nCOURSES\\nDeep Learning Specialization\\ndeeplearning.ai\\nNeural Network and Deep Learning\\ndeeplearning.ai\\nConvolution Neural Networks\\ndeeplearning.ai\\nMachine Learning\\nStanford University, Coursera\\nThe Analytics Edge\\nMIT University, Edx\\nStatistical Thinking for Data Science and Analytics\\nEdx\\nACHIEVEMENTS\\nIIT-JEE/AIEEE\\nAIR 3085/2455\\nCompetitions and Hackathons\\nParticipated in various data science competitive platforms and\\nsecure top 10 percentile rank.\\nINTERESTS\\nReading\\nTraveling\\nProjects at Zenon\\nProjects at Opera\\nProjects at Impact\\n',\n",
       " 'Aravind Pai\\nData Scientist\\nData Scientist with hands-on experience in solving real world\\nproblems. Solving some of the challenging research driven\\nproblems in daily life.\\naravindpai23@gmail.com\\n+91 8074101068\\nGurugram, India\\nwww.analyticsvidhya.com/blog/author/aravindpai/\\nlinkedin.com/in/aravind-pai\\ngithub.com/aravindpai\\nWORK EXPERIENCE\\nData Scientist\\nSpektacom\\n07/2020 - 04/2021, \\nBangalore, India\\nDeveloped computer vision-based products for cutting-\\nedge technologies in sports and deployed them in\\nproduction.\\nData Science Intern\\nAnalytics Vidhya\\n12/2019 - 06/2020, \\nGurugram, India\\nDeveloped Proof of Concept for Ball Tracking and Net\\nAssistant System for cricket using Computer Vision.\\nData Science Summer Intern\\nAnalytics Vidhya\\n05/2019 - 08/2019, \\nGurugram, India\\nExtracted highlights from a cricket video without using\\nMachine Learning and Deep Learning\\nGenerated one line summary for the long movie reviews\\nusing Attention based Encoder Decoder architecture\\nData Science Intern\\nProbyto\\n06/2018 - 11/2018, \\nCoimbatore, India\\nDevelops custom AI Solutions, provide fully managed AI services and\\nachieved AI innovation at scale\\nDeveloped a prototype for capturing the Adverse Drug\\nReactions from the user comments\\nDesigned and built streaming data pipeline for the social\\nmedia data using Amazon Web Services\\nEDUCATION\\nM.Sc 5 year Integrated Data Science\\nPSG College of Technology\\n2015 - 2020, \\nCGPA: 8.0/10\\nHigher Secondary Course\\nNarayana Junior College\\n2014 - 2015, \\n97.8%\\nSecondary Course\\nNarayana High School\\n2012 - 2013, \\nCGPA: 9.8/10\\nSKILLS AND TOOLS\\nMachine Learning\\nDeep Learning\\nComputer Vision\\nNatural Language Processing\\nPytorch\\nFastai\\nKeras\\nPython\\nSQL\\nGoogle Data Studio\\nACHIEVEMENTS\\nAuto No Ball Patent\\nOwn the inventorship rights on Auto No Ball patent\\nCOURSES\\nDeep Learning Specialization by Andew NG\\nStanford CS 224N: Natural Language Processing with\\nDeep Learning\\nPROJECTS\\nAugmented Reality for Sports\\nDeveloped an Augmented Reality application for the biopic of Rahul\\nDravid that brings covers to life.\\nCricket Commentary Analysis\\nAnalyzed the Indian team performance using the cricket commentary\\ndata\\nNeural Machine Translation system\\nDeveloped a model to convert text in one language to text in another\\nlanguage\\nLANGUAGES\\nEnglish\\nFull Professional Proﬁciency\\nKannada\\nFull Professional Proﬁciency\\nHindi\\nProfessional Working Proﬁciency\\nTelugu\\nProfessional Working Proﬁciency\\nTamil\\nProfessional Working Proﬁciency\\nAchievements/Tasks\\nAchievements/Tasks\\nAchievements/Tasks\\nAchievements/Tasks\\n',\n",
       " ' \\n \\n \\n \\n \\n \\nSENIOR DATA SCIENTIST \\n● Machine Learning ● NLP ● Deep Learning ● AI ● Semantic Search Engine ● RASA ● Python \\n \\nPROFESSIONAL SUMMARY \\n \\nSenior Data Scientist (Kaggle Competition Expert) with a strong math background and experience in Advanced \\nMachine Learning, NLP, RASA NLU, Deep Learning and Statistics. Total 8+ years of experience in Data Science. \\n \\nWhile working in current company, achieved value realization of 30 Million dollars through implementation of \\nvarious operations project in different towers for APAC zone. While working as APAC Zone lead and Commercial \\nTower lead, build a 20+ project pipeline for the team. \\nDeveloped Context Based Search Engine Product using fastText, NLP and Python. This includes a feature of real \\ntime Over the Top training of fastText model using event Queue (Redis) processing and Task Scheduler (Huey – \\nlight version of Celery). \\nTECHNICAL SKILLS SUMMARY \\n \\n• \\nMachine Learning: Classification, Regression, Clustering algorithms; Predictive and Statistical Modelling;  \\nEnsemble models; Advanced ML( LightGBM and Xgboost ), Hyperparameter Optimization; Natural \\nLanguage Processing(NLP) using Spacy and Gensim; Word2Vec, Word Embeddings;  Intent and Entity \\nextraction using RASA NLU; Context based Search Engine(Semantic and Syntactic) using fastText; \\nTimeseries Forecasting; Recommendation Engine, Collaborative Filtering; Deep Learning (CNN, LSTM, RNN), \\nKeras, Pytorch, Transformers, BERT, NER, Question and Answering Model; SparkNLP; H2O AutoML, \\nAnomaly Detection, Unsupervised Clustering(Kmeans and DBSCAN). \\n• \\nSoftware and Programming Languages: Python (scikit-learn, numpy, scipy, pandas), Huey (as task \\nscheduler), Redis (as event queue processor), Flask (Rest API), Linux, GitHub, Pie Git Cloud, Azure Dev Ops, \\nJava, SQL, Selenium WebDriver, Oracle. \\n• \\nTools: Power BI, SparkBeyond, Microsoft Excel, Macro. \\n• \\nDomain Knowledge:   Retail, Banking, Health Care and Financial. \\n \\nPROJECTS | HACKATHONS \\n \\nParticipated in various Machine Learning and NLP Hackathons which involves Data mining, Data Analysis, Data \\npre-processing, applying machine learning algorithms and doing predictive analysis. Performed well in these \\nHackathons and below are the details – \\n• \\n2019 Data Science Bowl – Kaggle Hackathon – Got Silver Medal -  \\nCreated a model to predict the performance of children on PBS Kids Measure Up app based on their \\nhistory learning records. Created advanced Feature engineering by using Business knowledge and did \\nASHISH KUMAR SINGH \\n \\nE-mail: ashishkr.30@gmail.com  \\n \\n \\n \\n               Contact: +91 9619794540 \\nstacking and Ensembling of LGBM models. Used techniques like pseudo labelling and Customized \\nStratified Group Kfold Cross validation technique. Kaggle id – Ashufet. \\n \\n• \\nNFL Big Data Bowl – Kaggle Hackathon – Got Silver Medal -  \\nCreated a model to predict how many yards will an NFL player gain after receiving a handoff. Our models \\nwere used to predict real time NFL games and then based on Continuous Ranked Probability Score (CRPS) \\nParticipants were ranked. \\nApplied geometry-based Feature engineering using X, Y coordinates of players and did Ensembling of \\nNeural network and LGBM models. Kaggle id – Ashufet. \\n• \\nHome Credit Default Risk – Kaggle Hackathon – Got Bronze Medal -  \\nCreated a model to predict the probability that a customer would be a defaulter in paying his/her loans as \\nper the last loan records and application data. Training data consists of 7 different files having data from \\ndifferent sources with more than 4lac records. \\nApplied various advanced ML algorithms and advanced Feature Engineering techniques like Target Mean \\nEncoding, Aggregation methods and so on. Kaggle id – Ashufet. \\n \\n• \\nLinguipedia Code Fest NLP conducted by IIT BHU – Analytics Vidhya Hackathon – Secured 6th rank - \\nUsing NLP, Spacy and ML algorithms created a model which can classify the sentiments of the tweets \\nwhether it is a positive or negative review comments for different products. Techniques used like \\ntfIdfVectorizer, CountVectorizer and Gensim Word2Vec.  Analytics Vidhya id – ashishkr.30@gmail.com \\n \\nWORK EXPERIENCE \\nABInBev \\n                            Jan 2020 — Present \\nSenior Data Scientist \\nCurrently, working as APAC Zone lead and Commercial tower lead where I am managing around 10+ projects \\nand have also worked as Individual Contributor for multiple projects. \\nProject    :    Sales Team Target Forecasting  \\n Client  :      Internal \\nDeveloped a Forecasting Model to predict the daily target for the China sales team for each POC/SKU/Shift. Total \\nadditional sales worth of 10 Million USD for 1 year. Productized the complete solution using Azure Dev Ops. \\nProject    :    Commercial Area Optimization  \\n Client  :      Internal \\nBuild a solution for Commercial Area Optimization project within 2 months which gave around 15 Million USD \\nsavings yearly. In this project, goal was to optimize the sales team commercial area so that ABInBev Market share \\nincreases. \\n \\nProject    :    Aged GR Prediction  \\n Client  :      Internal \\nDeveloped a Classification model to predict Aged GR (GRNI - Good Received Not Invoiced) to save Inadvertent \\npayments which affect EBIDTA and cash flows. Build LightGBM model to do the prediction with F1 score of 95% \\nand Recall of 98%. Value saving of 5 Million USD. \\nSopra Steria \\n                            Jun 2019 — Dec 2019 \\nLead Data Scientist \\nProject     :     INTELLIGENT SEARCH                                     \\nClient     :     Internal (across all projects) \\nManaged a team of 10 people to develop Context based Search Engine using fastText model which was trained \\non data available on different portals of over 200 projects. Data included documents (docx, pdf, xlsx, pptx, txt, rtf) \\nof SharePoint, content of WIKI, JIRA and Outlook Calendar. Provided the feature of Real time Over the Top \\ntraining of fastText model for document add, update and delete using event Queue (Redis) processing and Task \\nScheduler (Huey – light version of Celery). Used Context based Spell Checker (complete logic developed by me) \\nto correct the spelling mistakes and used tf-Idf vectorizer as a booster to get an accuracy of around 90%. \\n \\nTata Consultancy Services \\n                          Mar 2013 — May 2019 \\nData Scientist \\nProject     :     Custom Entity Extraction from Search Query   \\nClient     :     Apple \\nImplemented NLP search bar on the client dashboard so that instead of manual selection of filters on UI, user can \\ntype the query in NLP search box and get the filters set on UI. To solve this business requirement we have used \\nRASA NLU for customized entity training and entity extraction (NER-CRF algorithm) from the NLP query. Other \\ntools which we have used are CHATITO for training data generation. We have created end to end product which \\nincluded creating model, creating pipeline for production deployment using Flask and Gevent WSGIServer. \\n \\n                                              ACHIEVEMENTS \\n• \\nQualified Gate exam in 2012 with 97 percentiles. \\n• \\nGot 142nd rank in ISRO (Indian Space Research Organization) exam and 112th rank in NPCIL (Nuclear Power \\nCorporation of India Ltd) exam all over India in 2012. \\n \\nACADEMIC CREDENTIALS \\nB.Tech [ECE] \\n                                                2008 — 2012 \\nFaculty of Engineering and Technology, Gurukul Kangri Vishwavidyalaya, Haridwar - 79.20% \\n \\nHSC [12th] \\n                                                2007 — 2008 \\nRam-Eesh International School, Greater Noida, Uttar Pradesh, CBSE board - 86% \\n \\nSSC [10th] \\n                                                2005 — 2006 \\nRam-Eesh International School, Greater Noida, Uttar Pradesh, CBSE board – 84.6% \\n \\nPERSONAL DETAILS \\nFather’s Name          :   Ashok Kumar Singh                                      Date of Birth              :   3rd April, 1991 \\nCurrent Address        :   Bangalore, Karnataka                                 Permanent Address  :   Greater Noida, UP -201310                           \\nHobbies                       :   Coding and taking part in various Hackathons, Gaming, Playing chess and cricket.                  \\n',\n",
       " '                                    BALLA SAI HARSHITH \\nAddress: Door no:19-1-13,                                   Email: saiharshithballa99@gmail.com                                                                                                                                                                                 \\nChinamamidipalli village,                                    Mobile no: +91-9493105147 \\nNarsapuram mandal,                \\n \\n      Github: https://github.com/harshithballa                                                                                         \\nWest Godavari-534275.   \\n \\nCAREER OBJECTIVE: \\n To work in pragmatic way in an organisation where I can show my talent and enhance \\nmy skills to meet company goals and objectives with full intensity. \\nACADEMIA: \\nQualification: Name of the \\ninstitution: \\nBoard/University: \\nYear of \\npass: \\nscore: \\nB.Tech(ECE) \\nSRKR Engineering \\nCollege, \\nBhimavaram \\nAndhra University \\n   2020 \\n8.86 \\nIntermediate \\nSri Chaitanya \\nJunior College, \\nVijayawada \\nAP-Intermediate \\n   2016 \\n96.8% \\nUI  SSC \\nBhashyam EM \\nHigh School \\nBoard Of Secondary \\nEducation, \\nAndhra Pradesh \\n   2014 \\n9.7 \\n \\nWORK EXPERIENCE: \\nSep 2020- Present: \\n• \\nWorking as a Software engineer in TCS. \\nAREAS OF INTERESTS: \\n• \\nArtificial Intelligence \\n• \\nMachine learning   \\n \\nTECHNICAL SKILLS: \\n• \\nSoftware languages: C, Java and Python. \\n• \\nLabVIEW programming. \\n• \\nTensorflow and OpenCV. \\n• \\nArduino  programming \\n• \\nMS Word, MS Power Point, MS Excel. \\nCERTIFICATIONS: \\n• \\nDone Deep learning Specialization and Tensorflow in Practice Specialization from \\ndeeplearning.ai ( Coursera ). \\n• \\nCertified for completing the Fundamentals of Machine Learning A-Z course( by \\nUdemy). \\n• \\nCertified for completing the Cisco Introduction to IoT course. \\n• \\nCertified for completing the Arduino course by IIT Bombay. \\n• \\nCertified for completing the Java course by Spoken Tutorials(IIT B). \\n• \\nCertified with Grade C in the Cambridge Business English Certificate \\nVantage(BEC). \\n \\nCO-CURRICULAR ACTIVITIES: \\n• \\nPresented a circuit model of Solar Tracker and won 1st prize in CRUX-2K16, A \\nHardware context organised by IETE student forum, SRKREC. \\n• \\nPaper Presentations on Li-Fi and on Metallic Hydrogen. \\n• \\nAttended workshops on Embeded Systems, Cyber Attacks, Big data, Mind \\nControlled Robotics. \\n• \\nParticipated in the grand finals of  Smart India Hackathon (SIH-2019). \\nEXTRA-CURRICULAR ACTIVITIES: \\n• \\nStudent volunteer for TRANCE2k-18 organised by department of ECE, SRKR \\nEngineering College. \\n• \\nStudent volunteer for Spardha 2K18 (a cross departmental hackathon) conducted by \\nour college. \\n• \\nStudent Co-ordinator for the workshop conducted by Robokart in TRANCE 2K19.  \\nPROJECTS: \\n• \\nOn Arduino: Solar Tracker, Obstacle avoiding robot, edge sensing robot and CNC. \\n• \\nCircuits: Water level indicator, Controlling home appliances using RF transmitter      \\nand receiver. \\n• \\nSoftware: Chatbots for assisting passengers in railway stations and bus stands.(SIH) \\n• \\nAI & Ml:  Face detection using OpenCV, Object detection using SSD, Classification \\nmodel using Transfer learning and deploying it on to the web. \\n \\nHOBBIES: \\n• \\nPlaying Cricket, Chess. \\n• \\nMeditation. \\nPERSONAL PROFILE: \\n• \\nNationality:                     Indian \\n• \\nMarital status:                 Single \\n• \\nFather Name: \\n           B V Seshagiri Rao \\n• \\nMother Name: \\n           B Swathi \\n• \\nLanguages Known:         English, Telugu \\n \\nPERSONALITY TRAITS: \\n• \\nI am a self-motivated person. \\n• \\nBeing confident and determined. \\n• \\nAbility to cope up with different situations. \\n \\nPlace: Hyderabad                             \\nDate:   04-02-2021                                                                                     B.Sai Harshith                \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n',\n",
       " '                                                                                                     EDUCATION \\n                                                                                                     INTERNSHIP \\n                                                                                                          PROJECTS \\n                                                                                      RELEVANT COURSEWORK  \\n                                                                                                \\n        SKILLS  \\n                                                                      POSITIONS OF RESPONSIBILITY AND AWARDS \\n                                                                                              WORK EXPERIENCE \\n                                                                                                     COMPETITION \\n     AYUSH PALIWAL  \\n     AEROSPACE ENGINEER (B.Tech)                                                                                                                                                                                    \\n     ayushpaliwal2015@gmail.com|9800315999 \\n \\n       Year                                          Degree/Exam                                      Institute                                                                        CGPA/Marks \\n       2019                                         B.TECH                                                  IIT Kharagpur                                                                     7.21/10 \\n       2014                                         Senior Secondary-CBSE                      Tuli Public School, Nagpur                                               86.2% \\n       2012                                         High School-CBSE                                Center Point School, Nagpur                                           9.0/10 \\n \\n \\n       Reliance Jio       \\n \\n \\n \\n \\n         Data Scientist       \\n \\n \\n                    July 2019-Present \\n       • Working on development and maintenance of machine learning models integrated to an automation pipeline of the project \\n       • Trained multiple classifiers on historical ambient condition data to forecast potential pest and disease (p&d) attack on the crop  \\n       • Studied p&d lifecycle to identify key factors that facilitate or hinder their growth and use them to build features for the model \\n       • Responsible for updating models when new sensors are added in IOT device or new seasonal p&d training data is available  \\n \\nBen Gurion University of the Negev, Israel           Deutsche Telekom Innovation Labs  \\n \\n \\n                    May 2019 \\n• Worked on a novel method of unsupervised optimization of embedding dimension based on the stability of embedding  \\n• Created a custom metric, Node Pair Distance Correlation, to calculate embedding stability at varying dimensions \\n• Implemented the method to successfully obtain optimum dimensions of Android Applications (APK apps) embedding \\n• Received an appreciation for the work and was offered an opportunity to continue the research project as research assistant  \\n \\nDeloitte                                                         Consulting: Information Management and Analytics                                     \\n    May 2018    \\n• Exploratory Data Analysis: Analyzed sales data by plotting sales against customer demographics, product and store details \\n• Statistical Inference : Drew inference by performing parametric and nonparametric tests to review feature importance \\n• Handling Missing Values: Developed a versatile imputing method which improved imputed missing value accuracy by 11.9%    \\n• Machine Learning: Used Random Forest to achieve 1970 rmse on predicted sales value where the benchmark was 5050 \\n \\n• LTFS Data Science FinHack 3                                     \\n \\n \\n \\n \\n \\n \\n                          February 2021 \\n   Accomplished rank 11 out of 34415 registered in the competition; got a score of (0.62), highest (0.83) in F1 scale  \\n• JantaHack: Healthcare Analytics II  \\n \\n \\n \\n \\n \\n \\n \\n \\n                    July 2020 \\n   Accomplished rank 7 out of 19419 registered in the competition; got a score of (0.4364), highest (0.4390) in Accuracy scale \\n• JantaHack: Machine Learning in Agriculture  \\n \\n \\n \\n \\n \\n \\n \\n       September 2020 \\n   Accomplished rank 2 out of 15381 registered in the competition; got a score of (0.9605), highest (0.9610) in Accuracy scale \\n• JantaHack: Machine Learning for Banking  \\n \\n \\n \\n \\n \\n \\n \\n \\n    May 2020 \\n   Accomplished rank 6 out of 8341 registered in the competition; got a score of (0.5365), highest (0.5399) in F1 scale \\n• Gartner HackElite  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n       September 2019 \\n   Accomplished rank 10 out of 660 participants in the competition; got a score of (0.9217), highest (0.9432) in AUC-ROC scale \\n \\n \\nDrivers of Customer Loyalty in Airline        Vinod Gupta School of Management, IIT Kharagpur                                   November 2018 \\n• Worked on airline review data which comprised of inflight facility ratings, comments and if passenger recommended the airline \\n• Created new unbiased inflight facility ratings by calculating sentiment score of the facility related sentence in review \\n• Found major driving traits for each airline, flying class and overall data using linear regression and Random Forest model \\n \\nApplied Machine Learning | Exploratory Data Analysis | Linear Regression and Modeling | Statistical Methods | R Programming \\n \\nProgramming Languages: C, Python and R | OS: Linux (CentOS and Ubuntu) and Windows | Database: SQL, Cassandra, MongoDB \\n \\n \\n• Secretary, Aerospace Engineering Society, IIT Kharagpur \\n \\n \\n \\n \\n \\n \\n               August 2016 \\n• Won Best Fresher Volunteer Award, Radha Krishnan Hall of residence, IIT Kharagpur                                                             April 2017 \\n• Won a cash prize for best project presentation at Reliance Jio  \\n \\n \\n \\n \\n \\n \\nAugust 2019 \\n \\n',\n",
       " \"SUDARSHAN KUMAR | 13CE10048 \\n \\nCIVIL ENGG. (B.Tech 4Y) \\n \\nEDUCATION \\n \\nYear \\nDegree/Exam \\nInstitute \\nCGPA/Marks \\n2017 \\nB.TECH \\nIIT Kharagpur \\n7.77 / 10 \\n2012 \\nAll India Senior School Certificate Examination \\nDAV Public School, Patna \\n93.2% \\n2010 \\nAll India Senior Secondary Examination \\nPatna Central School, Patna \\n9.6 / 10 \\n \\nAWARDS AND ACHIEVEMENTS \\n \\n•Acquired 98.48 percentile score in Joint Entrance Examination Mains,2013 among 12 lakh students that had appeared for the examination \\n•Received Certificate of merit & Medal for exceptional performance in Class XII Boards & Joint Entrance Examination from my school  \\n•Received Rajeshwari Sahu Memorial Scholarship of INR 12,000 and Merit-cum-Means scholarship of INR 1,02,000 from IIT Kharagpur \\n \\nINTERNSHIPS \\n \\nDRG Analytics & Insights Pvt. Ltd., Bangalore \\nAnalytics \\n        May'16-July'16 \\n \\n•Understood Payer landscape and provided a solution for extracting plan level benefit design information useful for pharmaceutical firms \\n •Firm understanding and exposure to associated data for drawing meaningful and actionable insights from claims data using MySQL \\n•Analysed claims data generated at Pharmacies in the US and gained expertise around data nuances specific to my project \\n \\n•Developed an algorithm with discreet business rules which helped extract Patient copay and CoInsurance data for branded drugs which \\nis very valuable for Pharmaceutical firms struggling with drugs access within the current Payer market in the United States \\n•Wrote a modular and reusable algorithm which could be used for extracting copay data for any branded drug in the US market \\n \\nIIM Lucknow \\nMarketing Management \\n      May'15-June'15 \\n \\n• Developed pricing strategy for baseball matches based on willingness-to-pay survey data for maximizing revenues  \\n•Performed profit analysis for new product’s market entry and its effect on the net income of existing products and the company \\n• Mastered various concepts of marketing like segmentation, brand positioning, branding in web etc. through various mini-cases \\n \\nLarsen & Toubro Const. Ltd., Patna \\nIndustrial Training \\n       December'14 \\n \\n•Analyzed various sections of beam and column designs and subsequent load analysis using softwares like ABAQUS and STAAD \\n•Tested and verified the quality of aggregates and concerete mix (mainly M20 and M30) that were passed to the field  \\n•Performed in-depth analysis for optimal results in the Quality Assurance/Quality Control Lab based on Indian Standard Codes \\n \\nPROJECTS \\n \\nB. Tech. Project, Civil Engineering Department, IIT Kharagpur \\n       August'16-present \\n \\n•Prepared a documentation on chemical methods of synthesis and characterization of Magnetic Iron-OxideNano Particles(MNOPs)  \\n•Application of MNOPs on Water and Waste-water treatment based on the experimental outcomes \\n \\nTransportation Lab Project, IIT Kharagpur \\n        November'14 \\n \\n•Designed Depth-First-Search(DFS) Algorithm, Floyd's Algorithm and Prim's Algorithm excel sheet using macros (VBA) \\n•Applied the algorithms for minimum spanning tree and shortest path lengths in Transportation engineering problems \\n  \\nCOMPETITION/CONFERENCE \\n \\nAnalytics Vidhya : The Ultimate Student Hunt \\n       October'16 \\n \\n•Cleaned, processed and treated missing values  and applied various feature engineering after understanding the data   \\n•Developed SVM Model, Random Forest and Gradient Boosting (GBM) to predict number of people visit park on a particular day \\n•Created an ensemble model using Neural Network & GBM to achieve RMSE of 116.1 and securing a rank of 74/1494 on the leaderboard \\nHackerRank : Predict Email Opens \\n         August'16 \\n \\n•Processed training and test data, reduced number of features based on suitable parameters to prepare the data for prediction \\n•Achieved an accuracy of 43.8%(maximum achieved accuracy in the contest - 60% ) in prediction of whether or not every user will  \\n  open an email using Random Forest Algorithm \\n•Also applied Logistic Regression and Naive Bayes Classifier and achieved a ranked of 126 in the Machine Learning CodeSprint \\n \\nKaggle : E-Commerce Analytics \\n          March'16 \\n \\n•Developed an analytics model that would help buyers and sellers predict the sales success of a set of eBay listings for iPads  \\n•Developed Logistic Regression Model for the binary output to predict the sales of the ipads \\n•Random Forest model with Bag-of-Words, among various other models was found to be the optimal model \\n \\nSKILLS AND EXPERTISE \\n \\nProgramming Languages : SQL, Visual Basic for Application, R Programming, C \\nSoftware Packages \\n : MS Office, SolidWorks, AutoCAD, Staad.Pro \\n \\nCOURSEWORK INFORMATION \\n \\nMachine Learning, The Analytics Edge, Probability and Statistics, Statistical Methods in Hydrology, The Data Scientist's Toolbox, \\nRegression Models, Data Analysis and Statistical Inference, Construction Planning and Management \\n \\nEXTRA CURRICULAR ACTIVITIES \\n \\n•Awarded 1st Prize in an overnight event(CRIAR) for constructing tower crane prototype in Megalith-2014(Civil Engg. Fest of IIT Kharagpur) \\n•Awarded 4th Prize and a special prize for best fresher's performance in Case study over E-waste management and disposal in Megalith \\n•Volunteered for 2 years for the betterment of poor children in the nearby villages outside IIT Kharagpur and taught them with utmost \\ncare as a part of National Service Scheme \\n•Ensured overall development of 5 freshmen from the department of Civil Engineering as a student mentor to them \\n \\n\",\n",
       " \"| |  \\nMay-July 2017 \\nCOMPETITIONS \\nLord of the Machines | Text Classification | Data Science Hackathon | Analytics Vidhya \\nMarch 2018 \\nZS Data Science Challenge - 2018 | Sale Forecasting | HackerEarth \\nJuly 2018 \\nPOSITIONS OF RESPONSIBILITY \\nMedia Cell Sub-Head | Spring Fest | Annual Social & Cultural Fest of IIT Kharagpur \\nOct 2015 - Feb 2017 \\nSKILLS AND EXPERTISE \\nHITEC City, Hyderabad, Telangana 500081, India \\nABHIROOP KUMAR \\n+91-9932549291 | abhiroopkumar.iitkgp@gmail.com \\nACADEMIC QUALIFICATIONS \\nYear \\nExamination/Degree \\nInstitution/Board \\nPerformance \\n2019 \\nBachelor of Technology in Mining Engineering \\nIndian Institute of Technology, Kharagpur \\n7.64/10 \\n2014 \\nCentral Board of Secondary Education (CBSE) (Class XII) \\nD.A.V. Public School, B.S.E.B Colony, Patna \\n84.60% \\n2012 \\nCentral Board of Secondary Education (CBSE) (Class X) \\nD.A.V. Public School, B.S.E.B Colony, Patna \\n9.2/10 \\nAWARDS AND ACHIEVEMENTS \\n▪ Published a Research paper in IEEE 16th India Council International Conference 2019 on the topic “Emotion Recognition from EEG Signal” \\n▪ Earned 3 Winner, 4 Top 10, and 3 Top 25 badges for excellent performance in various Data Science hackathons on Analytics Vidhya \\n▪ Secured 3rd position in Analyze This 2018 organized by American Express and 5th Rank in the ZS Data Science Challenge by ZS Associates \\n▪ Bagged 2nd and 5th Position among 34K+ data enthusiasts in Talent Hunt Hackathon by L&T Financial Services for two consecutive years \\n▪ Identified patients who will be on the brink of a significant increase in health care expenditures to help care management programs  \\n▪ Predicted individual‘s medication state and Parkinson's Disease severity using raw sensor(accelerometer and gyroscope) time-series data \\n▪ Identifying Risk and Stratifying members based on their historical clinical data and taking actionable decisions to reduce ER costs \\n▪ Predicted estimated claim process time and check issue date and deployed it on the provider’s dashboard for Realtime prediction \\n▪ Improved ZestMoney credit fraud detection model by 6% by analyzing member behavior and integrating socio-economic features \\n▪ Analyzed the spending pattern of the account holder to discover fraudulent activities▪ \\n \\n▪ Developed an ensemble sale forecasting model employing Holt-Winters, ARIMAX, and TBATS achieving an accuracy of 91.58% \\n▪ Effectuated by the company, the model is presently being used to forecast sale for PSP of over 400 stores in 31 states in the USA \\n▪ Raised Gross Margin of a company by 2.6% by optimizing the car rental pricing model incorporating competitor price information \\n▪ Developed a customer retention model to identify the potential customers who are likely to book a car in future months \\n▪ Applied k-Nearest Neighbors algorithm for efficient selection of 100 stores for running promotional campaigns to maximize revenue \\n▪ Integrated Apriori algorithm of association rule learning incorporating Market Basket Analysis for promotional marketing strategy \\n▪ \\nLeading a project titled ‘Classification of EEG Motor Imagery (MI) Multi-Class Signals used in BCI’ under Prof. Debasis Samanta \\n▪ \\nImproving the classification accuracy by noise reduction of MI signals, optimizing features and, implementing an ensemble classifier \\n \\n▪ Secured 1st position among 1200+ participants in a two-month-long Hackathon predicting the performance of an enrollee on tests \\n▪ Built a 3-level stacked model with 8 different classification algorithms as base-classifier and trained GLM & GBM as meta-classifier \\n▪ Improved model accuracy by incorporating Matrix Factorization via Singular Value Decomposition (SVD) in the stacked model \\n▪ Predicted the click probability of links inside a mailer for email campaigns of 1.68M unique users with an imbalance ratio of 80:1 \\n▪ Optimized model predicting power by applying CatBoost, Light GBM, XGBoost algorithms getting an AUC-ROC score of 0.646 \\n▪ Ranked 5th among 10000+ competitors in Pan-India contest, forecasting sales of 5 different products in 6 countries \\n▪ Applied Ensemble forecasting technique employing multivariate ARIMA model, linear regression, SVR, holt’s winter and TBATS \\n \\n▪ Individually garnered funds worth INR 3.2 Lakhs through corporate deals and alumni contributions \\n▪ Coordinated the publicity drive in Northeast India leading to a 68% YoY increase in participation and 35% increase in media outreach \\n▪ Conducted the nationwide prelims of 4 events (140% participants increase) in Mumbai with Nukkad taking place for the first time \\n▪ Conceptualized and edited the promo for Spring Fest 2017 telecasted on a popular music channel VH1 with total views of 2 million \\n▪ Directed and edited the Official After-movie and the Social initiative of Spring Fest 2017, Masoomiyat which has total views of 50000+ \\nSoftware: Python | RStudio | Power BI | Tableau | Adobe Premiere Pro || Languages: C | C++ | R | Python | SQL \\nRelevant Coursework: Econometric Analysis | Probability and Statistics | Machine Learning | Programming and Data Structure \\n▪ \\nEXTRA-CURRICULAR ACHIEVEMENTS \\nCultural \\n▪ Part of the bronze winning Inter Hall Eastern Vocals team of Patel Hall of Residence in General Championship 2016-17 \\nSports \\n▪ Represented Patel Hall as part of its Athletics team in the 800-meter race for 2 years in General Championship Sports \\nMentorship \\n▪ Mentored 6 freshmen students on campus under the purview of the Dean of Student Affairs, IIT Kharagpur \\n \\nWORK-EXPERIENCE/PROJECTS \\nOptum, United Health Group | Hyderabad | Data Scientist                                                                                                                  July 2019 - Present \\nZestMoney | Bangalore | Data Science Intern                                                                                                                                                   May-July 2019 \\nImpact Analytics | Bangalore | Data Science Intern                                                                                                                                         May-July 2018 \\nDrivezy  | Bangalore |  Business Analyst Intern                                                                                                                                                  Dec -Jan 2018 \\nPeel Works |  Mumbai |   Data Science Intern                                                                                                                                                  May -July 2017 \\nBachelor's Thesis Project |  Guide: Prof. Debasis Samanta, CSE-IIT KGP                                                                                                    July -April 2019 \\nAug 2016 – Feb 2017 \\nCore Organising Team Member | Spring Fest | Annual Social & Cultural Fest of IIT Kharagpur \\nMay-July 2018 \\nThe Data Identity (Winner) | Performance Prediction | Student DataFest 2018 | Analytics Vidhya \\nReceived Pre-Placement Offer (PPO) | Project: Deploying sales forecasting model and promotional analysis on products \\n\",\n",
       " ' \\nSalim Shaikh \\nManager, HDFC Bank Ltd. \\n \\n \\nEDUCATION \\n \\nDegree \\nInstitution \\nCGPA \\nYear of \\nPassing \\nM.Sc \\n(STATISTICS) \\nUniversity Of Mumbai \\n \\n6.83/7 \\n \\n2017 \\nB.Sc (STATISTICS) \\nUniversity Of Mumbai \\n91.6% \\n2015 \\nSKILLS \\n \\nLanguages \\nML/DL Tools \\nBig Data/ETL \\nPython \\nPandas / Dask \\nPySpark \\nSQL \\nOpenCv \\nH2o Sparkling water \\nR \\nSklearn \\nHiveQL \\nScala \\nGPU – CuDF, CuML \\nTalend OS \\nSAS \\nH2o \\nPentaho \\nC \\nNLP, RNN, CNN, \\nNN \\nMapReduce \\nSPSS / Minitab \\nPytorch \\nHadoop \\n \\nEXPERIENCE \\nHDFC Bank Ltd, Manager – Risk Analytics \\nDec,2018 – Present \\n\\uf0b7 \\nDeveloped OCR framework to extract Name, Organization Name, \\nGross/Net Salary from Govt & Non-govt salary slips (response \\ntime=25secs per image)\\uf020\\n\\uf0b7 \\nAutocure collection scorecard for SI & ECS customers using SOTA Deep \\nLearning techniques\\uf020\\n\\uf0b7 \\nPredictive model to identify which are the potential customers who \\nswipe on our POS/PG but don’t have our credit card for sourcing new \\nCredit Cards. Used Neural Networks, XGBoost, etc. and improved the \\nKS.\\uf020\\n\\uf0b7 \\nIncome estimation for the people who qualify for getting new Credit \\nCards through the above predictive model. Point estimation as well as \\ninterval estimation using XGBoost, Multinomial Naïve Baye’s, etc. was \\nused. \\uf020\\n\\uf0b7 \\nMulti-Bureau Application scorecard for Auto Loan using Logistic \\nRegression, Decision Tree\\uf020\\n\\uf0b7 \\nAutomated the creation of entire Bureau Application scorecard and \\ndeployed the scorecards on Azure\\uf020\\n\\uf0b7 \\nSupervising team members to use Machine Learning Techniques in their \\nprojects to improve accuracy.\\uf020\\nVodafone Idea Limited, Asst. Manager – Advanced \\nAnalytics \\nJUL,2017 – Dec,2018 \\n\\uf0b7 \\nPropensity models to manage drop in usage & subscriber churn for \\nprepaid vertical. This required micro-level targeting through separate \\nmodels for each circle and sub-segment. \\uf020\\n\\uf0b7 \\nBuilt a Model Automation Tool in Python to automate the entire model \\nbuilding process and thereby reduced the TAT from months to days\\uf020\\n \\nProfile \\n  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n L\\n \\n \\n \\n  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n  \\n \\n \\n \\n \\n \\n \\nMumbai, India \\n \\nPHONE \\n(+91) 8286588859 \\n \\nEMAIL \\nstatsguysalim@gmail.com \\n \\nLINKEDIN \\nwww.linkedin.com/in/salim-\\nshaikh-a082a7162/  \\n \\nBlogs \\n \\nBanking Case Study - Part 1 \\n \\nS/S \\n \\nHackathons \\n \\nKaggle  \\n \\nZindi Africa \\n \\nAnalytics Vidhya \\n \\nMachine Hack \\n',\n",
       " 'DHRUV DEEP BISHNOI \\n Data Scientist \\n dhruvbishnoi0010@gmail.com         \\n  Gurugram, India         \\n +91-9728427702           \\n  linkedin.com/in/dhruvdeepbishnoi \\n         \\nEDUCATION \\nSKILLS\\n \\nMBA in Information Technology & Marketing \\nIIT Roorkee \\n   June 2015 - May 2017 \\n \\nB.E in Computer Science Engineering  \\nThapar University, Patiala \\n   July 2010 - July 2014 \\n \\nEXPERIENCES \\n \\nData Scientist, Zenon Analytics \\nMay 2019 – Present                     \\n•  Building predictive models to enable the delivery of effective and \\nefficient campaigns which enable the client to target the appropriate \\naudience within an actionable time frame  \\n•  Working closely with the client leadership teams apart from \\nmodel development; by providing commercial and economic \\ninterpretation of model insights which contributes to the \\noverall strategic decision-making process  \\n \\nData Scientist, Absolutdata Research and \\nAnalytics \\nMarch 2018 – May 2019            \\n•  Building unsupervised machine learning algorithm to detect \\nanomalies in Power generation industry \\n•  Developing multi-dimensional customer segmentation basis \\nusage and analyzing the population for personalized actionable \\ninsights using machine learning algorithms \\n \\nConsultant, Mazars UK \\n May 2017 – Feb 2018            \\n•   Collaborated with the offshore team in the development of various \\nML models \\n•  Formulating the objective function, exploring data, model \\nbuilding and communicating insights with the team members \\n \\n \\nACHIEVEMENTS   \\n \\n•   Current Rank 99 on Analytics Vidhya platform \\n•   Won Rising Star Awards, 2018 in Absolutdata Analytics  \\n•   Certifications: Deep Learning Specialization by Andrew NG \\n \\nINTERESTS \\n \\n•   Participation in Machine Leaning competitions and Hackathons \\n•   Playing basketball competitively \\n•   Watching movies \\n \\n \\n \\n•  Python, Hive, SQL, PySpark \\n•  Natural Language Processing (NLP) \\n•  Deep Learning – TensorFlow, PyTorch \\n•  Clustering/ Segmentation, Classification, \\nRegression, Anomaly Detection \\n \\n \\n \\nPROJECTS \\n \\nNRR Model \\n• Net Response Rate model, for one of the world’s \\nlargest card company, categorized the prospect \\npopulation based on the maximum likelihood of \\nconversion which optimized the sales strategy \\nleading to 47% increase in conversion rates  \\n• Insights from the model further enabled the \\nmarketing team to develop focused campaigns by \\ntargeting categories with higher response rates \\nwhich ultimately lead lower marketing spends \\n• Model validation was carried out through Gini \\n \\nRisk Model \\n• Built a risk model for a reputed American \\nmultinational financial service company; which \\npredicted the probability of default for potential and \\nexisting prospects based on firmographic data, risk \\nattributes and neighborhood intelligence \\n• Risk scores were generated for each prospect which \\nenabled the client to segment potential prospects \\nand adjust their product features, prices charged \\nbased on the assigned risk level \\nIndustry Classification Model \\n• Model assisted in classifying target companies \\nwhich was a critical input for the sales & marketing \\nteam to develop tailored strategies for each industry \\n• Model output helped the Risk, NRR model as \\nIndustry was one of the key variables in these \\nmodels \\n• Model validation was carried out through accuracy \\nand coverage \\n \\nSegmentation \\n• Behavioral Segmentation for 14 million prepaid \\nsubscribers using K-means clustering for one of the \\nlargest telecom players in Ghana  \\n• Model provided an entire map of the customer base      \\nof the client which enabled the client to design \\ncustomized campaigns aimed at retaining the \\npreferred customer segments \\n• Profiling of segments and silhouette score was used \\nto validate the model \\n \\n \\n',\n",
       " \" \\n \\nKanishka Kayathwal                                                                                                          \\nMale \\nPhone: +91 9833570896 \\nEmail: kanishka24sept@gmail.com \\n                                                                                                                                                                                \\nACADEMIC QUALIFICATIONS \\nYear \\nDegree/Board \\nUniversity/Institution \\n%/CGPA \\n2020 \\nPost Graduate Diploma in Business Analytics \\nIIM Calcutta, ISI Kolkata, IIT Kharagpur \\n9.06/10 \\n2014 \\nB. Tech (Chemical Engineering) \\nIIT Bombay \\n7.10/10 \\nAWARDS & ACHIEVEMENTS \\nData Science \\nCompetitions \\n▪ Won a sponsored trip to SAS US global forum for securing 1st rank in BADM Championship        ’18 \\n▪ Won Silver medal (top 5% out of 1832 worldwide) in Kaggle's Instant Gratification challenge        ‘19        \\n▪ Ranked 1st in Stat Wars by ISI Kolkata, Time Series Data Science Hackathon at Integration              ‘19 \\n▪ Ranked 3rd in Data Hackathon conducted by HSBC for detecting change in customer behavior      ‘19 \\n▪ Secured rank in top 5 in D'conStruct - PwC DIAC's annual simulated business case competition     ‘18 \\n▪ Finished in top 21 out of 10,200+ participants in Brainwaves hackathon by Societe Generale              ‘19 \\n▪ Ranked in top 1% in Intel Scene Classification (17th) & AV Computer Vision Hackathon (23rd)       ‘19 \\nAcademic Awards ▪ Cleared CFA Level 1; Secured AIR 798 (top 0.2%) in IIT-JEE ‘10; Conferred NTSE Scholarship (2007) \\n▪ Won Ethics in AI award by Facebook Research for work in Targeted Bias in Indian Media Outlets                    \\nWORK EXPERIENCE (58 months)                                                                                                                                         \\nData Scientist                                                                      Mastercard AI Garage                                       Gurgaon (Apr’20 – Present) \\nTransaction Fraud \\nDetection \\n▪ Evaluated efficacy of card & merchant embeddings learned via graph algorithms on fraud detection  \\n▪ Applied Word2Vec based entity representations in transaction fraud classification model to increase  \\ndetection rate by 27% & reduce FPR by 30% for transactions with high fraud score \\n        Merchant \\nOpen/Closed Status \\n▪ Predicted open/closed status of merchant in real time using transactional data for Newark city \\n▪ Formulated rules based on segments obtained using clustering on merchant’s transaction patterns \\n▪ Scaling the solution in Spark and integrating it with Deployment Platform for complete US market \\n       Enhancing \\n      ThreatScan \\n▪ Enhancing ThreatScan: Product to identify vulnerabilities & gaps in bank’s authorization network  \\n▪ Built approve/decline transaction model for one of the US bank’s with AUC-PR score of 0.96 \\n▪ Trained GAN model to achieve similarity score of 0.80 between synthetic & real fraud transactions \\n▪ Evaluated bank’s model on generated transactions to detect patterns in approved fraud transactions  \\n         Charity \\nRecommendation \\n▪ Recommended relevant charities for given news articles using semantic textual similarity solution  \\n▪ Built a 2 step-model: first to filter articles with no charity theme by fine-tuning pretrained XLNet \\nmodel with an accuracy of 71% & then tagged remaining articles to four charities based on similarity \\nbetween them & charity description using pretrained transformers with hamming loss of 0.3 \\n          Patents \\n▪ Four patents approved by the Mastercard AI committee: currently in filing stage in the US \\nData Scientist                                                                      Play Games 24x7                                                Mumbai (Nov’17 – May’18) \\nCash  \\nDeposit Propensity \\nPrediction \\n▪ Predicted probability of player’s first cash deposit in 14 days from registration via Logistic Regression \\n▪ Handled class imbalance with SMOTE ENN technique & performed feature selection using Boruta \\n▪ Increased user conversion by 5% & annual revenue by INR 15 million through model deployment \\nUser \\nRevenue Segment \\nClassification \\n▪ Identified high revenue players at start of the journey based on their game behavior & cash deposit \\n▪ Created 150+ features, performed likelihood encoding on high-cardinality categorical attributes \\n▪ Applied XGBoost post PCA to attain AUC score of 0.80; interpreted model predictions using LIME \\nConsultant                                                                           Fractal Analytics                                                   Mumbai (Apr’16 – Oct’17) \\n                                         Handled Pricing and Promotional Analytics for Fortune 500 Company in FMCG Domain \\nCompetitive Price \\nBenchmarking \\n▪ Designed analytical tool to determine pricing strategy for a specific product against competition \\n▪ Segregated products and competitors using K-means; recommended price index to maximize share \\n▪ Enhanced delivery time efficiency by 57%, saving 60 man-hours per month through automation \\nFeature Promotion \\nImpact Analysis \\n▪ Tested retailer’s promo strategies with focus on consumer’s stocking up/filling in first & last week \\n▪ Built Classification trees to understand the impact of above featuring strategy on retailer’s sales \\nSenior Analyst                                         Received an early promotion with highest possible rating                 Mumbai (Jun’14 – Nov’15) \\nMarket Mix \\nModelling \\n▪ Developed an algorithm in R to compute Baseline Sales; estimated ROI over Trade Promotions \\n▪ Performed Regression analysis (R-squared 82%) to identify the factors contributing to volume sales \\n▪ Recommended optimal mix of promotions for PPG’s to enhance total category/brand weekly sales \\nProfessional \\nAchievements \\n▪ Consistently received a certificate of excellence from the client for on-time execution and delivery \\n▪ Awarded ‘Team of the Quarter’ for showcasing innovation in building pricing & promotion tool \\nDATA SCIENCE INTERNSHIP \\n                                                                                                Mastercard AI Garage                                       Gurgaon (Oct’19 – Mar’20) \\n \\n  Revenue \\nForecasting \\n▪ Built scalable revenue forecasting engine for Mastercard products at monthly level for finance team \\n▪ Implemented DL time series forecasting models for major drivers to achieve MAPE under 5% \\n▪ Ensembled various model forecasts using geometric & regression-based methods; adopted \\nreconciliation approach to obtain set of coherent forecasts \\nACADEMIC PROJECTS \\n \\n \\nKanishka Kayathwal                                                                                                          \\nMale \\nPhone: +91 9833570896 \\nEmail: kanishka24sept@gmail.com \\n                                                                                                                                                                                \\nCustomer Loyalty \\nScore Prediction \\n▪ Predicted loyalty score to understand customer preferences using 29M transactions of 200K users         \\n▪ Engineered temporal & aggregate features; performed feature selection using Null Importances \\n▪ Ensembled stacked boosting regressors & LightGBM classifier in Python to achieve RMSE of 3.61 \\nBias Detection in \\nIndian News \\n▪ Scraped last ten-year news from media outlets & filtered political articles using Topic Modeling \\n▪ Built linguistic models using NPOV corpus to capture language bias & sentiment via SentiWordNet \\n▪ Quantified Hyperpartisan Bias using 1D CNN architecture & ELMo embedding to obtain F1 score of 0.61 \\nADDITIONAL PROJECTS \\nMulti-Class Image  \\nClassification \\n▪ Leveraged transfer learning from Places365 dataset to classify ~25k scenes into six different classes \\n▪ Used SGD with restarts for faster convergence; fine-tuned ResNet-50 with differential learning rate \\n▪ Implemented Mixup & Cutout with Test Time Augmentation in PyTorch to get an accuracy of 95% \\nRecommender \\nSystem in Retail \\n▪ Built a Recommender System to predict most relevant items for 600+ users using past transactions \\n▪ Used a hybrid model of LSTM & Collaborative Filtering to achieve best mean F1-Beta score of 0.19   \\nPOSITIONS OF RESPONSIBILITY \\nManager \\nTechfest’13, IIT-B \\n▪ Led team of 150 students to showcase 80 exhibits from 12 countries at zero cost (30% increase y-o-y) \\n▪ Pioneered TechConnect to ensure diffusion of research in IITB (Budget: INR 2 million;150 projects)  \\n\",\n",
       " 'Army Institute of Technology, \\nPune \\nB.E Computer Engineering \\n   G M O T H Y \\n        7780272224  \\ngobillamothy85@gmail.com \\nhttps://github.com/G-Slient \\nhttps://www.kaggle.com/marison \\n \\nCAREER OBJECTIVE                                                                                                                                                                 _  \\nRecently Graduated (2020) Computer Engineering student at Army Institute of Technology seeking a position \\nin Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the goal of \\ndiscovering significant information, suggesting conclusions and support decision making. Eventually to \\nbecome a successful Data Scientist. \\nEDUCATION                                                                                                                                                                                     \\n                                                                                                                                                                                            \\nYear \\nDegree \\nInstitute \\n% / CGPA \\n2016-2020 \\nB.E in Computer \\nEngineering \\nArmy Institute of \\nTechnology, Pune \\n9.25 \\n2015-2016 \\n12th Grade \\nSt Patrick’s Junior College  \\n97% \\n2013-2014 \\n10th Grade \\nKendriya Vidyalaya No. 1 \\nUppal  \\n9.8 CGPA \\n \\nEXPERIENCE                                                                                                                                                           \\n1. Quantiphi                                                                                                    Mumbai (July-20-Present) \\nMachine Learning Engineer (Full time) \\n• \\nComputer Vision based Project involving Object detection, Image Classification. \\n• \\nImplementing Machine Learning Algorithms on Aws Cloud and deploying using Aws Sagemaker. \\n2. TIAA Global Services India                                                                                Pune (June 19-Aug 19) \\nIntern                                        \\n• \\nAsset Management System for Internal Management of resources. \\n• \\nMEAN Stack Application with various features for handling resources. \\n3. Big DataMatica                                                                                             Hyderabad (Dec 18-Jan 19) \\nData Science Intern \\n• \\nPlant Disease detection using convolutional neural networks. \\n• \\nBuilding chatbot with movie dialogues dataset. \\n• \\nDeploying Machine Learning Models on Cloud (Heroku, Google Cloud, Aws). \\n4. Rise Lab IIT Madras                                                                                        Chennai (May 18-July 18) \\nData Science Intern \\n• \\nExperimenting various Machine Learning Models on Bigdata. \\n• \\nExploratory Data Analysis on various Datasets & Interactive Dashboards using Metabase. \\n5. Accops                                                                                                                    Pune (Jan 18-Feb 18) \\nRemote Internship \\n• \\nIOT project using Temperature, Humidity Sensor, Node-MCU and Thingspeak. \\n \\n \\nTECHNICAL SKILLS                                                                                                                                                                         \\n• \\nProgramming Languages/Scripting Languages: Python, HTML, CSS \\n• \\nExperience in Pandas, NumPy, Scikit-learn, Keras, Linear models, Tree based algorithms, Clustering \\nalgorithms, Convolutional neural networks, Recurrent neural networks, Rasa Framework, Exploratory \\nData Analysis, Web-frame using Flask, MongoDB, MySQL, PostgreSQL, Angular, Node.js, TensorFlow. \\n• \\nCloud Platforms: Experience in AWS, Heroku, GCP, Microsoft Azure, Alibaba. \\n• \\nSoftware Packages: Jupyter Notebook, R-Studio, Git, GitHub, Photoshop. \\n• \\nPlatform: Linux, Windows. \\n \\n \\n \\nHONORS AND ACHIEVEMENTS                                                                                                                                                     \\n  \\n▪ \\nInter/Intra College Technical Fests: \\n1. First Runners up in Paper Presentation in PCCOE, Spectrum’17. \\n2. Second Runners up in Best Design for Smart City in COEP, Mindspark’17. \\n3. First Position in Idea Presentation and Technical Quiz in ZION’18. \\n4. First Position in PredictX (Machine Learning Kaggle Competition) in CEOP, Minspark’18. \\n5. First Position in AIT Ugcon Amalgam 2018,2019 Business Plan Competition. \\n6. First Position in Machine Hack (Machine Learning Competition) Metal Grade Prediction \\nChallenge and Video Games Sales Prediction Challenge. \\n7. Third in Machine Hack (Machine Learning Competition) Glass Quality Prediction Hackathon. \\n• \\nNational Level Hackathons: \\n1. Second Runners up in Nec Open Innovation Hackathon held at Delhi (2019). \\n2. First Position in Angular Hackathon conducted by Techgig (2019). \\n3. First Position in Infineon AI/ML Hackathon held at Bangalore (2019). \\n4. Second Runners up in Alibaba Cloud India Hackathon by Alibaba (2020). \\n5. First Runners up in Voice Based Payments Hackathon by NPCI (2020). \\n6. First Position in Alibaba Shapeup the Ecommerce with Technology (2020). \\n7. First Runners up in Store Transaction Imputation by Nielsen (2020). \\n8. First Runners up in Automated Multi-Label Text Classification by Times Internet (2020). \\n• \\nEconomic Times Campus Stars Class 2018-19: \\n▪ \\nSelected to the final list of India’s Largest and most coveted list of India’s brightest engineers.ET \\nCampus Stars is an initiative for recognizing and rewarding India’s brightest engineering \\nstudents. 91 students were awarded among a pool of 35,000+ applicants from over 2,000+ \\nengineering colleges in India. \\n• \\nBest Technical Performer (BE), 2019-20 at Army Institute of Technology, Pune. \\n   CERTIFICATIONS                                                                                                                                                                      \\n• \\nAWS Certified Solutions Architect – Associate \\n• \\nGoogle Cloud Certified Associate Cloud Engineer \\n \\nPERSONAL SUMMARY                                                                                                                                                                 \\n  \\nI am self-motivated person who believes in achieving the target with dedication and constant work. I will not \\nleave any work mid-way at any cost and will give the smallest of the tasks my best shot. I don’t leave any \\nstone unturned to grab opportunities that boost personality and make me a better individual. \\nLEADERSHIP                                                                                                                                                                                   \\n  \\n• FE and SE Technical Board member. \\n• Part of organizing team in Technical Aakriti’17, Solutions’17 and Solutions’18.   \\nINTERESTS                                                                                                                                                                                      \\n   \\n• My hobbies include playing Hockey, Football and running to refresh myself.  \\n• Interests include trying different types of dishes. \\n \\nREFERENCE                                                                                                                                                                                    \\n \\n• Dr. S R Dhore (HOD Computer Engineering Department), hodcomp@aitpune.edu.in \\n',\n",
       " 'Huzefa Lohawala\\nData Scientist, Bengaluru (IN)\\n� huzefa.lohawala.che14@iitbhu.ac.in\\n� LinkedIn\\n� Github\\n� +91-8602242352\\nWORK EXPERIENCE\\nAssociate Technical Lead\\nSep 2019 – Present\\nZycus Infotech, Bengaluru\\n◦ Currently implementing a quote validation engine using NLP techniques to extract details pertaining to the supplier and\\nthe items from quotes, and match them with their respective purchase requisition.\\n◦ Implemented search-based supplier recommendation engine using Elasticsearch and K-Nearest Neighbors Algorithm.\\n◦ Developed a supplier risk analysis engine to ﬂag suppliers facing bankruptcy, low credit ratings and suppliers who are\\nlaying oﬀ or furloughing their employees during the current pandemic.\\n◦ Implemented a 10-way email classiﬁcation algorithm using classical NLP techniques such as Word Sense Disambiguation\\nusing WordNet and Feature Extraction using Chi-Square Statistic.\\nTeaching Consultant - Data Science\\nAug 2020 – Present\\nupGrad, Bengaluru\\n◦ This a contractual position wherein I’m teaching students from upGrad’s B.Tech. and P.G. programs, fundamental\\nconcepts of Data Science.\\n◦ The curriculum includes training in SQL, Tableau, Python, it’s libraries essential for data science (such as Pandas, NumPy,\\nMatplotlib and seaborn), EDA, statistics and predictive analysis using scikit-learn.\\nProject Engineer\\nJun 2018 – Aug 2019\\nWipro Ltd, Bengaluru\\n◦ Implemented NLP techniques to extract key information from contracts, such as project start date, project end date, project\\ntype, termination clauses, warranty clauses etc.\\n◦ Used boosting to predict term days for collection using the invoice data. The end goal was to forecast days sales outstanding\\n(DSO) using the output from this model.\\n◦ Automated the process of computing loan advances for active and withdrawn employees. The automation reduced the\\neﬀorts of the Controllership team by 50 man-days.\\nEDUCATION\\nIndian Institute of Technology (BHU), Varanasi, India\\nJun 2014 – May 2018\\nB.Tech. in Chemical Engineering and Technology\\nDATASCIENCE HACKATHONS\\nInnoplexus Online Hiring Hackathon: Saving lives with AI (Analytics Vidhya)\\nMar 2019\\n◦ Created a CRF model to extract entities which were an indication towards a probable disease.\\n◦ Final leaderboard position was 1 out of 1998 participants.\\nLMG Analytics Data Science Hiring Challenge (Hackerearth)\\nNov 2018 – Dec 2018\\n◦ Used Gradient Boosting to predict whether or not an existing customer of a retail store will shop at it’s newly launched\\nstores.\\n◦ Final leaderboard position was 21 out of 4400 participants.\\nEricsson Foresight ML Hiring Challenge’19 (Hackerearth)\\nMay 2019\\n◦ Used Bidirectional LSTM to predict the material type of to-be-published research.\\n◦ Used NLP techniques to predict the overall rating for the review provided by a reviewer on a job portal.\\n◦ Final leaderboard position was 11 out of 2586 participants.\\nPractice Problem: Time Series (Analytics Vidhya)\\nOngoing\\n◦ Created a Time Series model using FbProphet to forecast customer footfall for a transportation ﬁrm.\\n◦ Current leaderboard position is 91 out of 12689 participants..\\nCOURSES & SPECIALIZATIONS\\nMachine Learning (Coursera)\\nIssued: Feb 2018\\n◦ Achieved a score of 96.9% for the course.\\nDeep Learning Specialization (Coursera)\\nIssued: May 2019\\n◦ Achieved an average score of 93.1% for the specialization.\\nTECHNICAL COMPETENCIES\\n◦ Tools: TensorFlow, Keras, PySpark, Elasticsearch, Docker, Tableau\\n◦ Interests: NLP, Time Series Analysis, Deep Learning, Statistics, Data Visualization\\n◦ Languages: Python, JAVA, MySQL, Bash\\n',\n",
       " \"Jaswinder Singh\\nLinkedin | jassican300@gmail.com | +[91] 7054122875\\nEDUCATION\\nIIT KANPUR\\nB.Tech in Chemical Engineering\\n2019 | CPI: 7.8 / 10\\nCOURSEWORK\\nProbability & Statistics\\nMachine Learning\\nComputational Methods in Engineering\\nIntroduction to Computing\\nNeural Network & Deep Learning\\nStructuring ML Projects\\nConvolutional Neural Network\\nSequence Models\\nPLATFORM STANDING\\nKaggle Notebooks, Highest Rank- 544\\nAnalytics Vidhya, Global Rank -170\\nMachineHack, Global Rank- 48\\nTECHNICAL SKILLS\\nSOFTWARES\\nJupyter Notebook | Matlab |\\nAdvance Excel | Tableau | Hadoop\\nLIBRARIES\\nNumpy | Pandas | Matplotlib | Seaborn |\\nSklearn | Keras | Pytorch | Tensorﬂow\\nPROGRAMMING LANGUAGES\\nPython | SQL | SAS |C | C++\\nACHIEVEMENTS\\nACADEMIC\\n99.97%ile in JEE MAINS’15\\nAIR-2382 in JEE ADVANCED’15\\nAIR-1097 in KVPY Scholar’14\\nHACKATHONS\\nRank 20-Demand Forecasting\\nRank 25-Customer Segmentation\\nRank 30-ML in Agriculture\\nRank 34-Healthcare Analytics II\\nRank 38-Topic Modeling I\\nSPORTS AWARDS / MEDALS\\nBest Outgoing Sportsperson’18\\nSports Performer of the Year’17\\nBest Incoming Sportsperson’15\\nInterIIT Medals- 2 Gold, 7 Silver\\nInter College Medals- 11 Gold, 7 Silver\\nWORK EXPERIENCE\\nEXL SERVICES, SPECIALIST DATA SCIENTIST\\nJUN'19-PRESENT\\nMODEL DEVELOPMENT TEAM - SCORECARD MONITORING AND MODIFICATION\\n• Working in a cross geographical team, building, monitoring and modifying models\\n• Monitored behavioral scorecard model for reliable out of time validation score\\n• Modiﬁed mobile lending application scorecard model from Logistic regression in SAS\\nto Random forest and Gradient boosting in python with complete automation.\\n• Performed reject inference with fuzzy augmentation and cutoff analysis.\\n• Results in 3% increase of Accuracy with roughly same predicted bad percentage.\\nMODEL DEVELOPMENT TEAM- PROBABILITY OF DEFAULT FORECASTING\\n• Performed exploratory data analysis and refreshed previous Linear regression models\\n• Analyzed macroeconomic factor along with their business impact in model building.\\n• Refreshed previous linear regression models and build new PD-12 linear regression\\nand Time series models in SAS with signiﬁcantly improved R-square\\nSTRATEGY IMPLEMENTATION AND INNOVATION TEAM\\n• Independently handle monthly audits and reporting of collection strategies across\\nmultiple clients that includes data manipulation using SQL, advanced Excel.\\n• Automated 3 Excel based strategies in Python for efﬁcient execution\\n• Initiated quarterly hackathon and manage complete organisation process.\\nPROJECTS\\nPRICE PREDICTION CHALLENGE , REGRESSION PROBLEM, MACHINE HACK\\n• Predicted house prices in India using address and 11 other factors to get least RMSLE\\n• Analyzed given data and extracted useful features such as city and locality name\\n• Generated features by Grouped frequency encoded and Mean encoded variables\\n• Achieved Rank 9, by using the ensembled model of XGBoost, LGBoost, and CatBoost\\nTOPIC MODELLING FOR RESEARCH ARTICLES\\nMULTILABEL CLASSIFICATION & NATURAL LANGUAGE PROCESSING, ANALYTICS VIDHYA\\n• Classiﬁed research articles of 4 different topics to 25 tags using text data\\n• Analyzed most occurring words with word cloud and countvectorizer\\n• Used models such as Logistic Regression, SVC, LSTM with onevsrest classiﬁer\\n• Achieved Rank 14, by tuning Threshold for max F1 Score with ensembling of models\\nCROSS-SELL PREDICTION , BINARY CLASSIFICATION, ANALYTICS VIDYA\\n• Predicted the interest of healthcare policyholders in Vehicle Insurance policy\\n• Generated multiple features and performed feature selection using LOFO\\n• Model used- Neural Network, Random Forest, & Advanced Grading Boosting\\n• Achieved Rank 24, using stratiﬁed K-fold by maximizing the AUC-ROC score\\nPOSITIONS OF RESPONSIBILITY\\nCAPTAIN, ATHLETICS TEAM, IIT KANPUR\\nJAN'18-DEC'18\\n• Spearheaded daily practice sessions of 50 player with coordination of 3 coaches\\n• Organized Student Athlete Program sessions for all round development of team\\n• Led team to Gold in Udghosh’18 (IITK), and Bronze Medal in InterIIT’18 (IITG)\\n1\\n\",\n",
       " \"                                                                                                     \\n1 \\n \\n____________________________________________________________________________________ \\n \\n                                                                    JYOTI SHARMA \\nMOBILE: 425-829-8544 \\n                                EMAIL:  erjyoti@gmail.com   \\n                                LINKEDIN: https://www.linkedin.com/in/jyotisharma1978/      \\n____________________________________________________________________________________ \\n \\nSUMMARY \\n\\uf0b7 \\nAround 16+ years of extensive IT experience in Database, Business Intelligence Technologies, software \\ndevelopment developing Web, Windows client-server architecture. \\n\\uf0b7 \\nProficient at building & maintaining analytics infrastructure, maintaining scalable data pipelines, and \\ndeveloping & testing architecture for data generation. Highly skilled in creating data set processes and \\naccelerating the performance of ETL processes. \\n\\uf0b7 \\nProficient in data mining & data visualization to deliver compelling business value to clients & successfully \\nexecute projects. \\n\\uf0b7 \\nAdept at performing deep dive for gaining actionable insights to benefit key stakeholders & facilitate sound \\ndecision-making.  \\n\\uf0b7 \\nProficient in developing complex machine learning and statistical modeling algorithms/techniques for \\nidentifying patterns and extracting valuable insights for key stakeholders and leadership. \\n\\uf0b7 \\nWorking knowledge in Agile/Scrum Environments. \\n\\uf0b7 \\nGreat management and People skills. \\n\\uf0b7 \\nPursuing Post Graduate Diploma in Management. \\n\\uf0b7 \\nPursuing Master in Machine Learning & AI. \\n \\nKEY SKILLS: \\nBusiness Intelligence | Data Analytics | Data Visualization | Inferential Statistics | Hypothesis Testing & A/B Testing | \\nData Modelling | Trend Analysis | Quantitative Analysis | Process Optimization & Development | Predictive Modelling | \\nSentiment Analysis | ML Algorithms | Model Development | Exceptional Interpersonal communication | Efficient multi-\\nTasker | Customer service-oriented | Deadline- oriented | Project Management  \\n \\nEDUCATION: \\n\\uf0b7 \\nBachelor of Tech. and Engineering (Computer Science)-Kurukshetra University \\n\\uf0b7 \\nPost Graduate Diploma, Machine Learning & AI–International Inst. Of Information Technology, IIITB  \\n\\uf0b7 \\nPursuing Masters in Machine Learning and AI (2019-2021)- Liverpool John Moore’s University \\n\\uf0b7 \\nCertified Scrum Master (Scrum Alliance) \\n\\uf0b7 \\nPursing Post Graduate Diploma, Management – IMT \\n \\nKEY ML/AI PYTHON PROJECTS: \\n\\uf0b7 \\nTELECOM CHURN: Help a telecom company to Predict telecom customers likely to churn with more than 90% \\naccuracy by analyzing 7000+ customers data, identifying the best models out of KNN, Naïve Bayes, \\nlogistic, Decision Tree, Random Forest, and SVM Model. Used exploratory data analytics to identify the \\nfeatures that convey whether the customer will churn.  \\n\\uf0b7 \\nCAR PRICING: The predicted price of the car with more than 80% accuracy on a small dataset using multiple \\nlinear regression. Also provided the feature importance by analyzing the features which impact the car \\nprices. \\n\\uf0b7 \\nRASA CHATBOT: Implemented the chat Bot using the rasa framework to provide the best restaurants within the \\nmentioned budgets, in an area for an asked cuisine.    \\n\\uf0b7 \\nGESTURE RECOGNITION: Gesture recognition using Deep Learning techniques for smart television. Tried 3D \\nCNN, LSTM, and GRU \\n\\uf0b7 \\nCREDIT APPROVAL MODEL: Predicted the likelihood of approval of credit card customer applications with \\naccuracy by building and evaluating them using Logistic Regression and Decision Tree models \\n \\nPROJECT DETAILS: \\nCompany: Teksystems Inc. USA                                                                                                 Jun 2018- Till Date \\nPROJECT – Office 365 CXP FastTrack \\nClient - Microsoft \\n                                                                                                     \\n2 \\n \\nDescription:  Office 365 is a set of cloud services available on a subscription basis from Microsoft. FastTrack Team \\nhelps the tenant onboarding and user adoption resources and guidance. \\nResponsibilities: Working with cross-functional teams, collaborate with external partners/stakeholders to drive core \\ninsights, trend analysis assisting with data collection, reporting, and A/B test development, and help drive customer \\nsatisfaction.  \\n\\uf0b7 \\nOversee the design and delivery of reports and insights that analyze business functions and key \\noperations and performance metrics. \\n\\uf0b7 \\nResponsible for translating business requirements into specifications to implement the required reports \\nand dashboards, from multiple data sources. \\n\\uf0b7 \\nProviding technical assistance and cross-training to other team members.  \\n\\uf0b7 \\nAdvanced data modeling and Statistical Analysis performing hypothesis Testing, A/B testing using T-\\nTest to analyze different designs and scenarios.  \\n\\uf0b7 \\nSubject matter expert in Setup Guide Wizards Usage for all Office related Service data, Automation \\nTags, and BOT Data. \\n\\uf0b7 \\nInvolved in trend analysis, variance analysis, findings, insights, and working with stakeholders to \\narticulate what actions are being taken in each scenario to drive or improve performance. \\n\\uf0b7 \\nCreated power pivot, PowerBI reports for data analysis based on the Tabular Model. Providing the ad \\nhoc reports as per different requirements from stakeholders. \\n\\uf0b7 \\nCreated NLP model on sentiment analysis specific to the feedbacks/ratings given by different users. \\nEnvironment: Visual Studio 2015/Visual Studio 2017, Team Foundation Server, GIT, SQL Azure, SQL Server \\nManagement Studio 2017, SSIS, TSQL, Power Pivot, Tabular Model, DAX, Power BI, Azure Data Explorer (Kusto), \\nPython  \\n \\nCompany: Inspur, USA                                                                                                                  Mar 2018- Jun 2018 \\nPROJECT – MSAnalytics \\nClient - Microsoft \\nResponsibilities: \\n\\uf0b7 \\nCreated POWER BI Visualizations and Dashboards as per the business requirements. \\n\\uf0b7 \\nCreated effective reports using visualizations such as Bar chart, Clustered Column Chart, Waterfall \\nChart, Gauge, Pie Chart, Treemap, etc. in POWER BI \\n\\uf0b7 \\nCreated Roles and implemented Row-level security for user Authentication in POWER BI \\n\\uf0b7 \\nImplemented logic to mask the data as per the GDPR compliance.  \\n\\uf0b7 \\nWorked on Data Analysis Expressions (DAX) for accessing data directly from the tabular SSAS \\ndatabase. \\n\\uf0b7 \\nImplemented the changes in the tabular model including the new tables/views and creating measures in \\nDAX. \\n\\uf0b7 \\nProvided POC to embed the POWER BI Reports in the Web application using C# REST API. \\n\\uf0b7 \\nWritten PowerShell script and scheduled a batch process to get the updated data from Share Point to \\nSQL Server Database.  \\nEnvironment: Visual Studio 2017, SQL Server 2016, SSIS, TSQL, POWER BI, Power Pivot, Tabular Model, DAX, \\nKusto, Azure Data Lake \\n \\nCompany: HCL America Inc., USA                                                                                               Feb 2016- Mar 2018 \\nPROJECT – Office 365 CXP FastTrack \\nRole – Technical Lead           \\nClient - Microsoft \\nDescription:  Office 365 is a set of cloud services available on a subscription basis from Microsoft. FastTrack Team \\nhelps the tenant onboarding and user adoption resources and guidance. \\nResponsibilities: \\n\\uf0b7 \\nInvolved in the creation of facts, dimensions, and star schema representation for the Datawarehouse. \\n\\uf0b7 \\nWriting and updating the stored procedures, Views, SQL scripts for performing analysis of data quality \\nas per business requirement. \\n\\uf0b7 \\nInvolved in daily batch loads (Full & Incremental) into Staging and Data Warehouse, troubleshooting \\nprocess, issues, and errors using complex Queries and stored procedures. \\n\\uf0b7 \\nInvolved in developing recommendation engine which provides different recommendations on Office \\nportal dashboard.  \\n\\uf0b7 \\nInvolved in data analyzing, data validation, and data cleanup. \\n\\uf0b7 \\nCreated power pivot reports for data analysis based on the Tabular Model. Providing ad-hoc reports \\nbased on different stakeholders. Design Percentage, Month over Month, and Year over Year measures \\n                                                                                                     \\n3 \\n \\nafter importing data into Pivot.  \\n\\uf0b7 \\nCreated the Data Pipelines in Azure Data Factory that gets and updates the data for each Tenant from \\nSql OnPrem to Azure SQL Storage Table. Installed On-premise data gateway and scheduled daily data \\nrefresh in Pipeline. \\n\\uf0b7 \\nResponsible for providing queries from SQL Azure database related to wizard usage based on various \\nparameters selected by tenants on wizards for analytical purposes. \\nEnvironment: Visual Studio 2015/Visual Studio 2017, C#, ASP.net, AngularJS, Source Depot, GIT, SQL Azure, SQL \\nServer 2012/2016, SSIS, TSQL, Typescript, JavaScript, CSS, PowerBI, Power Pivot, Tabular Model, DAX \\n \\nCompany: Walmart, Canada                                                                                                  Aug 2011- Nov 2015 \\nPROJECT – CrossCap MMS \\nRole – Technical Lead               \\nDescription: Reporting solution which gets the item files provides by Crosscap and generates the weekly Sales \\nbased on the Date Range provided in the file. Also, to generate weekly item file and create Category File for that \\nweek. \\nResponsibilities: \\n\\uf0b7 \\nInteracted with Business Users, analyzed user requirements, and Created technical design document \\nbased on the Requirement. \\n\\uf0b7 \\nInvolved in Data Modeling to develop the database design and created database diagram in MS Visio \\nand SQL Server Management Studio (SSMS) \\n\\uf0b7 \\nDesigned and implemented a variety of SSRS reports such as Parameterized, Drill Down, Snapshot, \\nCached, Drill Through, Ad-hoc, and Sub-Reports using Report Designer and Report Builder based on \\nthe business requirements using both Tabular and Matrix report formats. \\n\\uf0b7 \\nWorked on several types of Chart Reports such as Lines, Columns, Bars with both 2D and 2D views to \\ndisplay cumulative totals based on the day-to-day requirement. \\n\\uf0b7 \\nDesigned complex T-SQL queries, User Defined Functions \\n\\uf0b7 \\nStored Procedures and Triggers followed by thorough analysis and testing of those database objects \\nbefore deployment to the production server. \\n\\uf0b7 \\nMaintained Change Control and Release Management process for all database objects like Tables, \\nViews, Procedures, and Triggers.  \\n\\uf0b7 \\nCreated an ETL Process using SSIS that will get the file from sFTP and generate various .xlsx files \\nbased on the items and date range provided in the files for that week. \\n\\uf0b7 \\nThe process includes creating a weekly item file Report and upload the file on sFTP. \\n\\uf0b7 \\nProvided POS Sales for different items in different date ranges. \\n\\uf0b7 \\nArchive the inbound and outbound files in a specific directory using the SSIS package. \\nEnvironment: SQL Server 2012, Enterprise Edition, SQL Server Business Intelligence Development Studio (SSIS, \\nSSRS), Teradata, Team Forge. \\n \\nPROJECT – Yearly Bonus \\nDescription: Web-based Analytics tool used by Compensation Team, Store Managers, and Home Office Associates \\nto view the Bonus eligibility. The compensation Team can view the report and can make necessary changes to \\ncalculate the Bonus amount and generate the file for the payroll team. \\nResponsibilities: \\n\\uf0b7 \\nInteracted with Business Users, analyzed user requirements, and Created technical design document \\nbased on the Requirement. \\n\\uf0b7 \\nCreated the program specifications based on the technical design document. \\n\\uf0b7 \\nDesigned and developed ETL Process to import data from Teradata and HR Data Warehouse and \\ntransform and load data into SQL Server database \\n\\uf0b7 \\nInvolved in complete SSIS life cycle in creating SSIS packages, building, deploying, and executing the \\npackages in both the environments (Development and Production) \\n\\uf0b7 \\nMonitored performance and optimized SQL queries; and created and modified T-SQL stored \\nprocedures.  \\n\\uf0b7 \\nCreated Database and Database Objects like Tables, Stored Procedures, Views, Triggers, Rules, \\nDefaults user-defined data types and functions.  \\n\\uf0b7 \\nCreated UI for the associates to login and view the reports for any employer and generate the Bonus \\nletter. \\n\\uf0b7 \\nCreated web pages for Store Manager/Home Office Associates to login and view based on their access \\nrights. \\nEnvironment: MS SQL Server 2008, SQL Server Business Intelligence Development Studio, PeopleSoft, DSN2, \\nVisual Studio 2010, ASP.NET, C#, Team Forge, HP Quality center, TSQL. \\n \\n                                                                                                     \\n4 \\n \\nPROJECT – People Analytics \\nDescription: This is an analytical web-based tool for the Talent Management Technology Team as a part of \\ncentralizing and automating the Talent Management. \\nResponsibilities: \\n\\uf0b7 \\nInteracted with Business Users, analyzed user requirements, and Created technical design document \\nbased on the Requirement. \\n\\uf0b7 \\nCreated and reviewed the program specifications based on the technical design document. \\n\\uf0b7 \\nETL Process to extract data from PeopleSoft (DB2) and Teradata. \\n\\uf0b7 \\nConducted and automated the ETL operations to Extract data from multiple data sources, transform \\ninconsistent and missing data to consistent and reliable data, and finally load the data. \\n\\uf0b7 \\nCreated Dashboard to show different reports based on different matrix-like Headcount, Termination, \\nPromotion, Demotion, Performance matrix, etc in different regions, markets, and stored. \\n\\uf0b7 \\nDeveloped reports using SSRS with subreports, dynamic sorting, defining data source and subtotals for \\nthe report. \\n\\uf0b7 \\nCreating Stored Procedures to handle complex calculations and wrote logic to maintain the purging and \\nhistory of data. \\n\\uf0b7 \\nCode Review of the offshore team. \\n\\uf0b7 \\nUnit testing for the modules implemented by other team members.  \\nEnvironment: SSIS, MS SQL Server 2008, Teradata, PeopleSoft, Visual Studio 2010, ASP.NET, C#, Team Forge, \\nHP Quality Center, TSQL. \\n \\nPROJECT – Manitoba Holiday Pay \\nDescription: This is a web-based project for Canada Payroll to support them to generate the payroll file for the store \\nassociates who work in Manitoba and are entitled to the Holiday Pay. \\nPayroll associates can view the report based on store, WIN, First and Last Name along with the hours and pay for all \\nthe holidays in Manitoba. \\nResponsibilities: \\n\\uf0b7 \\nCreated Technical design document and program specification based on the requirement. \\n\\uf0b7 \\nDesigned all the modules based on the requirement using ASP.Net, C#, SQL Server 2008. \\n\\uf0b7 \\nImplemented SSIS Package that will get the hours worked for all the associates working in stores in \\nManitoba and perform the calculation based on Manitoba labor regulations. \\n\\uf0b7 \\nOnce the calculation is done for the hours/pay the file will get generated and put on sFTP wherein the \\nPayroll team will download and process the file.  \\n\\uf0b7 \\nCreated reports related to Associates store, First and Last Name, Holiday, Hours/Pay  \\nEnvironment: SSIS, MS SQL Server 2008, DSN2, Teradata, Visual Studio 2010, Team Forge, HP Quality center, \\nTSQL. \\n \\nPROJECT – SumTotal LMS \\nDescription: This is a backend project for Walmart Canada wherein data files will be generated and send to the \\nSumTotal (third party vendor) providing e-learning courses.  \\nResponsibilities: \\n\\uf0b7 \\nResponsible for the determination of the requirements of the module and apply those requirements. \\n\\uf0b7 \\nCreated technical documentation of the project based on the Business Requirement document. \\n\\uf0b7 \\nWritten program specifications for generating the files to upload it on the sFTP server along with the \\nDelta feeds to be sent daily. \\n\\uf0b7 \\nConstructed ETL Process using SSIS packages that get the data from PeopleSoft and manipulates as \\nper SumTotal Requirement and then generate the files and upload it to Sum Total sFTP. Schedule the \\nprocess to run daily. \\n\\uf0b7 \\nDesigned and developed the Triggers, Functions, and Stored procedures. \\nEnvironment: SSIS, MS SQL Server 2008, Team Forge, HP Quality center, TSQL. \\n \\nPROJECT – BIMS \\nDescription: The project is being used by both store's home office associates to create and resolve the issues \\nrelated to stores. Managing tickets based on Department group and sending email notifications was a major \\nfunctionality of this project. One ticket will be created, an email will be sent to the concerned department group \\nregarding the description of the issue. If the ticket has been created and is unassigned for more than 24hrs, an email \\nwill be sent to the category manager and to the associates associated with that group. For all overdue tickets which \\nhave been created more than 48hrs and are still unresolved, an email will be sent to the category manager with \\ndetails of who has been assigned to the ticket. Store associates can search the ticket to find out the status of the \\nticket. \\n                                                                                                     \\n5 \\n \\nResponsibilities: \\n  \\n\\uf0b7 \\nInvolved in Requirements gathering, Conceptual Design, Analysis, and Detail design, Development, and \\nSystem Testing. \\n\\uf0b7 \\nDeveloped core functionality with the .NET Framework. \\n\\uf0b7 \\nImplemented Agile methodology for the development of the application. \\n\\uf0b7 \\nDatabase designing with the implementation of stored procedures.  \\n\\uf0b7 \\nExpertise in creating complex Stored Procedures, DTS packages, triggers, cursors, tables, views, and \\nother SQL joins and statements for applications.  \\n\\uf0b7 \\nCreated ETL process that gets the updated data related to the store, department, category, etc. \\n\\uf0b7 \\nDesigned and developed SSIS Packages (ETL) to import data from Teradata and SQL Server sources \\n(heterogeneous sources) and transform and load data into SQL Server database \\n\\uf0b7 \\nInvolved in complete SSIS life cycle in creating SSIS packages, building, deploying, and executing the \\npackages in both the environments (Development and Production) \\n\\uf0b7 \\nImplemented Event Handlers and Error Handling in SSIS packages \\n\\uf0b7 \\nAdvanced extensible reporting skills using SQL Server Reporting Services (SSRS). \\n\\uf0b7 \\nUsed SSIS for each loop to implement data updates from various sources to the data warehouse. \\n\\uf0b7 \\nScheduled jobs to run daily and along with sending emails to the category manager if any ticket is \\noverdue.  \\n\\uf0b7 \\nUsed Visual Team Foundation Server for version control, source control, and reporting. \\n\\uf0b7 \\nFixing bugs. \\nEnvironment: SQL Server, SQL Server Business Intelligence Development Studio (SSIS, SSRS), .Net framework, \\nMicrosoft Office Share Point Server, XML, MS Visual Source Safe and Windows Server 2008, Visual Studio \\n2008/2010, C#, Entity Framework. \\n \\nCompany: The Marketing Store, Canada                                                                                   Sep 2010- Aug 2011 \\nPROJECT – Nissan CRM \\nRole - Application Developer                   \\nDescription: This is a multilingual web-based project for Nissan North America dealership for Corporate as well as \\ndealer level users. \\nResponsibilities: \\n\\uf0b7 \\nCreated technical documents for the performance Review Report, RO Analysis report, Inner Circle, \\nOuter Circle, Express Service.  \\n\\uf0b7 \\nProvided Production support along with new development based on the change request. \\n\\uf0b7 \\nCreated class diagrams and sequence diagrams using Visio. \\n\\uf0b7 \\nSolely responsible for the implementation of code and designing database based on the requirement \\ndocument.  \\n\\uf0b7 \\nCreated reports related to the customer invoice, Coupon redemption for the dealer and corporate users \\nusing grid view Control. Also written code to export the report to excel sheet. \\n\\uf0b7 \\nWritten stored procedures and queries to get the data related to reports.  \\n\\uf0b7 \\nCreated SQL jobs related to reporting data. \\n\\uf0b7 \\nResponsible for creating test/production build. \\n\\uf0b7 \\nFixing bugs related to all modules. \\nEnvironment: \\nASP.Net 3.5, C#, .Net Framework 3.5, Visual Studio 2008, JavaScript, MS SQL Server 2008, Windows XP \\nProfessional, Team Foundation Server- 2010, T-SQL, Visio, Microsoft Enterprise Library.                                  \\n \\nCompany: Mobiroo Inc., Canada                                                                                               Oct 2009 – Sep 2010                                       \\nPROJECT – Mobiroo \\nRole: Software programmer/Lead \\n                    \\nDescription: Web-based Mobile application that is used for the promotional industry to download the apps from \\nBlackberry. These downloaded apps will have customized branded banner ads and splash pages. The company \\nplaces the brand within paid apps as a banner ad and/or a landing page. These branded apps can then be given \\naway as promotional items to the customers in the form of customizable prepaid branded app cards. Apart from this it \\nalso allows users that do not have a Blackberry to download songs instead of an app. \\nResponsibilities: \\n\\uf0b7 \\nDesigned technical document of the Website along with the Mobile application. \\n\\uf0b7 \\nDesigned program specifications for all the modules like Admin, Distributor, Developer, and Mobile \\nApplication and created UI and business logic layer using Asp.Net and C#.Net. \\n                                                                                                     \\n6 \\n \\n\\uf0b7 \\nWritten code for blackberry and Android as well as unit testing, technical analysis, debugging, and \\nintegration. \\n\\uf0b7 \\nCreated Tracking reports using ASP.Net server controls using C# and JavaScript. Also created \\ndistributor and customer reports using SSRS. \\n\\uf0b7 \\nWritten SQL Queries, Stored Procedures, Views, and User-defined functions in SQL Server 2005 and \\nSQL Server 2008 using T-SQL. \\n\\uf0b7 \\nThe transition of the solution and database from the old server to the new server and from Visual Studio \\n2005 to Visual Studio 2008 version. \\n\\uf0b7 \\nCreated web service for sending dynamic banners to the mobile application. \\n\\uf0b7 \\nPerforming root cause analysis and rectified issues accordingly. \\n\\uf0b7 \\nDeployment on the server for each build. \\nEnvironment: \\nASP.Net 2.0/3.5, C#, .Net Framework 2.0/3.5, Visual Studio 2005/2008, JavaScript, MS SQL Server 2005/2008, \\nWindows XP Professional, TortoiseSVN, T-SQL, SSRS, SSIS. \\n \\nCompany: Summitworks Technology Inc., USA                                                                    Mar 2008 – Aug 2009                                \\nPROJECT – CORE \\nClient: Infosys, USA      \\nRole: Software Lead                                                     \\n                           \\nDescription: CORE is a web-based project being implemented by Infosys together with Health ways Inc \\nResponsibilities: \\n\\uf0b7 \\nDesigned documents of the Core including Sequence and Deployment Diagram.  \\n\\uf0b7 \\nPlayed a major role in data migration for the Activity, Membership, and Enrollment for the Forever Fit, \\nSilver Sneakers, and Prime Products. It includes Terminated Enrollees, Active Enrollees, and True \\nguest. Also contributed to functional design and specification, table mapping, and program coding for \\nthe data extraction. \\n\\uf0b7 \\nMigrated the data related to the swipe system from legacy to the new system. \\n\\uf0b7 \\nRequirement gathering, data cleansing, de-duplication, program coding to extract data from the legacy \\nsystem and provided various audit reports for data verification using SSIS. \\n\\uf0b7 \\nCreated scripts in T-SQL to migrate the data using SSIS. \\n\\uf0b7 \\nPerformance tuning of the application by doing modifications in SQL Server 2005. \\n\\uf0b7 \\nCreated UI layer and written business logic layer using ASP.Net and C#.Net as code behind for Activity \\nModule. Consumed WCF service in the Payment and Contract Module. \\n\\uf0b7 \\nPerformed code reviews, database design, and unit testing. \\n\\uf0b7 \\nCreated Reports using Crystal Report for Payment and Location modules. \\n\\uf0b7 \\nResponsible for the entire archival project for Legacy Core System.  \\n Environment: \\nASP.Net 3.0, C#, .Net Framework 3.0, WCF, Visual Studio 2005/2008, JavaScript, MS SQL Server 2005, Windows \\nXP Professional, Clear Case, SQL Server Integration Services 2005(SSIS), T-SQL, MS-Access, VBA, Visual Source \\nSafe, Crystal Reports. \\n \\nCompany: LogicaCMG Pvt. Ltd                                                                                             Aug, 2004 – Jan, 2008                                \\nPROJECT – i-Net \\nRole: IT Consultant \\nClient: Netherlands Government                                                                               \\nResponsibilities: \\n\\uf0b7 Window-based application for the Minister of affairs department. \\n\\uf0b7 Implementation of Database Design and Writing Stored Procedures using SQL Server 2000. \\n\\uf0b7 Handled AMF module by creating designing documents and implementing use case diagrams using C#. \\n\\uf0b7 Written various classes and used style sheets to maintain the uniformity in look and feel. \\n\\uf0b7 Maintain existing code modules as well as design and implement new Modules to face new challenges. \\n\\uf0b7 Implemented a security module based on rights assigned to the user. \\n\\uf0b7 Designed data assess layer using ADO.Net datasets & data adapters. \\n\\uf0b7 Used FxCop for validating .NET code and nDoc to generate documentation. \\nEnvironment: C#, Windows-based Forms, .Net Framework 2.0/1.1, WinForms, ADO.Net, MS SQL Server 2000, \\nApplication Data Blocks, Windows XP Professional, Infragistics Controls, FxCop, nDoc. \\n \\nPROJECT – FCSC \\nClient: UK Government                                   \\n                \\n \\n \\nRole: IT Consultant \\nResponsibilities: \\n                                                                                                     \\n7 \\n \\n\\uf0b7 \\nLow-level designing for the modules implemented. \\n\\uf0b7 \\nBatch Payment, Batch Printing and Registration Module. \\n\\uf0b7 \\nCreated tables, views, and relations using SQL Enterprise Manager. \\n\\uf0b7 \\nQuerying the database tables to obtain the required results. \\n\\uf0b7 \\nBuild Prototypes for both client and server applications in C#. \\n\\uf0b7 \\nCreated a Windows Service in C# which would act as a request listener. \\n\\uf0b7 \\nDesign XML (using XSD) to be used at the Server / Client side for storing terminal configuration details \\nand application process state. \\nEnvironment: ASP.Net 1.1, C#.net, VB.Net Framework 1.1, ADO.Net, MS SQL Server 2000, Visual Source Safe \\n6.0, IIS, Windows XP Professional       \\n \\nTECHNICAL SKILLS \\n \\nLanguages \\nASP.Net, C#, VB.Net, ADO.Net, Visual Basic 6.0/5, C++, XML, Python \\nDatabases \\nMS-SQL/T-SQL, SQL Azure, MS Access, MySQL, Teradata, ORM: ADO.Net Entity \\nFramework, Tabular, SSAS, SSIS, DAX, Azure Data Factory, Cosmos \\nPackages \\nSciKit-Learn, NumPy, SciPy, Plot.ly, Pandas, NLTK, Beautiful Soup, Matplotlib, Stats \\nModels \\nStatistics/ML/AI \\nLinear/Logistic Regression, SVM, Ensemble Trees, Random Forests, Clustering, Gradient \\nBoosted Trees, Neural Networks, Deep Learning, NLP \\nREPORTS \\nSQL Reporting Services (SSRS), Crystal Reports, Power Pivot, Power BI, Tableau \\nInternet Technologies \\nASP.Net, HTML, XHTML, SOAP, WCF, WPF, VBScript, JavaScript, Active Server Pages, \\nWCF, WPF, PowerShell \\nOperating Systems \\nMS-DOS, Windows- 95/98, Windows NT 4.0, Windows 2000, Windows XP \\nVersion Control \\nClearCase, Tortoise svn, Visual Studio Team Foundation Server \\nDevelopment IDE \\nVisual Studio 6.0, Visual Studio.Net 2017/2015/2008/2005/2003/2002, MS Interdev 6.0, \\nOthers \\n.Net Framework 3.0/2.0/1.1, Visio, ClearCase, ClearQuest, Microsoft Data Application \\nBlocks, SourceSafe, VBA, Web Forms, Win Forms, ODBC, ADO, DAO, ADO.Net, LINQ, \\nMultithreading, Remoting, XML Web Services, ActiveX, COM, IIS, Win32 API, T-SQL, \\nKusto, Infragistics Controls, Telerik Controls, Axure RP Pro \\n \\nCERTIFICATIONS: \\n\\uf0b7 \\nMCAD CERTIFICATION  \\n70-315 Developing and Implementing Web Applications with Microsoft Visual C# .NET and Microsoft Visual \\nStudio .NET \\n70-316 Developing and Implementing Windows-based Applications with Microsoft Visual C# .NET and \\nMicrosoft Visual Studio .NET \\n70-320 Developing XML Web Services and Server Components with Microsoft Visual C# .NET and the \\nMicrosoft .NET Framework. \\n\\uf0b7 \\nCertification in Analyzing and Visualizing Data with Power BI- Powered by Microsoft. \\n\\uf0b7 \\nStatistics Essentials for Data Science – Simplilearn \\n\\uf0b7 \\nData Visualization with Python – IBM Cognitive Class \\n\\uf0b7 \\nPython for Data Science – IBM Cognitive Class \\n\\uf0b7 \\nPython for Data Science – Simplilearn \\n\\uf0b7 \\nTableau Desktop 10 - Simplilearn \\n\\uf0b7 \\nA-Z Machine Learning using Azure Machine Learning (AzureML) – Udemy \\n\\uf0b7 \\nProgramming with Python - Simplilearn \\n\\uf0b7 \\nMaster DAX Fundamentals: Power BI, Power Pivot & SSAS – Udemy \\n\\uf0b7 \\nMachine Learning Advanced Certification Training - Simplilearn \\n\\uf0b7 \\nNeural Networks and Deep Learning – Coursera \\n\\uf0b7 \\nDeep Learning Interface with Azure ML Studio – Coursera \\n\\uf0b7 \\nIntroduction to TensorFlow for Artificial, Machine Learning, and Deep Learning – Coursera \\n\\uf0b7 \\nMachine Learning with Python – Coursera \\n                                                                                                     \\n8 \\n \\n\\uf0b7 \\nCertified Scrum Master  - Scrum Alliance \\n \\n \\n\",\n",
       " 'K Krishna Chaitanya\\nM.Sc.(Integrated) Physics, IIT Kanpur\\nMobile No. +91 8105442793\\nAlt. Mobile No. +91 9491418897\\nEmailid – kkchaitu27@gmail.com\\n                                                                                                                                                                \\nExperience Summary       \\n \\n                                                                                                                     \\n➢ Have 9+ years of experience in IT industry including 7+ years of experience in Data \\nSciences.\\n➢ Worked on developing data science solutions for low-latency systems in Adtech domain for \\nmore than 4.5+years.\\n➢ Exceptional ability to translate abstract business problems to Data Science Solutions.\\n➢ Provided scalable data science solutions to opportunities available in data available with the \\ncompanies.\\n➢ Lead a team of 2 members and worked as a key point of contact for data sciences.\\n➢ Helped companies to improve their revenue by detecting patterns in data available with \\nthem.\\n➢ Worked on projects that ensure brand safety of companies that advertise.\\n➢ Practical knowledge in Python, R, Scala, Java, MySql, Spark, AWS, Docker, Aerospike, \\nHTML, CSS etc.\\n➢ Proven knowledge in Machine Learning, Deep Learning, Artificial Engineering, Data \\nAnalysis, etc.\\n➢ Have Certifications in Algorithms, Data Sciences, Deep Learning and Scalable Machine \\nLearning.\\n                                                                                                                                                                \\nCertifications     \\n \\n                                                                                                                                     \\n\\uf0d8 Deep Learning Nano Degree  - Udacity\\n\\uf0d8 Scalable Machine Learning, University of California, Berkeley – Edx\\n\\uf0d8 Big Data Analytics and Optimization from INSOFE, Hyderabad\\n\\uf0d8 Statistical Learning, Stanford University\\n\\uf0d8 Data Analytics and Statistical Inference, Duke University – Coursera\\n\\uf0d8 Algorithmic Toolbox, University of California San Diego – Coursera\\n\\uf0d8 Data Structures, University of California San Diego – Coursera\\n\\uf0d8 Algorithms on Graphs, University of California San Diego – Coursera\\n\\uf0d8 Natural Language Processing Specialization,  Deeplearnig.AI – Coursera\\n                                                                                                                                                                \\nData Science and Software Skills \\n \\n                                                                                                        \\n\\uf0d8 Python, R, Java, Scala, HTML and CSS\\n\\uf0d8 Big Data - Hadoop, Pig, Hive, and Spark\\n\\uf0d8 Deep Learning, Machine Learning, Statistics, Probability, and Data Analysis\\n                                                                                                                                                                \\nPositions of Responsibility             \\n \\n                                                                                                       \\n\\uf0d8 Team Lead – Airpush India Pvt. Ltd., Xangars Solutions Pvt. Ltd.\\n\\uf0d8 Coordinator – Vivekananda Samiti, Students Gymkhana, IIT Kanpur\\n                                                                                                                                                                \\nAchievements         \\n \\n                                                                                                                                \\n\\uf0d8 Competitions Expert on Kaggle\\n\\uf0d8 KVPY Scholar, Indian Institute of Science, Bangalore\\n\\uf0d8 NIUS Scholar, HBCSE and TIFR, Mumbai\\n\\uf0d8 Undergraduate Associate, Saha Institute of Nuclear Physics, Kolkata\\n                                                                                                                                                                \\nProfessional Experience                                                                                                                       \\nPokkt                                                                                                                   Mar 19 – Present\\n➢ Click Through Rate Prediction – Predicted and ranked hundreds of advertisement \\ncampaigns so that maximum revenue is achieved in the bidding process using Python, and \\nSpark generating thousands of dollars revenue..\\n➢ Conversion Rate Prediction – Built machine learning models for predicting conversion \\nrate of conversion based advertisement campaigns to increase overall revenue.\\n➢ Timeout Determination Model – Architected and implemented adative timeout \\ndetermination model for Demand Side Platform(DSP) for Pokkt.\\n➢ Bid Call Decision Process – Architected and implemented decision process to call third \\nparty Demand Side Platform(DSP).\\nAirpush                                                                                      Oct14 – Apr16 and Feb17– Aug17\\n➢ Gaming Algorithm Applied to CTR Prediction – Used Xbox’s Gaming algorithm \\nTrueSkill Predictor to match players that are close on skill based on their playing history and\\nrank players based on the players they have played with in the context of CTR prediction in \\nadvertising domain.\\n➢ CTR and Conversion Rate Prediction – Worked on different algorithms for predicting \\nclicks and conversions that happen in Mobile advertisement domain.\\n➢ Fraud Detection – Detecting Fraud that happens at request level, impression level, click \\nlevel and app level that happens in Advertising Domain\\n➢ Discerning validity of patterns in data. For eg. Does users who installed sports apps have \\nmore click through rate when sports advertisements are shown to them.\\n➢ Adult Content Detection Model – for preventing adult content to Kids Apps using deep \\nlearning techniques for Airpush to ensure brand safety for advertisers.\\nArimaResearch                                                                                                  Nov17 – Mar19\\n➢ Worked as consultand and helped Arima Research for doing various POCs.\\n➢ Sentiment Classification, Ranking different Resumes with reference to Resume query, \\nClustering various resumes based on content of resumes.\\nPersonagraph                                                                                                     Apr16 – Dec16\\n➢ User profiling – Worked on User profiling extracting User demographics and interests from \\napps installed in one’s mobile.\\nXangars Solutions Pvt. Ltd.                                                                               Apr14 – Oct14\\n➢ Have done Sales Forecasting for revenue of top 50 stores of Aditya Birla’s more stores in \\nBengaluru.\\nPlanetSoft(Now Ebix)                                                                                         Dec11 – Nov13\\n➢ Worked in a core framework team which is the basis for various products that get built in the\\ncompany. It comprises of Google Web Toolkit. Primarily used Java, HTML and CSS.\\nGitHub Profile - https://github.com/kkchaitu27\\nLinkedin Profile - www.linkedin.com/in/kkchaitu27\\nKaggle Profile - https://www.kaggle.com/kkrishnachaitanya\\n',\n",
       " 'KRISHNA PRIYA \\nEmail: krishnapriyakejriwal@gmail.com                                                                               GitHub: https://github.com/krishnapriya-18 \\nMobile No: +917856048599                                                                                 LinkedIn: https://www.linkedin.com/in/krishnapriya18 \\n \\nEDUCATION:                                                                                                                                                             \\n \\nIndian Institute of Technology, Roorkee, India - M.Tech in Geophysical Technology                                  June 2015 – May 2020 \\n• \\nCGPA: 7.76 out of 10 \\n \\nDelhi Public School, Siliguri, India (CBSE)                                                                                                     May 2010 – Apr. 2014 \\n• \\nHigher School Certificate (Class 12 - Science): 81.4 out of 100 \\n• \\nSecondary School Certificate (Class 10): 9 out of 10 \\n \\nWORK EXPERIENCE: \\n \\nData Science Associate | ZS Associates – Bengaluru, India                                                                            Jul. 2020 – Present \\n \\n• \\nNamed Entity Recognition (NLP): Developed a weak supervised named entity recognition framework for carpet industry products. The \\nmodel achieved an F1-Score of 78 percent. Modules: Pandas, Numpy, spaCy, Snorkel \\n \\n• \\nSemantic Textual Similarity (NLP): Developed a framework for predicting similar clinical trials using unsupervised, weak supervised and \\ntransfer learning methods. Deployed the model in a DataBricks production environment. Achieved an F1-Score of 81 percent for weak \\nsupervised (Snorkel) Model and 91.5 percent for supervised transfer learning (Roberta) Model.  \\nModules: spaCy, scispacy, K-Means, Snorkel, Pytorch, Transformers, Pyspark, DataBricks \\n \\n• \\nMulti-Channel Marketing Touchpoint Optimization (Mixed Integer Non-Linear Programming): Optimized the number of times a Health \\nCare Professional (HCP) should be contacted through a marketing channel, to help a pharmaceutical company market their retail and non-\\nretail brands. Modules: Pandas, Numpy, Pyspark, PuLP, DataBricks \\n \\nData Science Intern | Happay – Bengaluru, India                                                                                             May 2019 – July 2019 \\n \\n• \\nCustomer Review Analysis (NLP): Web Scraping + Sentiment analysis of reviews from Google play store, Apple app store and G2 \\nCrowd. Generated insights on Happay’s shortcomings and their competitor’s weakness which led to better marketing pitch of their product.  \\nModules: Beautiful Soup, Selenium, TFIDF, Word2vec, Random Forest \\n \\n• \\nPython Interactive Dashboard (Analytics): Development of Analytics Dashboard for Visualization and Exploration of employee level \\nInsights. Modules: Dash, Plotly, HTML, CSS \\n \\n• \\nMarketing Campaign Click through rate prediction (Predictive Modelling: Classification): Suggested important insights on campaign \\nrunning-time and day, banner positions for display campaigns, etc. Modules: Matplotlib, Dask, Xgboost \\n \\n• \\nMulti-Class Text Classiﬁcation (NLP): Classiﬁcation of employee expenses from the reimbursement bills under predefined categories like \\ntravel, food, hotel etc. Modules: K-Means, TFIDF, Word2vec, XGBoost \\n \\n• \\nMulti-Channel Marketing Attribution Analysis (Optimization): Developed a workﬂow for analysing and predicting marketing channel that \\nleads to better conversions, worked on various approaches including First Touch, Last Touch, Linear, and Markov Models. Delivered \\ninsights on which model is better for the company’s business. \\n \\nMachine Learning Research Intern | CSIR - National Institute of Oceanography – Goa, India                  May 2018 – July 2018 \\n• \\nDeveloped an end to end machine learning framework to identify rock types (facies) under the earth surface. Transformed the problem into \\na machine learning multiclass classification with features based on wireline log measurements of the wells at a fixed depth interval. The \\nfeatures used were radioactivity, porosity, photoelectric effect, sound velocity, resistivity, depth and relative position. Oversampling \\ntechniques like SMOTE and ADASYN together with a random forest model resulted in a weighted F1-Score of 88 percent. \\nModules: Pandas, Numpy, Random Forest, Matplotlib, Imblearn \\n \\nAWARDS / DATA SCIENCE HACKATHONS: \\n  \\n Top 1% (Rank 7 out of 776) - AirQo Ugandan Air Quality Forecast Challenge (Zindi)                                                June 2020 \\n• \\nProblem Statement: Predict future air quality levels and empower communities to plan and protect their health. (Forecasting) \\n \\n Winner (Rank 1 out of 216) - COVID-19 Tweet Classification Challenge (Microsoft)                                                       May 2020 \\n• \\nProblem Statement: Develop a machine learning model to assess if a Twitter post is about COVID-19 or not. (NLP) \\n Winner (Rank 2 out of 272) - Akeed Restaurant Recommendation Hackathon (Zindi)                                                    May 2020 \\n• \\nProblem Statement: Build a recommendation engine to predict what restaurants customers are most likely to order from given \\nthe customer location, restaurant information, and the customer order history. (Predictive Analytics: Classification) \\n Winner (Rank 3 out of 314) - The Zimnat Insurance Assurance Challenge (Zindi)                                                          May 2020 \\n• \\nProblem Statement: Develop a predictive model that determines the likelihood for an insurance customer to churn - to seek an \\nalternative insurer or simply drop out of the insurance market altogether. (Predictive Analytics: Classification) \\n Top 0.5% (Rank 9 out of 2555) - A Data Science Hackathon by Bain & Company (Analytics Vidhya)                            Apr 2020 \\n• \\nProblem Statement: Time Series Sales Forecasting. (Multivariable - Multiple Time Series) \\n Top 0.5% (Rank 24 out of 6300) - LTFS Data Science FinHack 2 (Analytics Vidhya)                                                        Jan 2020 \\n• \\nBusiness Forecasting: Predict number of loan applications on a daily basis for different loan segments. (Regression) \\n Winner (Rank 5 out of 650) - Chartbusters Prediction: Foretell the Popularity of Songs (MACHINE HACK)                 Jan 2020 \\n• \\nProblem Statement: Predict how popular a song will be in future. (Predictive Modelling: Regression) \\n Winner (Rank 2 out of 1400) - Predicting Food Delivery Time - Hackathon by IMS Proschool (MACHINE HACK)       Nov 2019 \\n• \\nProblem Statement: (Predictive Modelling – Multiclass Classification) \\n Top 0.5% (Rank 11 out of 7000) - National Finalist - ZS Data Science Challenge 2019 (InterviewBit)                           July 2019 \\n• \\nReceived a PPO – Pre-Placement Offer for Data Science Associate Role. \\n• \\nProblem Statement: Probability of Ronaldo scoring a goal, Classification of tweets (Healthcare Professional or not) \\n \\nSKILLS: \\n \\n• \\nLanguages / Tools / Databases: Python, SQL, PySpark, DataBricks \\n• \\nWeb Scraping: BeautifulSoup, Selenium \\n• \\nData Pre-Processing and Visualization: Pandas, Numpy, Matplotlib, Seaborn, Plotly, Dash \\n• \\nMachine Learning / Deep Learning: Scikit-learn, Pytorch, Keras, Regression, Classiﬁcation, Clustering, Bagging, Boosting \\n• \\nNatural Language Processing: TF-IDF, Word2vec, Gensim, Transformers, spaCy, NLTK \\n• \\nWeak Supervised Learning: Snorkel \\n• \\nOptimization (Linear and Non-Linear Programming): PuLP, Scipy, Gekko \\n',\n",
       " '                                               MANASI SINGH \\n8130921709/manasisingh11@gmail.com \\n \\nAcademic Qualifications  \\nCourse Name  \\nCollege/School/University \\nYear of Passing \\nMarks \\nObtained (%) \\nMA Economics  \\nDelhi School of Economics, \\nDelhi University, New Delhi \\n2017 \\n63.4 \\nBA(HONS)   \\nEconomics \\nSri Venkateswara College, Delhi \\nUniversity, New Delhi \\n2015 \\n82.84 \\n12th Class  \\nArmy Public School, Dhaula \\nKuan, New Delhi \\n2012 \\n93.4 \\n10th Class  \\nHansraj Public School, \\nPanchkula \\n2010 \\n95 \\n \\nWork Experience \\n• \\nData Scientist at Fractal Analytics                                                      ( July 2017 – Feb 2020) \\no Growth Driver Analysis for a leading CPG firm  \\n▪ \\nIdentified the Key Performance Indicators (KPIs) \\n▪ \\nUsed Bayesian Belief Networks to leverage direct and indirect relationships, \\ncapturing lead-lag relationships, objectively making trade off decisions between \\nmarkets, functions and categories and making informed decisions at the correct \\ngranular level \\no Financial data forecasting for a leading CPG firm \\n▪ \\nForecasted key indicators such as GSV, TTS, Turnover  \\n▪ \\nBuilt a scalable machine forecasting architecture with approximately 25 models \\nachieving accuracies of greater than 90% on a consistent basis \\n▪ \\nUsed techniques like ARIMA, GARCH, ETS, Prophet, XGBoost, Bayesian etc \\no Automation of model generation for Growth Driver Analysis \\n▪ \\nAutomated the model generation process using Bayesian Belief Networks  \\n▪ \\nIncreased efficiency, eliminated manual modelling errors and achieved a \\nsignificant decrease in the time taken to generate models (From 3-5 hours to \\napproximately 1 hour) \\n \\nSkills & Certifications \\n• \\nTools: Proficient in R, Python, SQL, Stata, MS Word, MS Excel, MS PowerPoint \\n• \\nTechniques: Supervised learning techniques (Linear, Logistic, LDA, QDA, KNN, Bayesian \\nBelief Networks, SVM), Unsupervised learning techniques (PCA, K Means), Forecasting \\ntechniques (ARIMA, GARCH, ETS, Prophet), Decision Trees, Random Forest, XGBoost, \\nBagging, Boosting and Ensemble learning \\n• \\nCertifications: Practical Time Series Analysis by The State University of New \\nYork (Coursera), Neural Networks and Deep Learning by deeplearning.ai (Coursera), \\nStructuring Machine Learning Projects by deeplearning.ai (Coursera), Improving Deep \\nNeural Networks: Hyperparameter tuning, Regularization and Optimization by \\ndeeplearning.ai (Coursera) \\n \\nExtra-Curricular Activities & Academic Projects \\n• \\nFinal Year MA Project on estimating poverty lines for rural and urban India through the        \\nuse of the Linear Expenditure System                                                                                  2017                \\n',\n",
       " \"\\uf0e1 https://www.linkedin.com/in/nandini-\\nb-b4baaa178/\\n\\uf09b http://www.github.com/nandinib1999\\n\\uf0e0 nandinib1811@gmail.com\\nKEY SKILLS\\nPython\\nDeep Learning\\nTensorFlow\\nMachine Learning\\nMySQL\\nNLP\\nHTML\\nCSS\\nJavaScript\\nDjango\\nFirebase\\nSOCIAL LINKS\\nVOLUNTEERING\\nContent Writer for Women in ML \\nand Data Science, Delhi. \\nResponsible for curating the content \\nthat goes on social media platforms, \\ncreating posters for various events, \\nand blogs about different events that \\nare conducted.\\nVolunteer at Social Library ASU\\nIt is an initiative to provide the under-\\nprivileged hostel helps and workers \\nwithin the university premises with an \\nequal right to education. We \\nconducted classes to teach basic \\nEnglish and maths.\\nOPEN SOURCE\\nParticipant | Girl Script Summer of \\nCode\\nCERTIFICATIONS\\nPROFESSIONAL EXPERIENCE\\nRAPIDKEN.AI\\nJan '21- Present\\nAI ENGINEER\\nWork From Home\\nRAPIDKEN.AI\\nMay '20- Jan '21\\nSOFTWARE ENGINEER INTERN\\nSCIENTIA INNOVATION PVT LTD\\nFeb '20- May '20\\nSOFTWARE ENGINEERING INTERN\\nWork From Home\\nTHIRD EYE INC.\\nJun '19- Feb '20\\nDATA SCIENCE INTERN\\nNoida\\nEDUCATION\\nPOSTGRADUATE PROGRAM IN ARTIFICIAL \\nINTELLIGENCE AND MACHINE LEARNING\\nMay '20- Present\\nNIT Warangal\\nPost-Graduation\\nB.TECH CSE\\nAug '17- Present\\nApeejay Stya University\\nGurgaon\\nBachelor\\nGPA: 4.21/4.3\\nNandini Bansal\\nAI ENGINEER\\nKaggle \\nhttps://www.kaggle.com/nandini19\\n99\\nMedium \\nhttps://nandinibansal1811.mediu\\nm.com/\\nCodeChef \\nhttps://www.codechef.com/users/n\\nandinib1999\\nLeetCode \\nhttps://leetcode.com/nandinib1999\\n/\\nHealth Check - Added a new sub-\\nproject of Fetal Health \\nClassification\\nNeoAlgo - Added minimum \\nparenthesis removal & playfair \\ncipher algorithm \\nIntroduction to Deep Learning with \\nPyTorch | Udacity | Bertelsmann \\nAI Track Scholarship | November \\nDesigning rigorous algorithms for the task of extracting candidate phrases from the \\ntext documents and performed excessive processing on the text. These algorithms \\nhave reduced the bad looking or less meaningful candidates phrases by 10%.\\nWorking with supervised and unsupervised algorithms for linking similar documents. \\nWorked with BiLSTM-CRF and Sentence Transformers to develop a model for \\nkeyphrase extraction and document similarity. \\nFinetuned transformer models and worked on state-of-the-art methods such as \\nDAPT, TAPT to generate more meaningful embeddings of the text documents.\\nDesigned custom loss and evaluation functions for the embeddings generation.\\nDeveloped a prototype solution for automated evaluation of student answer sheet \\nusing Google Vision API and Mask RCNN.\\nAlso worked on fine-tuning the BERT for the STS task.\\nIdentified opportunities and derived insights through the use of algorithmic and \\nstatistical data mining & visualization techniques. \\nBuild predictive models and machine-learning algorithms and combine models \\nthrough ensemble modelling, and produce results. \\nExecute analytical methods while conveying the results to technical and business \\nteams.\\nCoursework includes Predictive Analytics, ML, CNN, RNN, Sequence Models, NLP, GANs \\nand Reinforcement Learning.\\nWorked on various projects such as Emotion Detection using CNN, Handwritten text \\nrecognition, Denoising the images using autoencoders, etc.\\nMember of Institutional Innovation Council (IIC) by AICTE & MHRD\\nFounding member & secretary of the technical club, Proxima - organized various \\nworkshops, seminars & hackathons as a part of it.\\nDr Stya Paul Memorial Scholarship Recipient \\nMember of Annual Technical Fest Organizers of ASU - TechShield\\nPROJECTS\\nDiabetic Retinopathy Severity Detection \\nusing EfficientNet & Attention Layer\\nApr '21- Present\\nhttps://github.com/nandinib1999/aptos19-diabetic-retinopathy\\nAs a part of the GSSOC'21 Health Check project, I am working on this project. The dataset \\nused was obtained from Kaggle. It consists of fundus scans of the eyes which are \\ncategorized into 5 classes based on the severity of DR. The dataset is highly imbalanced. \\nI have so far created a baseline model with EfficientNetB5 with a validation accuracy of \\n0.8449. I am currently working on improving the performance of the model further by \\nadding the Attention layer and TTA.\\nChest Abnormalities Detection using \\nXRay Scans and Tensorflow Object \\nDetection\\nJul '20 - Present\\nhttps://github.com/nandinib1999/vinbigdata-chest-abnormalities\\nAn automated system that could accurately identify and localize findings on chest \\nradiographs would relieve the stress of busy doctors while also providing patients with a \\nmore accurate diagnosis. In this project, I have worked with a dataset of 18000 diacom \\nscans to automatically localize and classify 14 types of thoracic abnormalities. For \\nlocalization & classification, I have used Tensorflow Object Detection API.\\nQuotes Generation using GPT2 Language \\nModeling\\nNov '20- Feb '21\\nhttps://github.com/nandinib1999/gpt2_quotes_generation | https://huggingface.co/nandinib1999/quote-\\ngenerator\\nUsing the language modelling scripts provided by HuggingFace, I finetuned the GPT2 \\nmodel on a custom dataset of inspirational quotes to synthetically generate quotes. I used \\nGoogle Colab's GPU for the fine-tuning task and the perplexity of the model after the first \\nepoch was 15. I have deployed the model on the HuggingFace model hub.\\nCar License Plate Scanner\\nMay '20-\\nhttps://github.com/nandinib1999/license-plate-scanner\\nUsing darknet Yolo, I have trained a custom YOLO object detector for detecting a car \\nlicense plate. The dataset used is a combination of the Kaggle dataset and some images \\nscraped from the internet. Image processing techniques are applied to the detected \\nlicense plate to extract the text using PyTesseract.\\nFake or Real Tweet using BERT & \\nTensorFlow\\nMar '20-\\nhttps://www.kaggle.com/nandini1999/fake-or-real-tweet-bert-nlp\\nThere are countless sources of fake news nowadays that continue to spread false \\ninformation 24/7. Social Media has become one of the biggest platforms to spread such \\nlies. Using the BERT model, I trained a model over a dataset of disaster tweets to identify \\nwhich one is fake. The model achieved a public score of 0.83 on the Kaggle Leaderboard.\\n2019 – March 2020\\nPL/SQL Oracle Certified Associate \\n| Oracle | September 2019 \\nPractical Machine Learning with \\nTensorflow | IIT Madras | Google | \\nAugust 2019 – November 2019\\nMachine Learning | Stanford \\nUniversity | May 2019 – August \\n2019\\nThe Joy of Computing with Python \\n| IIT Madras & IIT Ropar | August \\n2018 – November 2018\\n\",\n",
       " 'NITHILAA U\\n18PD22\\nFather’s name\\nGender\\nDate of Birth\\nLanguages known\\nEmail\\nMobile\\nUma Sankar E\\nFemale\\n28th September 2000\\nEnglish, Tamil, Telugu\\nnithilaau28@gmail.com\\n+91-89408-02277\\nPermanent Address\\n11, Kavitha Layout,\\nIswarya Nagar, Extn,\\nUdumalpet,\\nTamil Nadu – 642154.\\nOBJECTIVE\\nTo obtain a position as a Student Intern for a period of six months from May\\n2021 to November 2021.\\nACADEMIC QUALIFICATION\\nCurrently pursuing 3rd year of 5-year Integrated M.Sc. Data Science at the\\nDepartment of Applied Mathematics and Computational Sciences at PSG College of Technology.\\nSKILL SET\\nLanguages\\nPython, C++, R, C\\nBack-End\\nOracle, SQL\\nPlatform\\nWindows, Linux\\nTools\\nTableau\\nAREAS OF INTEREST\\n●\\nSupervised and Unsupervised learning\\n●\\nProbability and Statistics\\n●\\nData Visualization\\nACADEMIC RECORD\\nCourse\\nInstitution\\nBoard/University\\nCompletion By\\nMarks\\nM.Sc.\\nPSG College Of Technology,\\nCoimbatore.\\nAnna University\\n2023\\n8.49\\nXII\\nSrinivasa Vidhyalaya Mat. Hr.\\nSec. School, Udumalpet\\nState Board\\n2018\\n93.25%\\nX\\nDISHA - A Life School, Pollachi\\nICSE\\n2016\\n93%\\nINDUSTRY BASED PROJECT EXPERIENCE\\nChamber of Products (August 2020 - September 2020) - Machine learning intern\\nWorked on developing an efficient text summarizer for any given news article\\nwith different text summarizing techniques like TFIDF, Gensim, and found BERT to be more\\nefficient. Also, created a simple website to demonstrate the proper working of the summarizer\\ncreated.\\nNON-ACADEMIC PROJECT\\nDesignable (September 2020 - Till date) - Software Designing Intern\\nWorked with PSG alumni in creating a product development software. The\\nwebsite aims in exploring the market survey, by formulating the hypothesis of a particular\\nproblem, finding the user and the competitor of the product using SWOT analysis. It has a\\nsurvey template, a product roadmap, and a dashboard to visualize the market segmentation\\ncriteria. The front end of the website was developed using HTML5, CSS, and JavaScript, and the\\nback end using Flask.\\nACADEMIC PROJECTS\\n●\\nStockLock, the study of the impact of COVID-19 in Indian Stock Market and GDP Response,\\na research paper, implemented in Python to study and analyze effects of a pandemic on\\nthe stock market using various ML regression algorithms like linear, multiple linear,\\npolynomial, and SVM regression, and statistical models of ARIMA.\\n●\\nSmartTweet, a Twitter sentiment analysis tool, developed using Python, which classifies\\nthe tweets according to the sentiment expressed in them - positive, negative, or neutral\\nusing the open-source python package, Tweepy to get the Twitter data. The Naive\\nBayesian model has been deployed to classify the tweets, along with graphical analysis.\\n●\\nH1B-VisualBay, Analysis of H1B visa approval, developed using Python, using the H1B visa\\npetitions data from 2011 to 2016, from Kaggle. This project helps to find out which\\nlocations, employers, job titles and salary range makes up most of the H1B petitions and\\nhelps in graphical analysis of data using lollipop, donut plot, and violin plots.\\n●\\nHealry, a website hosted on google drive about COVID-19, to study the various metrics and\\nKPIs using Google Analytics. The website aims in creating awareness about the COVID-19\\npandemic through blogs and self-check quiz and helps in visualizing the real-time data\\nusing interactive and live COVID-19 updates. The front end was developed using HTML5,\\nCSS, and JavaScript.\\n●\\nSoccerStats, Analysis of international football results from 1872 to 2019, developed using\\nR. This project presents interesting results for football lovers by analyzing head-to-head\\nbalances in football games and which countries are currently showing the greatest\\nprogress in official football games, using various plots and data visualization techniques.\\nEXTRA-CURRICULAR ACTIVITIES AND ACHIEVEMENTS\\n●\\nCertified by Coursera for the completion of the courses Deep Learning and Neural\\nNetworks And Deep Learning offered by deeplearning.ai.\\n●\\nCertified by Coursera for the completion of the course Exploratory Data Analysis offered\\nby Johns Hopkins University.\\n●\\nKaggle contributor.\\n●\\nWinner (1st), District (Coimbatore) Level Drawing Competition, hosted by the Tamilnadu\\nForest Department.\\n●\\nParticipant, State (Tamilnadu) level drawing competition, hosted by the Electricity Board\\nOf India.\\n●\\nWas a part of the Entrepreneur Club, PSG College Of Technology.\\nDECLARATION\\nI, Nithilaa U, do hereby confirm that the information given above is true to the\\nbest of my knowledge.\\nPlace: Coimbatore\\nDate : 03/05/2021\\n(Nithilaa U)\\n',\n",
       " 'Omkar Pathak\\nSOFTWARE ENGINEER · FULL STACK PYTHON DEVELOPER\\nPune, Maharashtra, India\\n\\uf10b (+91) 8087996634\\n|\\n\\uf0e0 omkarpathak27@gmail.com\\n|\\n\\uf015 www.omkarpathak.in\\n|\\n\\uf092 OmkarPathak\\n|\\n\\uf08c omkar-pathak-94473811b\\n“Make the change that you want to see in the world.”\\nExperience\\nSchlumberger\\nPune, Maharashtra, India\\nDATA ENGINEER\\nJuly 2018 - Present\\n• Responsible for implementing and managing an end-to-end CI/CD Pipeline with custom validations for Informatica migrations which\\nbrought migration time to 1.5 hours from 9 hours without any manual intervention\\n• Enhancing, auditing and maintaining custom data ingestion framework that ingest around 1TB of data each day to over 70 business\\nunits\\n• Working with L3 developer team to ensure the discussed Scrum PBI’s are delivered on time for data ingestions\\n• Planning and Executing QA and Production Release Cycle activities\\nTruso\\nPune, Maharashtra, India\\nFULL STACK DEVELOPER INTERN\\nJune 2018 - July 2018\\n• Created RESTful apis\\n• Tried my hands on Angular 5/6\\n• Was responsible for Django backend development\\nPropeluss\\nPune, Maharashtra, India\\nDATA ENGINEERING INTERN\\nOctober 2017 - January 2018\\n• Wrote various automation scripts to scrape data from various websites.\\n• Applied Natural Language Processing to articles scraped from the internet to extract different entities in these articles using entity\\nextraction algorithms and applying Machine Learning to classify these articles.\\n• Also applied KNN with LSA for extracting relevant tags for various startups based on their works.\\nGeeksForGeeks\\nPune, Maharashtra, India\\nTECHNICAL CONTENT WRITER\\nJuly 2017 - September 2017\\n• Published 4 articles for the topics such as Data Structures and Algorithms and Python\\nSofttestlab Technologies\\nPune, Maharashtra, India\\nWEB DEVELOPER INTERN\\nJune 2017 - July 2017\\n• Was responsible for creating an internal project for the company using PHP and Laravel for testing purposes\\n• Worked on a live project for creating closure reports using PHP and Excel\\nProjects\\nPyresparser\\nAPI/Python Package\\nPERSONAL PROJECT\\nJuly 2019 - Present\\n• A simple resume parser used for extracting information from resumes\\n• Extract information from thousands of resumes in just a few seconds\\n• Author and maintainer of this project\\nGarbage Level Monitoring System\\nIoT\\nTEAM PROJECT\\nOctober 2017 - May 2018\\n• To find a economical and smarter alternative to current garbage problems\\n• Users can monitor levels of all garbage bins from a global dashboard provided\\n• Was responsible for Django backend development\\nNOVEMBER 3, 2019\\nOMKAR PATHAK · RÉSUMÉ\\n1\\nPygorithm\\nAPI / Python Package\\nPERSONAL PROJECT\\nJuly 2017 - Present\\n• Author and maintainer of this project\\n• An educational library to teach all the major algorithms\\n• Got covered in Fosstack, FullStackFeed, Kleiber and Tagged under Hotest Github Project on ITCodeMonkey\\nSmart Surveillance System using Raspberry Pi and Face Recognition\\nIoT\\nPERSONAL PROJECT\\nJanuary 2017 - February 2017\\n• Face Recognition using OpenCV and Python\\n• Raspberry Pi was used as the data server\\n• User notified if any suspicious activity detected in real time\\nPassword Strength Evaluator using Machine Learning\\nMachine Learning\\nPERSONAL PROJECT\\nMarch 2017\\n• SVM algorithm used for training and classification\\n• Flask framework used\\n• Self-generated dataset\\nEducation\\nMarathwada Mitra Mandal’s College of Engineering\\nPune, Maharashtra, India\\nB.E. IN COMPUTER ENGINEERING\\n2014 - 2018\\n• Aggregate 74%\\nSkills\\nProgramming Languages:\\nPython, C, PHP, C++, Shell Script\\nFrontend Technologies:\\nHTML, CSS, JavaScript, Angular 6/7\\nBackend Technologies:\\nDjango, Flask (Python), Laravel (PHP)\\nOperating Systems:\\nLinux, Unix, Windows\\nDatabases:\\nMySQL, SQLite, MongoDB\\nOther:\\nGit, NLP, Scikit-Learn, OpenCV, Cloud (GCP, Azure, DigitalOcean)\\nHonors & Awards\\n2018\\nTop rated Python developer, in Pune and Fifth in India at Github\\nIndia\\n2018\\nQuora Top Writer,\\nIndia\\n2018\\nAwarded ‘The Best Outgoing Student Award 2017-18’,\\nMMCOE, Pune\\n2018\\nWon 2nd Prize, in an Hackathon organized by MIT-ADT Persona Fest 2018\\nPune\\n2018\\nBest Paper Award, in National Level Conference on “Emerging Trends in Computing , Analytics\\nand Security - 2018”(NCETCAS-2018)\\nMMCOE, Pune\\nExtracurricular Activities\\nContributor in Pune PyCon 2018\\nPUNE, MAHARASHTRA, INDIA\\n2018\\n• Was a part of Website Designing and volunteering committee\\nNOVEMBER 3, 2019\\nOMKAR PATHAK · RÉSUMÉ\\n2\\nMentor at GirlScript Summer of Code 2019\\nPUNE, MAHARASHTRA, INDIA\\n2019\\n• Mentored 4+ teams in various domains\\nOrganizing head for the National level technical event -\\nInnovatus\\nPUNE, MAHARASHTRA, INDIA\\n2018\\n• Organized project competitions\\nWorkshop on IoT and Python\\nMMCOE, PUNE\\n10 Jan 2017\\n• Conducted a workshop for second year students to give them a brief overview about IoT by completing three mini projects and taught\\nthem basics of Python programming language\\nPublications\\nSmart Surveillance System using Raspberry Pi and Face\\nRecognition\\nDOI10.17148/IJARCCE.2017.64117\\nGarbage Level Monitoring System\\nInterests\\n• Competitive Programming\\n• Photography\\n• Sketching\\n• Reading/Writing on Quora\\n• Contributing to Open Source projects\\nNOVEMBER 3, 2019\\nOMKAR PATHAK · RÉSUMÉ\\n3\\n',\n",
       " 'Prashant Arora\\nMachine Learning Enthusiast\\nMentoring machines to perform better .\\nprashantarora998@gmail.com\\n08630831390\\nAmroha, Delhi, India\\nlinkedin.com/in/prashant-arora-3063b5155\\ngithub.com/neyoxdrago\\nWORK EXPERIENCE\\nMachine Learning Engineer\\nMyWays\\n07/2020 - 08/2020, \\nWork From Home\\nTask was to debug some existing codes of recommendation systems.\\nTeaching Assistant\\nCoding Blocks\\n05/2020 - Present, \\nWork From Home\\nClearing Doubts of students raised during online and oﬄine classes .\\nMachine Learning Intern\\nAnalysed.in\\n07/2019 - 10/2019, \\nWork From Home\\nUsing Natural Language Processing to build a resume builder . Working\\non analysis of different kinds of resumes.\\nMachine Learning Intern\\nAvanov solutions\\n04/2019 - 06/2019, \\nWorking on different computer vision problems like person counter\\nsystem , detecting users mood , detecting damages on vehicles,etc.\\nEDUCATION\\nBachelor of Science ( Hons. ) Computer\\nScience\\nKeshav Mahavidyalaya , University Of Delhi\\n2017 - 2020, \\nCLASS XII\\nDelhi Public School , Moradabad\\n2017, \\n91%\\nCLASS X\\nDelhi Public School , Moradabad\\n2015, \\n9.4 CGPA\\nSKILLS\\nMachine learning\\nPython\\nDeep Learning\\nComputer Vision\\nNatural Language Processing\\nMySQL\\nFlask\\nData Analytics\\ndjango\\nPERSONAL PROJECTS\\nDetecting Brain Tumor with Brain MRI scans\\n (08/2019 - 08/2019)\\nDetermining the probability of brain tumor to be predicted on the\\nbasis of brain MRI scans.\\nEmail Answering Bot (07/2019 - 07/2019)\\nA bot to automatically reply all new unread emails with a deﬁned\\nformat and text.\\nSkin Lesion detection to predict skin cancer\\n (06/2019 - 06/2019)\\nDetecting and segmenting Skin lesions through skin images for\\npredicting skin cancer\\nSentiment Analysis of Movie reviews\\n (05/2019 - 05/2019)\\nClassifying user review as positive , neutral or negative .\\nAchieved 90+ % accuracy.\\nACHIEVEMENTS\\nGlobal Rank 17 on Machine Hack.\\nLANGUAGES\\nEnglish\\nProfessional Working Proﬁciency\\nHindi\\nFull Professional Proﬁciency\\nINTERESTS / HOBBIES\\nReading\\nPlaying Football\\nTravelling\\n',\n",
       " 'RAJAT RANJAN \\nMobile No: +919886043959 (IN) \\n+61432028099 (AU) \\n \\n \\nEmail: rajat.ranjan24@gmail.com \\nCurrent Location: Melbourne AU \\n \\n \\n \\n \\n \\nObjective: \\nTo pursue a challenging career and be a part of progressive organization that gives a scope \\nto enhance my knowledge and utilizing my skills towards the growth of the organization and \\nself. \\n \\nProfessional Summary \\n \\nExperienced System Engineer with a demonstrated history of working in the information \\ntechnology and services/product industry. Skilled in J2EE Web Services, Python, React, Web \\nDevelopment and MySQL Database along with insights in Machine Learning. \\nStrong and productive in Front end Applications. Experience in agile as well as waterfall \\nmodels along with CI/CD using bamboo with a background in Banking Industry. Machine \\nlearning with experience in data wrangling, analytics and pre-processing and gaining insights \\nusing data visualisations with keen to build model that best fits the problem statement. \\n \\n \\nExperience Summary: \\n1. Deputee : ANZ (Melbourne AU) (Nov, 19 – Present) \\nDesignation: Developer (Australia) \\n \\nCompany: EdgeVerve (An Infosys Company) \\nDesignation: Senior Systems Engineer (Pune) \\n \\nProjects \\n1. Distributed and loosely coupled application development of Internet banking \\nusing microservices. Maintaining release activities and restful API contracts. \\n2. Development in React/Redux, Spring boot, J2EE applications, python scripting, \\ntest API automation. \\n3. Evaluate effective risk management on the borrower’s overall ability to repay \\nloan with domain specific knowledge and insights using customer data with \\nvisualisations. \\n4. Analytics based solution for Customer Churn by recommending customer centric \\nproducts using recommendation engines. Dashboarding done in React JS with \\nbackend data from model pipeline in Python. Documented findings. \\n5. Data analysis of used services in Banking industry, its feasibility and optimisation \\nfor new use cases. \\n \\n \\n2. Designation: Senior Systems Engineer (Pune, India) \\nCompany: EdgeVerve (An Infosys Company) (Nov, 16 – Nov,19) \\n \\nProjects \\n \\n1. Customisation in Finacle Product development for ANZ internet banking. \\nDeveloping microservices architecture in Java application and maintaining SQL \\nscripts for release activities. \\n2. Performing Unit test, UAT for newly added services and creating proof of concept \\nto integrate applications with third party services. \\n3. Information Extraction of important clauses in an ISDA document and to develop \\npredictive modelling so as to reduce human effort. More than 60% of human \\neffort was reduced and the combined accuracy of trained models skyrocketed to \\n77% for selected clauses. \\n4. Maintaining JIRA and updating user stories for better understanding of \\nrequirements aligned with Agile work flow. \\n \\n \\n3. Company: Infosys Limited (Jun, 16 – Nov, 16) \\nDesignation: Systems Engineer Trainee (Mysore, India) \\n \\nProjects \\n \\n1. Web Based customer friendly clothing E commerce website using Angular, JAVA \\nand web services as a full stack developer incorporation Agile workflow. \\n2. Python terminal Application for Cricket tournament Management system \\nintegrated with MySQL (OOP Framework) \\n3. Learning new technologies with Python, Machine learning and participation in \\nhackathons. \\nCore Competencies: \\n \\n \\n \\nTechnologies \\nPython, Flask, Keras, TensorFlow, Data Pre Processing, Data \\nManipulation, Data Visualisation, Classification & Regression, \\nPredictive Modelling, Natural Language Processing, \\nCompetitive \\nProgramming, Microservices, Java, Gradle, React JS,Redux \\nDatabases \\nMy SQL , Mongo DB \\nTools \\nJupyter Notebook, Anaconda, Postman, JIRA, PyCharm, IntelliJ \\nIdea, MS Office, VS studio code \\nOthers \\nGIT, Jenkins, Bamboo, GitHub, Bitbucket, JIRA \\nFunctional \\nClient Engagement, Showcase activities, Sprint Planning and \\nreview, Retrospectives, Creative Thinking, Design thinking. \\n \\n \\n \\n \\n \\n \\nAwards & Achievements: \\n• 7th Rank in AWS Deepracer League organised by ANZ. \\n• 1st at CRISIL Machine Learning Hiring Challenge’19 for Information Text Extraction \\n• Intel Edge AI Scholarship Winner 2019, Nanodegree Udacity \\n• Microsoft Azure Scholarship Winner Udacity \\n• Top 21 finalist in YES Bank Datathon for providing Machine learning based Banking \\nSolution. \\n• 3rd in Brainwaves 2019 Machine Learning Hackathon by Societe Generale. \\n• 11th in LTFS FinHack Machine Learning Hackathon among thousands of participants \\n• Top 5 in MachineHack Global Leaderboard \\n• Top 10 in Analytics Vidya Global Leaderboard Rankings \\n• 3rd in Edelweiss Hackathon Machine Learning organised by Hackerearth \\n• Top 25 ranking in Crowd Analytix Hackathon - Propensity to fund Mortgages \\n• Ranked under 25 in many Hackathons organised by Analytics Vidya \\n• Gold Level Badge in Hackerrank Problem Solving and Python sections \\n• Appreciations for Client Showcase activities \\n• Ranked among top 5000 best coders in Techgig Code Gladiators and Code Chef \\n• Competitions Contributor in Kaggle \\nCertifications: \\n• Python for Data Science & Machine Learning BootCamp – Udemy \\n• Machine Learning by Andrew Ng - Coursera \\n• Machine Learning Developer Summit 2019 \\n• Intel Edge AI Scholarship Nanodegree Udacity 2019 \\n• AMCAT Certified \\n• Certificate of Excellence by Techgig for Machine Learning Hackathons (Code Gladiators 18) \\n• EV Super league – Best Performer Certificate consecutively for two quarters and High \\nPerformer award at EdgeVerve \\n \\n \\nProjects Undertaken – Self \\n \\n• Sentiment Analysis - Dynamic UX (Full stack Machine Learning) embedded with ML \\nmodels to predict customer feedback sentiment over web. Web Server using Flask, \\nModel Deployment using Heroku, Github. \\nTools: Python, NLTK, RESTful Web Services \\n \\n• ML Models as Scalable Micro Services - Generic API for regression and classification \\nusing embedded models over web to cater different datasets with integrated \\npipeline. \\nTools: Python Scripts, CircleCI, joblib, RESTful, Heroku, Git \\n \\n• Keras Transfer Learning - Keras pretrained ResNet Model deployed on GCP with \\nFront end in React to display the label of Images with its probability. \\nTools: GCP, Python Flask, React, ML \\n \\n \\nQualifications: \\n \\n \\nS. No \\n \\nBoard \\nUniversity/ \\n \\nInstitution \\n \\nYear \\nPercentage/ \\n \\nCGPA \\n \\nClass \\n \\n1. \\nB. Tech in Computer \\n \\nScience & Engineering \\nG.I.T.A. \\nBhubaneswar \\n \\n2016 \\n \\n8.49 \\n \\nFirst \\n2. \\nXII Board – CBSE \\nD.A.V. Hehal , Ranchi 2012 \\n89.2% \\nFirst \\n \\n3. \\n \\nX Board – CBSE \\nSurendra Nath \\nCentenary School, \\nRanchi \\n \\n2010 \\n \\n9.6 \\n \\nFirst \\nLinks: \\n• Github: https://github.com/rajat5ranjan/ \\n• Analytics Vidya: https://datahack.analyticsvidhya.com/user/profile/rajat5ranjan \\n• MachineHack: https://www.machinehack.com/members/rajat5ranjan/ \\n• Hackerearth: http://www.hackerearth.com/@rajat5ranjan \\n• Portfolio Website: https://rajat5ranjan.github.io \\n• LinkedIn: https://www.linkedin.com/in/rajat-ranjan24/ \\n• Kaggle: https://www.kaggle.com/rajatranjan/ \\n \\n \\n \\n \\n \\n \\nDeclaration: \\n \\nI hereby declare that all the above information mentioned regarding me & my credentials \\nare true to the best of my knowledge. \\n \\n \\nPlace: Melbourne AU \\n(Rajat Ranjan) \\n',\n",
       " 'RAVI NIRALA\\n+919661276718 | ravinirala123@gmail.com | linkedin.com/in/ravinirala | github.com/ravinirala\\nEDUCATION\\nInstitute\\nDegree/Certificate\\nYear\\nIndian Institute of Technology, Delhi\\nB.Tech in Chemical Engineering\\n2017 - Present\\nKrishak College Dheodha, Bihar\\nClass XII (BSEB)\\n2017\\nGyan Bharti Model R.C.\\nClass X (CBSE)\\n2015\\nTECHNICAL SKILLS\\nLanguages: Python, SQL, MongoDB, C, C++, HTML\\nSoftwares: Jupyter Notebook, Tableau, MS Excel, MS Powerpoint, Adobe Photoshop\\nLibraries: Pandas, NumPy, Matplotlib, Seaborn, Scikit-Learn, NLTK, TensorFlow\\nPROJECTS\\nFace Mask Detector | Python, OpenCV, CNN\\nJune 2020 - July 2020\\n• Used OpenCV to capture, resize and modify colormap of images\\n• Developed Convolutional Neural Network with a fully connected layer to classify images as with or without mask\\n• Tested the model in conjunction with Cascade Classifier to detect human faces through webcam feed\\nTwitter Sentiment Analysis | Python, NLTK, NLP\\nJune 2020\\n• Performed techniques like Tokenization and Stemming from Natural Language Toolkit library to prepare dataset\\n• Implemented Term frequency-Inverse document frequency to represent text into numerical features after\\npreprocessing the dataset\\n• Applied Logistic Regression and Random Forest Classifier for sentiment analysis classification\\nWhatsApp News Bot | Python, Flask, Twilio\\nDecember 2019\\n• Built a WhatsApp Bot to provide news for a query using Twilio\\n• Used NewsAPI to fetch news from different sources for a given query\\n• Developed the web application using Flask and deployed it on Heroku for end users\\nMachine Learning Model for prediction of Loan Defaulters | Python, ML\\nMay 2020\\n• Predicted loan Defaulters for a given dataset using machine learning models like Logistic Regression, Decision Tree and\\nRandom Forest. Random Forest model gave the highest accuracy\\n• Applied and tested various techniques like imputation of missing values, feature engineering and preprocessing of data\\nSCHOLASTIC ACHIEVEMENTS\\n• Joint Entrance Examination (JEE) Main, 2017 : Secured rank in top 1 percentile among 12 lakh+ candidates\\n• Joint Entrance Examination (JEE) Advanced, 2017 : Secured rank in top 5 percentile among 2.5 lakh+ candidates\\n• Certificate of Merit : Awarded by MHRD, Govt. of India for outstanding performance in all subjects\\nCERTIFIED COURSES\\n• Machine Learning A-Z : Hands On Python In Data Science - Udemy\\n• Data Visualization and Communication with Tableau - Coursera\\n• Introduction to SQL and Intermediate SQL for Data Science - DataCamp\\nPOSITIONS OF RESPONSIBILITY\\nCreative Activity Head, Rendezvous\\nJuly 2018 - Oct 2018\\n• Created Posters and Banners for promotion of multiple events of the festival\\n• Designed Advertisement published in the newspaper, The Hindu, for promotion of the festival\\nPublicity Coordinator, Spic Macay\\nSep 2019 - Oct 2019\\n• Achieved 50% increment in registrations by coordinating with 10+ colleges in Delhi\\n• Achieved 100% YOY increment in online outreach, managed all official communication\\n',\n",
       " '/\\nSIDHARTH SACHDEVA\\n sidharth19.sachdeva@gmail.com\\n 09650302606\\n www.linkedin.com/in/sidharth-sachdeva-5035b598/\\n sidharth19s\\n\\uf0e0\\n\\uf095\\n\\uf041 H-173, DLF Skycourt, Sector-86, Gurgaon\\n\\uf0e1\\n\\uf09b\\nSUMMARY\\nExperienced Data Scientist with hands-on skills in Python, SQL, Statistical Analysis, Machine Learning, Deep Learning & Model Building with good\\nvisualization and presentation skills. 8+ yrs of experience across shipping and healthcare.\\nAdept in Data-driven decisions help to maximize business development by deriving valuable insights and building predictive models using endless relevant\\ndata.\\nComing with hands-on experience in data wrangling, exploratory data analysis, and Predictive modelling to extract actionable information from huge\\ndatasets.\\nExpertise in solving analytical problems using quantitative and statistical approaches and developing a predictive statistical model through supervised and\\nunsupervised machine-learning techniques.\\nEMPLOYMENT\\nPlunes\\nAssociate Product Manager · Mar. 2020 to Oct. 2020 · Gurgaon\\nLed a team of 12 contributors to complete Delhi-NCR onboarding with Tier-1 hospitals.\\nNegotiated and established exclusive long-term business agreements at slashed rates.\\nLaunched a Covid response team, which helped receive Series B funding.\\nMaersk\\nMarine Engineer · Jan. 2011 to Mar. 2015 · Copenhagen\\nAmong the first engineers to work on the largest container ship in the world.\\nLed design and operation changes across an entire fleet of vessels.\\nQNET\\nConsultant Executive Sales Analyst · Mar. 2015 to Nov. 2019 · \\nGurgaon\\nLed a team of 30+ contributors to expand distribution channels across 4 metropolitan cities.\\nAlong with my team, responsible for revenue generation of over 6 Cr. \\nEDUCATION\\nSpringboard · June 2020 to Mar. 2020\\nData Science Certification/2020/Springboard. Currently working on deep learning along with two capstone projects.\\nAMET · June 2006 to Mar. 2011\\nB.E. Marine Technology 2011\\nStudied marine engineering and nautical science in a dual course and got placed with the largest container shipping company in the world, Maersk Line.\\nPROGRAMMING SKILLS: Python , Pandas, Numpy, Sklearn, Matplotlib, Seaborn\\nMACHINE LEARNING / DEEP LEARNING: K-nearest neighbors, Random Forests, Naive Bayes, Regression Models, PyTorch\\nDATABASE MANAGEMENT/ DATA SOURCING: MySQL, PostgreSQL, Spark, PySpark\\nPROBABILITY & STATISTICS: Probability Basics and Random Variables, Probability Distributions, Hypothesis Testing, Modelling\\nDATA VISUALIZATION: Tableau, PowerBI,MS Excel, Plotly\\nSKILLS\\nPROJECTS\\nFootball Match Result Prediction\\nApr. 2020 to Nov. 2020\\nUsing 10 years of football match data on the EPL, I have built a multi-class classification model.\\nHaving applied machine learning I am able to predict football matches with better probability than the bookies.\\nTechnology Used: Python, Beautiful Soup, Machine Learning – Random Forest Model, XGBoost, Platform – Jupyter Notebook\\nDescription: Data sourced from a combination of different websites using Beautiful Soup. Data cleaning, preprocessing followed by EDA of each match’s\\nattributes across 10 years. Hypothesis testing done for 20+ hypothesis for better understanding of attributes along with outliers.\\nModel selection done using various ML algorithms with cross validation and hyper-parameter tuning. \\nPersonal Loan Campaign prediction\\nAug. 2020 to Nov. 2020\\nUsing data for 5000 existing customers, built a classification model to predict success in a personal loan campaign.\\nHaving applied machine learning, good recall value has been achieved, and along with lift chart analysis, will help with customer onboarding.\\nTechnology Used: Python, Machine Learning – Logistic Regression, Decision Tree, Pruned Decision Tree, Random Forest Model, Gradient Boosting along\\nwith \\nH2o from AutoML, Platform – Jupyter Notebook\\nDescription: EDA on data reveals certain very interesting patterns in customer behaviour, which combined with prediction from ML models along with the\\nlift chart will result in a successful targeted marketing campaign.\\nModel selection done using various ML algorithms with cross validation and hyper-parameter tuning. \\n',\n",
       " 'ANJALI SINGH \\n  Business Analytics Specialist, \\n      Advanced Analytics \\n \\n \\n \\nEmail: anjali9singh@gmail.com \\nPhone: +91-8884544016 \\nAddress: Hyderabad, Telangana \\nSOCIAL MEDIA \\nWORK EXPERIENCE \\n \\nhttps://www.linkedin.com/in\\n/anjali-singh-08316123/ \\n \\nPROFESSIONAL SKILLS \\nMachine Learning Algorithms \\nStatistical Modelling \\nPython \\nHive \\nPostgreSQL \\nAzure DataBricks \\nProgram Management \\nStakeholder Management \\n EDUCATION \\nExperienced and Data Driven Data Scientist with approx. 8 years of experience in the \\nIndustry. Creative, and highly organized with the ability to integrate out-of-the-box \\nthinking and problem-solving analysis to improve processes.  \\n \\n \\n \\n \\nBusiness Analytics Specialist, Advanced Analytics & Insights \\nMicrosoft / October 2018 - Present \\n• \\nDeveloped and delivered Delivery Excellence and Global Capacity Management \\npredictive solutions  \\n• \\nApplying statistical & Machine Learning techniques to problems such as Customer   \\nSatisfaction analysis, Demand Forecasting using Prophet, Consulting Projects \\nBurn Forensics to understand usage trends, Insights, and drivers \\n• \\nEnd-to-end execution of the Data Science process: understanding business \\nrequirements, data discovery and extraction, model development and evaluation, \\nto production pipeline implementation \\n• \\nDesign, Develop and code scripts \\no \\nTo cleanse, integrate and evaluate large datasets from multiple disparate \\nsources including structured, and unstructured data \\no \\nTo embed Machine learning algorithms such as logistic regression, \\nregularized models, random forests, clustering, XGBoost, Catboost, stacking \\netc., and automate processes \\n• \\nDevelop Black Box Interpretability solutions using Shapley values SHAP \\nframework, InterpretML framework \\n Business Analyst II – Advanced Analytics, Data Science \\nVMware / October 2016 – August 2018 \\n• \\nDeveloped Propensity Statistical Models across areas such as Sales and \\nMarketing - cross-sell/up-sell analytical models, and Global Support & Services \\nand Customer Success Solutions \\n• \\nGenerate insights to conceptualize and drive development of analytical \\nsolutions. Build statistical models for renewals-cross-sell, support services, \\ncustomer satisfaction – for upgrade, product satisfaction (PSAT) etc. \\n \\nCONTACTS \\nData Analyst \\nFlipkart- ekart Analytics, Consultant/ September 2015 – October 2016 \\n• \\nWorked extensively on huge datasets to perform data analysis to design metrics \\n(KPIs) to measure the performance of the business processes, and to provide \\ncompelling management reporting on a regular basis \\n• \\nDeveloped solution for Perfect Customer Delivery, and Perfect Customer \\nExperience \\nSoftware Engineer – Web Analytics Developer \\nHCL Technologies Ltd / September 2010 – July 2013 \\n• \\nAnalyzed web traffic by using conversion and traffic variables, and captured the \\ncustomer journey on the online stores to generate insights, and identify issues \\nsuch as bounce rate \\n• \\n \\nSoftware Developer – Business Intelligence \\nTeam Computers Pvt Ltd / July 2010- September 2010 \\n• \\nDeveloped Data Models, and created reports and dashboards for business \\npartners \\n \\nHackathons \\n3rd Rank  - Women Data Science \\nHackathon by Bain & Company \\n \\nMobility Analytics -Rank 1st \\nhttps://datahack.analyticsvidhya.com/cont\\nest/janatahack-mobility-\\nanalytics/#LeaderBoard \\nRecommendation Systems- \\nRank 2nd \\nhttps://datahack.analyticsvidhya.com/cont\\nest/janatahack-recommendation-\\nsystems/#LeaderBoard \\n \\nMLADS Speaker  \\n \\n \\n \\n \\n \\n \\n   \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nBusiness Analytics- ISB, 3.32/4  \\nB. Tech (IT)- UPTU, 78.5% \\n12th Std- CBSE, 79.2% \\n10th Std- CBSE, 84.6% \\n \\n \\nBusiness Analytics Graduate, \\nISB Hyderabad \\nPredictive Intelligence for \\nAutomatic Global Support Ticket \\nRouting \\nMicrosoft- Machine Learning, AI and Data Science Conference, Fall \\n2020 \\n',\n",
       " 'SUNIL KUMAR\\n+91 7397419727\\nsunilkumarn@protonmail.com\\nlinkedin.com/in/sunil-kumar\\ngithub.com/DragonPG2000\\nEDUCATION\\nBachelor of Technology | Software Engineering\\nAug. 2017 – May 2021\\nSRM Institute of Science and Technology\\nChennai, India\\nWORK EXPERIENCE\\nAssociate Researcher\\nJuly 2018 – August 2019\\nNextTech Minsky AI Lab.\\nChennai, India\\n• Worked on building SOTA models for Kaggle competitions.\\n• On NLP related subtasks I was able to break current SOTA scores.\\nResearch Intern\\nSeptember 2020 – October 2020\\nOffNote Labs\\nBangalore, India\\n• Worked on text detection and recognition for low resource Multi Lingual Languages.\\n• Primary Focus was on building OCR models for better text identification for Indic Languages.\\nDeep Learning Intern\\nDecember 2020 – Present\\nStealth Mode Startup\\nGurgaon, India\\n• Working on using Deep Neural networks for Video analytics.\\n• Built models incorporating multi modal fusion of Audio and Video related data for emotion recognition from\\nlive video streams.\\nCOMPETITIONS\\nSIIC Melanoma Detection\\n2020\\nBronze Medal\\nKaggle\\n• I participated in the Kaggle Competition hosted by SIIC to detect the presence of Melanoma A very common\\ntype of Skin Cancer.\\n• With an F1 score of 0.938 I won a bronze medal for my performance.\\nCharacter Recognition in ancient Japanese Paintings\\n2019\\n11th Place\\nNishika\\n• Competed in the Nishika Challenge to identify the different types of people In ancient Japanese paintings.\\n• My model had an F1 score of 0.91 and placed me in the 11th place.\\nSRM Fablab Hackathon\\n2019\\nWinner\\nSRM Fablab\\n• My team proposed an interconnected system to track the productivity and usage of lab equipment.\\n• I built a Facial recognition system based on Facenet in order to prevent unauthorised access.\\n• As an additional feature I built a productivity tracker for tracking if lab equipment was used properly.\\nPROJECTS\\nMechanism of Action Prediction\\n2020\\nUsing the data from Kaggle competition Mechanism of Action. I built a model that can predict the\\nMechanism of action of a drug given the tabular properties of a subject. My model had a log loss score of\\n0.19\\nYoutube 8 million Video Understanding\\n2020\\nUsing the open source Youtube 8 million I built and iterated various attention based paradigms for video\\ncategorisation.\\nThe modifications led to a Global Average Precision score of 0.81, an improvement from the competition LB top score.\\nSKILLS\\nLanguages: English (Full Professional fluency), Tamil (Native), Hindi (Native)\\nProgramming: Python (NumPy, SciPy, Matplotlib, Pandas,Tensorflow, Scikit Learn), Scilab, C/C++, Java\\n',\n",
       " \" \\nNITESH KUMAR \\nContact   : +91-9641575820                               Email id  : niteshkumardmk1@gmail.com                                 LinkedIn : linkedin.com/in/niteshiitkgp \\n  \\nEDUCATION \\n  \\n  Year \\n               Degree/Exam \\n             Institute \\n                 CGPA/ % \\n  2018 \\n               Integrated MSc (5 Yr) in Applied Geology \\n             IIT Kharagpur \\n                 7.45 / 10 \\n  2012 \\n               Intermediate Examination \\n             BSEB, Patna \\n                 71% \\n  2010 \\n               Secondary School Examination \\n             BSEB, Patna \\n                 74.8% \\n   \\nINTERNSHIPS / WORK EXPERIENCE \\n  \\nAssistant Manager (Fraud Prevention), Paytm Payments Bank, Noida                                                                             (August'20 – Current) \\n   • Working on improving the predictive power of existing classification model to identify fraudulent accounts by using different modelling    \\n      techniques, ensembling output from different models and by incorporating new features into the model \\nBusiness Analyst (Pricing & Revenue), OYO Rooms, Gurgaon    \\n(June'18 – July’20) \\n• Successfully tested and implemented 'Inverted Pricing' & 'Gears based Pricing' in pricing algorithm which lead to shift in demand trend   \\n   by 2 days and received ‘Highest Business Impact’ award for the same in Q1'19  \\n• Ideated, executed and concluded various pricing related experiments and shared the results with different stakeholders   \\n• Automated the micro and macro level surges / dips in pricing based on predicted occupancy and other factors of clusters / cities  \\n• Identifying the cluster and city level peaks in advance and setting the base prices accordingly over which dynamic pricing happens  \\n• Created automated dashboards at cluster and city level to track the performance on metrics like occupancy and avg room rate (ARR) \\n  • Worked upon migrating the dynamic pricing algorithm from R to a modular, scalable and more efficient codebase in Python \\nQuantium Analytics, Hyderabad \\nSales Forecasting for New Products \\n(May'17 - July'17) \\n• Retrieved and pre-processed the dataset using Teradata database and identified the new products which were launched \\n• Predicted percentage of pre-period sales of existing products which will be switched to a new product after its launch \\n• Used GBM in R for prediction using features product attributes, pricing, promotion, customer behavior, loyalty, segmentation etc. \\n• Interpreted the model using Partial Dependence Plots and analyzed the marginal effects of features on response variable \\n \\nTrendwise Analytics, Bangalore \\nAudio, Text and Image Analytics \\n (May'16 - July'16) \\n• Introduced two new features to the existing framework, Tone Analyzer to detect different types of tones (joy, anger, fear, sadness \\netc.) present in an audio file and Document Classification to categorize the text retrieved from same audio file using IBM Watson \\n• Developed an API to extract text written on an image using IBM Watson Visual Recognition \\n  \\nAWARDS AND ACHIEVEMENTS \\n     \\n• Received ‘Highest Business Impact’ award for Q1-2019 and ‘Employee of the Month’ award for Aug’19 and May’20 at OYO Rooms  \\n• Winner of McKinsey Analytics Sales Excellence Hackathon organized on Analytics Vidhya (Jan’18) \\n• Winner of AbInBev Data Science Talent Hunt Hackathon organized on Analytics Vidhya (Feb’18) \\n• Winner of data analytics event of Impulse (annual fest of electrical department, IIT Kharagpur) (March'17) \\n• Runner-up in LTFS Data Science FinHack 2, organized by L&T Financial Services on Analytics Vidhya (Jan’20) \\n• Secured a position in Silver category in Shell.AI Hackathon for Sustainable and Affordable Energy (Nov’20) \\n• Currently hold rank 8 out of 0.3 million data science enthusiasts and former student rank 1 on Analytics Vidhya \\n• Secured national rank 1 and worldwide rank 15 in 17th edition of Data Mining Cup 2017 among 200+ teams of 48 countries \\n• Among top 30 finalists out of approx 1000 teams from all over the world in International Data Analysis Olympiad (IDAO) 2018 \\n• Among top 10 finalists after 2 elimination rounds in 4th edition of IndiaHacks Machine Learning Challenge 2017 on HackerEarth  \\n• Ranked 10/1000+ participants in a data science competition organized by National Stock Exchange and ISB on HackerRank  \\n  \\nCOMPETITIONS / HACKATHONS \\n  \\nHackerRank: OLX- Code and the Curious (July'17) \\nAds recommendation  \\n Rank 2/1,1000+ \\n \\n• Analyzed the record of around 7,000 OLX visitors who have visited 18,000+ unique products on sold in 1 month \\n• Recommended 10 ads to buyers whose sellers are likely to be contacted by buyers based on the OLX browsing history of buyers \\nAmerican Express: Analyze This (Oct'16)                                                   Election Poll Prediction                                                        Rank 9/1,500+  \\n  • Predicted the party to which a new set of citizens will vote for based on their previous votes and engagement with the party  \\n  • Used a custom evaluation metric to optimize the Xgboost model performance and fine tune the hyperparameters \\nEXL Analytics: EXL Excellence Quotient (Jan'17 - Feb'17) \\nCustomer Churn Prediction \\n   Top 25/500+ \\n \\n• Developed a customer churn prediction model to identify the customers who are likely to churn from broadband service \\n• Used Bag-of-Words technique in R to derive features from enquiry statement of the customers to use them in model \\n• One of the 2 teams from IIT Kharagpur and among 25 national teams who qualified for 2nd round of the competition \\nZS Young Data Scientist Challenge (July'17) \\nEvents Sequence Prediction \\n Rank 17/500+ \\n \\n• Analyzed the medical history of 3,000 patients who have visited for around 6,000 events (Diagnosis, Procedure or Treatment) in 3 years \\n • Predicted next 10 events reported by patients in order of occurrence based on their 3 years of medical history \\n• Achieved private leaderboard NDCG (Normalized discounted cumulative gain) @K (K = 10) score 0.10 (highest 0.11) \\n    \\nCERTIFICATIONS \\n  \\n• R Programming \\n     • Getting and Cleaning Data \\n      • Practical Machine Learning \\n• The Analytics Edge \\n     • Data Analysis and Statistical Inference \\n      • Introduction to Probability and Data \\n  \\nSKILLS AND EXPERTISE \\n  \\n• R \\n                               • SQL & Hive                                                  • Python                                                     • Tableau                       \\n\",\n",
       " 'Zishan Kamal  \\nData Scientist \\n \\n# 14244, Prestige Lakeside Habitat, Gunjur Village, Varthur Hobli, Bangalore (India) – 560087 \\n \\n\\uf029 +91– 9886726680 / +91– 9632700442  \\n\\uf02a zishan.kamal@gmail.com \\n zishan-kamal  \\nAV rank \\n \\nA data scientist with 6 years of experience with a keen interest to apply machine leanrning and AI techniques \\nto solve complex business problems. Overall, possess a rich experience of 16 years in software development \\nand IT ranging from application development, data analytics to project management. \\nKey strength includes collaborating with stakeholders, subject matter experts, client teams and other functional \\nteams globally to understand business problems, design and execute solutions using various data sources and \\nadvanced analytics approach. \\nProfessional Summary \\n\\uf0a7 \\nExperience as a Data Scientist in building Data Science solutions using Machine Learning, Statistical \\nModeling, Data Mining, Natural Language Processing (NLP), Deep Learning and Data visualization. \\n\\uf0a7 \\nExperience working with large data; interpret and communicate insights and finding from analysis and \\nexperiment toboth technical and non-technical audience in products, service and business. \\n\\uf0a7 \\nImplemented several supervised and unsupervised learning algorithms such as Ensemble methods, \\nLinear / Logistic regression, Regularization techniques, Naïve Bayes, SVM, Clustering techniques, \\nDeep Learning, Natural Language Processing, Transfer learning and Time series forecasting methods. \\n\\uf0a7 \\nPlayed key role in requirement gathering, articulating business needs, formulating the problem \\nstatement, solution design, presenting viable solutions to client and providing technical (AI & ML) \\nguidance to the team to implement these solutions.  \\n\\uf0a7 \\nDemonstrated ability to adapt to multiple domains, dynamic work environments and techno functional \\nroles \\n\\uf0a7 \\nRich experience in all phases of software development life cycle utilizing multiple development \\nmethodologies including Design Patterns, OOD, Agile methodology and Structured Programming. \\n\\uf0a7 \\nAn effective Communicator & Coordinator with Excellent Networking & Interpersonal skills. \\n\\uf0a7 \\nStrong skills in building client relationship, conflict resolution and Strategic planning. \\n\\uf0a7 \\nExcellent communication, interpersonal and leadership skills. \\n\\uf0a7 \\nWide exposure to solving Analytical Problems \\n\\uf0a7 \\nActive participation in data science hackathons and initiaves within and outside the organization. \\nExperience Summary \\n \\n \\n\\uf0a7 \\nWorking as Data Scientist in Wipro Technologies Limited since January 2017. \\n\\uf0a7 \\nWorked as Project Manager in Cognizant Technology Solutions from January 2011 to January 2017. \\n\\uf0a7 \\nWorked as Technology Lead in Infosys Technologies Limited from October 2004 to January 2011. \\n \\n\\\\ \\n\\\\Education \\n \\n \\n\\uf0a7 \\nBusiness Analytics and Intelligence – 2018-2019 (1 Year Classroom Programme) \\nIndian Institute of Management, Bangalore \\n \\n\\uf0a7 \\nBachelor of Engineering (Information Science) – 2000-2004 \\nVisveswaraiah Technological University, Belgaum \\n \\n \\n \\n \\nKey Skills \\n \\n\\uf0a7  Data visualization and Interpretation\\uf020\\n\\uf0a7  Supervised Learning\\uf020\\n\\uf0a7  Data pre-processing and Imputation\\uf020\\no    Linear and Logistic Regression \\n\\uf0a7  Hypothesis Testing\\uf020\\no    K-Nearest Neighbour \\n\\uf0a7  Unsupervised Learning\\uf020\\no    Support Vector Machine \\n   o    Principal Component Analysis \\no    Classification and Regression trees \\n   o    Clustering \\no    Random Forest  \\n\\uf0a7  Time series forecasting \\uf020\\no    Gradient Boosting Machines  \\n\\uf0a7  Deep Learning – CNN, RNN, LSTM,     \\nAutoencoder\\uf020\\n\\uf0a7  Natural Language Processing \\uf020\\n \\nKey Project highlights  \\n \\nExtraction of attributes from various contract documents  \\n \\nThe project is aimed at legal departments who spend substantial manual effort (~5 days) in reading heavy \\nlegal documents of sizes ranging from 50 to 500 pages and then reviewing key attributes present in the \\ndocument. AI based tool aims to reduce this manual effort by extracting large number (50 to 500) of key \\nattributes and providing a displayof the document where portions are mapped to each of the attribute being \\ndiscussed. The solution uses Sequence tagger with bi-direction LSTM + CRF with GloVe word embeddings. \\n \\nForecast warranty claims of one of the largest Two-Wheeler manufacturers \\n \\nProblem faced by client was that Forecasting of warrantly claims was based on qualitative technique and was \\nnot data driven, thereby resulting in long service times for customer due to stock out of parts coming under \\nwarrantly claims. \\nThe project aimed to develop a data driven forecasting model based on historical timeseries data that is \\navailable and enable the organization to have an accurate view of warranty claims. The organization aimed to \\nimprove its customer service and reduce its inventory carrying costs. Several traditional timeseries forecasting \\ntechniques like ARMA, ARIMA, SARIMA and Gradient boosting trees with date related derived features \\nwere tried but LSTM model gave the best result. \\n \\nDetection of anomaly and fraudulent transactions \\n \\nFraud risk monitoring (FRM) division used to perform anomaly and fraud detection based on a set of defined \\nrules. Rules needed to be tweaked frequently and entire detection was done manually. Solution was designed \\nto automate the detection with the help of ML techniques. This resulted in huge effort saving and getting better \\nresults. Solution was implemented using Autoencoders with 3 layers of standard feed forward encoder and \\ndecoder. \\n \\nClassification of emails received by marketing team \\n \\nMarketing team in a retail sector recieved several emails related to query, complaints, promotions, some \\nunrelated etc. They had to go through all the emails, identify the type of email and redirect it to relevant \\nemailing group to take necessary steps and respond to the email. A system was put in place to classify the \\nemails. Final solution was a BERT model to perform multiclass classification. \\n \\nCapturing details from Invoices \\n \\nBack office team receives several travel airline invoices and needs to capture various details for accounting \\nand validation purposes. Capturing of details from invoices were manual in nature and hence resulted in lots \\nof efforts. Invoices were in PDF format with mix of text and image (scanned copy) format. A solution was \\nimplemented to automate the capture of key attributes along with the model’s confidence level so that manual \\nreview can only be done where model’s confidence was low. This project drastically reduced the human effort \\nand extra burden from BO team especially during their month end and quarter end processing. \\n \\nCross selling initiative of a bank \\n \\nBusiness wanted a mechanism to identify list of customers who were most likely to be the target of this \\nCross-Sell initiative. Customer spending patterns, their demographical information and transaction history \\nwere used to identify potential customers.  \\n \\nCustomer churn analytics for a bank \\n \\nBank wanted to identify customers likely to churn balances in the next quarter by atleast 50% vis-à-vis current \\nquarter. Customers information such as age, gender, demographics along with their assets, liabilities and \\ntransactional were analysed to predict the prop ensity to churn for each customer. Final solution was a model \\ntrained with SMOTE oversampling with Gradient Boosting trees. \\n',\n",
       " 'Rooban Sappani\\n18PD30\\nFather’s name\\nGender\\nDate of Birth\\nLanguages known\\nEmail\\nMobile\\nSappani Muthiah\\nMale\\n21st December 2000\\nEnglish, Tamil\\n2000rooban@gmail.com\\n+91-97904-56377\\nPermanent Address\\nA 1/6, Baskara Apts,\\nNo. 1 Tolgate,\\nTiruchirappalli,\\nTamil Nadu,\\n621216.\\nOBJECTIVE\\nTo obtain a position as a Student Intern for a period of six months from May\\n2021 to November 2021.\\nACADEMIC QUALIFICATION\\nCurrently pursuing 3rd year of 5-year Integrated M.Sc. Data Science at the\\nDepartment of Applied Mathematics and Computational Sciences at PSG College of Technology.\\nSKILL SET\\nLanguages\\nC++, Python, C, R\\nPlatform\\nWindows, Linux\\nTools\\nTensorflow, OpenCV, GitHub, Oracle\\nAREAS OF INTEREST\\n●\\nSupervised and Unsupervised Learning\\n●\\nData Structures and Algorithms\\n●\\nDeep Learning\\n●\\nComputer Vision\\nACADEMIC RECORD\\nCourse\\nInstitution\\nBoard/University\\nCompletion By\\nMarks\\nM.Sc\\nPSG College of Technology,\\nCoimbatore\\nAnna University\\n2023\\n7.86\\nX\\nIndian School of Bahrain\\nCBSE\\n2016\\n94.00%\\nXII\\nThe Velammal International\\nSchool\\nCBSE\\n2018\\n92.80%\\nINDUSTRY BASED PROJECT EXPERIENCE\\nNIT Trichy - Machine Learning Research Intern\\nResearch and development of an Automated Crime News Classification model. Worked under\\nDr. A. Santhanavijayan as a deep learning intern to work on the development of the research\\npaper, \"A Knowledge Centric Hybridized Approach for Crime Classification incorporating Deep\\nBiLSTM Neural Network\", which has been submitted(Under Review) to Multimedia Tools and\\nApplications, Springer.\\nNON-ACADEMIC PROJECTS\\n●\\nReal Time object detection, using the YOLO v3 kernel data on kaggle, an automated\\nobject detecting model was built on keras. Darknet 53 was used for feature extraction and\\nthe YOLO v3 algorithm was used for object detection. With the help of OpenCV, a real\\ntime system was built on python to detect upto 257 different objects through videos.\\n●\\nAll about Faces, A real time system that works on a picture of a person’s face to detect the\\nlocation of the face(using Haar Cascade), presence of a mask, the gender and the\\nethnicity of the person. Dataset: UTK-face, Tools used: Kaggle, CNN (Transfer learning,\\nVGG16), keras and opencv.\\n●\\nGoogle Stocks Predict, an application built using Google Data, to predict stock prices of\\nGoogle, using a LSTM Neural Network. The data being structured, was pre-processed\\nusing Min-Max scaling.\\nACADEMIC PROJECTS\\n●\\nGive me a Movie, Content based recommendation system implemented in C++ with the\\nIMDB dataset. Data structures: Bloom filters for IR, BK trees for spell checking, linked list.\\nAlgorithms: Dynamic levenshtein algorithm for search suggestion and recommendation,\\nFNV and Murmurhash3 to construct the bloom filters.\\n●\\nFake News Detector, an application built on Python to automatically classify news articles\\ninto\\nfalse\\nor\\ngenuine.\\nTF-IDF vectorizer was used for word embeddings and\\nPassive-Aggressive classifier was the algorithm used.\\n●\\nForest fire Analysis, Implemented in python for analysing the past 5 years data from\\nMontesinho park, Portugal and using various Machine Learning and visualizations, have\\nefficiently predicted future locations and degree of fire in the Park.\\nEXTRA-CURRICULAR ACTIVITIES AND ACHIEVEMENTS\\n●\\nParticipated in the smart internz IBM Hackathon 2020(creating a chatbot for automotive\\nresume selection) and came to the final round.\\n●\\nActive Machine Learning, Deep Learning and Python freelancer on freelancer.in.\\n●\\nCompleted the Deep Learning Specialization consisting of 5 courses organized by Andrew\\nng on Coursera.\\n●\\nMember of the Coursera Professional Community(Through IBM Professional Data Science\\nCourse).\\n●\\nPiano Practicals and Theory, ABRSM levels 1,2,3,4,5 passed with distinction.\\nDECLARATION\\nI, Rooban Sappani, do hereby confirm that the information given above is true\\nto the best of my knowledge.\\nPlace: Coimbatore\\nDate : 03/05/2021\\n(Rooban Sappani)\\n',\n",
       " 'Wayal Rushikesh\\n|LinkedIn|:Wayal Rushikesh\\nwayalrushi5@gmail.com | 8597165877\\nEDUCATION\\nIIT KHARAGPUR\\nDUAL DEGREE\\nELECTRONICS AND\\nELECTRICAL COMMUNICATIONS\\nENGINEERING(AIR-690)\\n2017-2021\\nCGPA: 8.76/10\\nAKLANK PUBLIC SCHOOL\\nCLASS XII\\n2016-2017\\nPercentage: 84.6\\nPODAR INT. SCHOOL\\nCLASS X\\nNTSE Qualified\\n2014-2015\\nCGPA : 10/10\\nSKILLS\\nPROGRAMMING\\n• Python\\n• C and C++\\nLIBRARIES\\n• Pytorch\\n• Keras\\n• Scikit learn\\n• Numpy\\n• Pandas\\nUTILITIES\\n• Tableau\\n• Solid Works\\nCOURSEWORK\\n•Machine Learning\\n•Neural Networks\\nand its Application\\n•Probability and Stochastic\\n•Mathematical Methods\\n•Algorithms(Theory and Lab)\\n•Engineering Mathematics\\n•Programming and Data Structures\\n•Matrix Algebra\\nEXTRA-CURRICULAR\\nNational Sports Organisation\\n•Taught the students of primary school\\n•Part of the NSO - Yoga\\nInter Hall Events\\n•Data Analytics\\n•Open-Soft\\nINTERNSHIPS\\nHSBC\\n| JUNIOR ANALYST\\n•Study of order-book resilience.\\n•Developed an algorithm to classify the aggressiveness of orders.\\n•Quantiﬁed the changes in behaviours of liquidity measures around\\naggressive orders.\\n•Quantiﬁed the differences in market impact left by various orders and in\\nvarious markets.\\n•Extended the created mechanism to ﬁnd optimal combination of aggressive\\norders to give low cost and minimal market impact.\\nCOMPETITIONS\\nINSTANT GRATIFICATION | KAGGLE\\n•Binary prediction challenge having a puzzling data-set.\\n•The activation of certain columns was dependent on certain factors and\\nhugely affected the prediction\\n•Found out the best model and hence data distribution type. Rank:244\\nML/AI CHALLENGE\\n| FLIPKART GRID – TE[A]CH THE MACHINES\\n•Trained A transfer learning algorithm to predict the bounding box of objects\\nNETAPP DATA CHALLENGE | KSHITIJ 2019, IIT KHARAGPUR\\n•Used Tf-idf and word2vec to build a news classiﬁer based on headlines and\\nshort description\\n•Stood 2nd among more than 150 teams, held at Data Analytics Event.\\nINTEL SCENE CLASSIFICATION CHALLENGE | ANALYTICS VIDHYA\\n•Used Fastai libraby (resnet152) to build image classiﬁer\\n•Final rank 82 with accuracy 0.94\\nMOOCS\\nMACHINE LEARNING A-Z™\\n•Study of regression, CART models, Text parsing Regression trees,\\nClustering, Data Visualization and its implementation.\\nDEEP LEARNING SPECIALIZATION\\n•Study of CNN, RCNN for image processing and LSTM for text processing\\nMACHINE LEARNING(CS239) | OFFERED BY STANFORD UNIVERSITY\\n•Study of CNN,RCNN and ResNet\\nGROUPS\\nKHARAGPUR DATA ANALYTICS GROUP | IIT KHARAGPUR\\n• Conducting of data antics camps.\\nPOSITION OF RESPONSIBILITY\\nCAPTAIN : DATA ANALYTICS | MEGHNAD SAHA HALL OF RESIDENCE\\n• Headed the gold wining team in Open IIT Data Analytics competition.\\n• Trained new batch in Data Science\\n• Responsibility of conducting meetings on regular intervals for preparation\\nof Inter hall event competitions.\\n1\\n',\n",
       " 'SAI SUCHITH MAHAJAN\\nIndian Institute of Technology Palakkad\\n� mahajan.saisuchith@gmail.com\\n� +91-8329867394\\n� Palakkad, India\\n� www.linkedin.com/in/sai-suchith-mahajan/\\n� https://github.com/Saisuchith\\nwww.kaggle.com/suchith0312\\nEDUCATION\\nBachelor of Technology (Electrical\\nEngineering)\\nIndian Institute of Technology Palakkad\\n� 2020\\n� Palakkad\\n• CGPA: 8.16\\nCOURSES\\n• Machine learning, Deep learning\\n• Reinforcement learning, Numerical Analysis\\n• Linear Algebra, Convex Optimization\\n• Probabilistic Systems Analysis\\nTECHNICAL SKILLS\\n• C, C++, Python\\n• PyTorch, Keras, Scikit learn\\n• Matlab, MongoDB\\nACHIEVEMENTS\\n• Kaggle Competition Master Rank : 1151\\n• Secured a rank of 9(Gold Medal) out of 7198\\nparticipants in Kaggle Home Credit Default\\nRisk competition\\n• Secured a rank of 4 in online and 5 in ofﬂine\\nBrainwaves 2019\\n• Recipient of KVPY Fellowship Award 2015\\n• Placed National wise Top 1% in NSEC\\n(2015-2016)\\nEXPERIENCE\\nAssociate Data Scientist\\nSociete Generale Global Solution Centre\\n� September 2020-Present\\n� Bengaluru, India\\n• Developed a ML, heuristic based pipeline for automating client\\non-boarding for a payment app.\\n• Developed a Deep Learning pipeline to automatically extract\\nthe information of UBO’s from RBE documents.\\nSubject Matter Expert\\nHackerEarth\\n� April 2019 – May 2020\\n� Bengaluru(Remote)\\n• Creating new question content related to ML/Data Science for\\nHackerEarth’s question Library\\nData Science Intern\\nSigTuple Technologies Pvt. Ltd.\\n� May 2019 – July 2019\\n� Bengaluru, India\\n• Developed a generic automated pipeline for evaluation of 4\\ndiagnostic products and generated intelligent insights\\nEDUCATION\\nBachelor of Technology (Electrical\\nEngineering)\\nIndian Institute of Technology Palakkad\\n� 2020\\n� Palakkad\\n• CGPA: 8.16\\nCOURSES\\n• Machine learning, Deep learning\\n• Reinforcement learning, Numerical Analysis\\n• Linear Algebra, Convex Optimization\\n• Probabilistic Systems Analysis\\nTECHNICAL SKILLS\\n• C, C++, Python\\n• PyTorch, Keras, Scikit learn\\n• Matlab, MongoDB\\nACHIEVEMENTS\\n• Kaggle Competition Master Rank : 1151\\n• Secured a rank of 9(Gold Medal) out of 7198\\nparticipants in Kaggle Home Credit Default\\nRisk competition\\n• Secured a rank of 4 in online and 5 in ofﬂine\\nBrainwaves 2019\\n• Recipient of KVPY Fellowship Award 2015\\n• Placed National wise Top 1% in NSEC\\n(2015-2016)\\nPROJECTS\\nInferring Wireless Channel Gains\\nPredicting channel gains at one frequency given at\\nanother frequency\\n• Deep neural networks are used to approximate the mapping\\nbetween uplink channel gains and downlink channel gains. This\\nmapping will help eliminating the downlink training and\\nfeedback overhead in co-located/distributed FDD massive\\nMIMO systems.\\nTweet Sentiment Extraction\\nExtract phrases from the tweet\\n• Challenge is to look at the sentiment and tweet and extract the\\nwords or phrase that best supports the sentiment. Trained\\nBERT, RoBERTa model for extracting the phrases.\\nJigsaw Unintended Bias in Toxicity Classiﬁcation\\nDetect Toxicity in comments\\n• Trained LSTM and BERT models to detect toxic comments and\\nminimizes unintended bias with respect to mentions of\\nidentities. Was in Top 5 % of competition in ﬁnal results.\\n',\n",
       " 'Saurav Mishra  \\n \\n \\n \\nDepartment of Biological Sciences and Bioengineering                                                                         Phone: (+91)-9435838761  \\nMinor: Machine Learning & Applications\\u200b                                                                             Email: \\u200bsauravmishra554@gmail.com \\n \\nE\\u200bDUCATIONAL \\u200bQ\\u200bUALIFICATIONS \\nP\\u200bROJECTS \\nLTFS Data Science FinHack 3\\u200b \\u200b(Analytics Vidhya)                       \\u200b                                                                                                                     (Jan’21-Feb’21) \\nBasic Needs Basic Rights Kenya- Tech4 Mental Health\\u200b \\u200b(Zindi NLP Competition)\\u200b                                                                               (Aug’20-Sep’20) \\nPUBG Final Score Prediction (Data Mining) \\u200b(Mentor: Prof. Arnab Bhattacharya)                                                                                      (Jul’18-Nov’18) \\nS\\u200bCHOLASTIC \\u200bA\\u200bCHIEVEMENTS \\n●\\nSecured \\u200b3rd\\u200b position among \\u200b750+\\u200b participants in House Price Prediction hackathon organized by Machine Hack                     \\u200b(2020) \\n●\\nObtained \\u200b12th \\u200brank among \\u200b600+\\u200b participants in Cross Sell Prediction hackathon organized by Analytics VIdhya                        \\u200b(2020) \\n●\\nAll India Rank- 4856\\u200b in \\u200bJEE Advanced 2016\\u200b out of 1.5 lakh successful candidates of JEE Mains 2016\\n                                   \\u200b(2016) \\n●\\nAchieved \\u200b98.10 percentile \\u200bin \\u200bJEE Mains Examination 2016 \\u200bamongst 12 lakh students                                                        \\u200b              \\u200b(2016) \\nT\\u200bECHNICAL \\u200bS\\u200bKILLS \\n●\\nLibraries                                                                        \\u200bNumpy, Pandas, Scikit Learn, Matplotlib, Seaborn, Nltk \\n●\\nProgramming Languages and Software               \\u200bC, C++, Python, SQL , MS Office, GitHub \\nR\\u200bELEVANT \\u200bC\\u200bOURSES \\n  \\nData Mining                                             Introduction to Machine Learning                                                        Data Structures and Algorithm \\n \\nEconometrics                                          Advanced Statistical Methods for Business Analytics                        Probability and Statistics  \\nP\\u200bOSITIONS \\u200bO\\u200bF \\u200bR\\u200bESPONSIBILITY \\nManager, Events, Udghosh’18 \\u200b(Annual Sports Festival IIT Kanpur)\\u200b                                                                           \\u200b                             \\u200b(Jun’18- Oct’18) \\n●\\nRecruited and led a \\u200b2-tier\\u200b team comprising of about \\u200b18 \\u200bmembers under various positions and divisions \\n●\\nHandled a team of \\u200b100 Campus Ambassadors \\u200bfor publicizing the festival in various colleges across the nation \\n●\\nResponsible for smooth conduction of \\u200bChess, Carrom, and Mr.Udghosh events \\u200bwith around \\u200b150+\\u200b participants \\nManagement Executive, Josh’18 \\u200b(Annual Sports Festival  IIT Kanpur) \\u200b                                                                                              \\u200b(Dec’17- Jan’18) \\n●\\nSpearheaded a team of \\u200b5 members \\u200bto conduct \\u200b17 major events \\u200bof all sports including Football, Cricket, Hockey, Athletics, etc. \\n●\\nTended to the needs of \\u200b1500+\\u200b participants for smooth conduction of events, double the previous year’s count of \\u200b800\\u200b students \\nE\\u200bXTRA-\\u200bC\\u200bURRICULAR \\u200bA\\u200bCTIVITIES \\n●\\nBagged \\u200b5 gold\\u200b and \\u200b1 silver\\u200b medal in 4*100m relay, 4*400m relay athletics event in various national level sports competitions \\n●\\nParticipated in \\u200b400m, 4*400m Relay\\u200b Athletics Event in \\u200bInter-IIT Sports Meet’18 \\u200borganized by IIT Guwahati \\n●\\nDramatics:\\u200b Performed \\u200b‘Sab Chalta hai’ \\u200bnukkad natak\\u200b \\u200b(street play) in \\u200bDramatics Evening 1 \\u200bin front of a crowd of\\u200b 300+ \\u200bpeople \\n●\\nBuilt a \\u200bSemi-Autonomous Robot\\u200b which was capable of moving objects in an arena, in a team of \\u200b5 members\\u200b in \\u200bTakneek’16 \\nYear \\nDegree/Certificate \\nInstitute/School \\nCGPA/Percentage \\n2016-2020 \\nB.Tech \\nIndian Institute of Technology, Kanpur \\n6.9/10 \\n2015 \\nXII-CBSE \\nHindustani Kendriya Vidyalaya, Guwahati \\n84.8% \\n2013 \\nX-SEBA \\nY.W.C.A. English School, Guwahati \\n83% \\nObjective \\n●\\nTo classify the time period in which a loan applicant would require a top-up on the existing loan \\n Approach \\n●\\nAnalyzed both Bureau data, which contained information about all loans of the applicants and, Application \\ndata which contained LTFS loan specific data to identify significant trends \\n●\\nGenerated some innovative features by rolling up the bureau data to application level and merged these \\nfeatures to the Application data for further processing and model building \\n●\\nPerformed \\u200badversarial validation\\u200b and removed some features to get more reliable predictions  \\n●\\nApplied \\u200bStratified K-Fold\\u200b cross validation and tuned the model hyper-parameters using \\u200bGrid Search  \\nResult \\n●\\nClassified the loan applications using \\u200b5-fold LGBM\\u200b and secured \\u200b34th\\u200b rank among \\u200b500+\\u200b teams \\nObjective \\n●\\nTo classify text from university students in Kenya into four different mental health categories \\n Approach \\n●\\nPerformed text pre-processing by removing punctuations and stopwords  \\n●\\nNormalized the words by \\u200bstemming \\u200band converted text into numerical features like \\u200bbag-of-word\\u200b and \\u200btf-idf  \\n●\\nApplied various ML models like \\u200bLogistic Regression\\u200b, \\u200bNaive Bayes\\u200b, and \\u200bLGBM \\u200bto predict class probabilities \\n●\\nUsed pre-trained word embeddings to train \\u200bRNN\\u200b based models like \\u200bLSTM \\nResult \\n●\\nPredicted the class probability using LGBM on tf-idf features as it was the best performing model. \\nObjective \\n●\\nTo predict final win place percentile from in-game statistics and initial player ratings \\n●\\nTo discover some of the best ways to survive and increase the chances of winning \\n Approach \\n●\\nCollected 5500 games worth of anonymized player data consisting of over 30,000 observations and 25 \\nattributes and performed an extensive \\u200bExploratory Data Analysis \\u200bon it \\n●\\nPre-processed the data by removing \\u200boutliers\\u200b, \\u200bimputing\\u200b the missing values, and removing redundant \\nfeatures found through \\u200bcorrelation matrix\\u200b and feature importance \\n●\\nApplied various ML models like \\u200bLinear\\u200b \\u200bRegression, Decision Tree, XGBoost\\u200b to predict win place percentile \\nResult \\n●\\nXGBoost was the most effective algorithm on this dataset, giving a Mean Absolute Error of 0.07 \\n',\n",
       " \"Shyam Sundar\\nMachine Learning Engineer\\nshyamnambiraja01@gmail.com\\n9789954424\\nhttps://www.linkedin.com/in/shyam-sundar4801/\\nhttps://github.com/Shyam4801\\nhttps://shyamnambiraja01.medium.com/\\nProfile\\nMachine Learning engineer with a demonstrated experience in Computer Vision research for diverse set of business use \\ncases including optimizing and deploying models on Edge platforms\\nProfessional Experience\\nMachine Learning Engineer, e-con systems\\n01/2020 – present | Chennai, India\\n- \\nDeveloped a Biometric application based on Face recognition \\nincluding model evaluation , performance benchmarking\\n- \\nPerformance evaluation of State of the art Anomaly detection \\ntechniques focusing on manufacturing use cases like Surface defects , \\nThreaded rod \\n- \\nMissing Component detection of manually annotated PCB board \\nimages using object detection\\n- \\nDeveloped an SDK for Wire rope anomaly detection based on a \\nclassification model\\n- \\nDeveloped a Smart cart application using a classification model \\nfeaturing on device fine tuning\\n- Evaluated different Edge platforms and their frameworks like Intel's \\nOpenVino, Nvidia , Xilinx VitisAI\\n- \\nPlatform-aware Neural Network adaptation\\n- \\nContributed technical blogs to promote our existing products and \\nsolutions\\nProjects\\nPrice optimization\\nChoosing the right pricing strategy using a reliable price response \\nfunction as well as choosing an optimum price under uncertainty \\nthereby maximizing profit\\nProduct recommendation\\nProvide personalized recommendations to customers analyzing their \\nbehavior and thereby reducing customer retention rate \\nSupply chain optimization\\nAnalyzing the complexity involved in balancing the operating costs \\namong various players in the supply chain , thereby forecasting the \\ndemand so as to maximize the profit\\nExposure / Skills\\n• Deep Learning • Tensorflow • Pytorch  \\n• Python  • MySQL • Tableau  • \\nStatistical Testing •  Machine Learning \\n(Supervised and Unsupervised) •  \\nEnsemble Techniques •  Time Series \\nforecasting (Univariate)\\nEducation\\nPGP DATA SCIENCE AND \\nENGINEERING, Great learning, \\nGreat Lakes Institute of \\nManagement\\n03/2019 – 10/2019 | Chennai, India\\nBachelor's Degree in ECE, College \\nof Engineering Guindy, Anna \\nuniversity\\nChennai, India\\nHigh school, \\nGRT Mahalakshmi Vidyalaya\\nCertificates\\nReinforcement Learning Specialization\\nIntroduction to TensorFlow for \\nArtificial Intelligence, Machine \\nLearning, and Deep Learning by \\ndeeplearning.ai on Coursera.\\nInterests\\nTech enthusiast , Pencil Sketching , \\nFitness , Badminton\\n\",\n",
       " 'Sri Chandra Duddu\\nManager, Axis Bank AICoE\\nData Scientist with ~2 years of work experience leveraging Predictive Data Modeling and Artiﬁcial\\nIntelligence products to deliver insights and implement action-oriented solutions to complex business\\nproblems. Vivid expertise in variety of Machine Learning problems specially in Agriculture and Banking\\ndomain. Computer Vision and Natural Language Understanding (NLU) are special areas of interest.\\nduddusrichandra@gmail.com\\n+91 9933999147\\nBangalore, India\\nlinkedin.com/in/sri-chandra-duddu\\nWORK EXPERIENCE\\nData Scientist\\nAxis Bank Business Intelligence Unit (BIU)\\n07/2019 - Present, \\nBangalore, India\\nProﬁcient with Python. Hands on Experience in creating, tuning and\\nmanaging the AI product roadmap\\nDeveloped a RASA powered Search for MIS capable\\nChatbot which could exploit the use of Knowledge Graphs\\nfor retrieving MIS directly for business stakeholders and the\\nuse of Buttons to achieve controlled, assisted conversation\\nfor employees to resolve incentive related queries\\nEmployed Positive-Unlabeled (PU) Learning based\\napproach for Hidden Aﬄuent Identiﬁcation using\\nthoughtfully designed Hidden variables like Merchant\\nTransactions, Bureau trade-lines and Customer Investments\\nFacilitated the pilot launch of a third-party delivered\\nRobotic Call Centre project for the organization from end\\nto end design, development and implementation\\nDesigned a Savings Account Feature Revamp Proposition\\nto complement minimum balance requirement criteria with\\ncustomer behavior metrics using Regression analysis\\nAccomplished SARIMAX based time series forecasting\\napproach to Savings Accounts Fee Budgeting capable of\\nEvent Detection through Intervention Analysis\\nExploited the inherent working of Bi-directional Encoder\\nRepresentations from Transformers (BERT) Deep Learning\\narchitecture for extracting contextual word embeddings for\\nText Classiﬁcation and Clustering\\nEDUCATION\\nAgricultural Systems and Management -\\nM.Tech (Dual Degree)\\nIndian Institute of Technology, Kharagpur\\n2018 - 2019, \\n8.50/10\\nAgricultural Systems\\nModeling\\nAgri-Project Cash Flow\\nAnalysis & Finance\\nDigital Soil Mapping\\nDeep Learning\\nFoundations &\\nApplications\\nAgricultural and Food Engineering - B.Tech\\nIndian Institute of Technology, Kharagpur\\n2014 - 2018, \\n7.50/10\\nLogistics Systems\\nProduct Development\\nManagement &\\nProductivity\\nMachine Learning\\nSKILLS & EXPERTISE\\nPython\\nSAS\\nSQL\\nR\\nAWS\\nFlask\\nDocker\\nGoogle Earth Engine\\nINTERNSHIPS & PROJECTS\\nDevelopment of FAQ Bot to service Axis Bank Products\\nSuccessfully built a Retrieval based Chatbot framework in Flask\\nfrom scratch in Python assessed against Axis Aha! and was awarded\\nthe Pre - placement Oﬀer\\nAchieved 96% Reliability by extracting 3000 Latent Semantic\\nAnalysis components and classifying Query Intents using Multi-\\nlayer Perceptron\\nReal-time Paddy crop health monitoring and\\nmanagement using Drone for diseases Detection\\nAchieved a 94.1% Mean Average Precision (MAP) in predicting the\\npaddy plant diseases for an Intersection over Union (IoU) > 0.5\\ncriteria\\nEmployed Faster-RCNN meta-architecture with a ResNet 101\\nfeature extractor in predicting Detection Results that aﬀected\\nPaddy plant diseases\\nSuccessfully designed an Alert System which communicates with\\nthe farmer of the health condition of the Paddy crop once\\nSurveilled by the Drone\\nEstimation of Soil Arsenic Content using Partial Least\\nSquares (PLS) Regression\\nSpectral Reﬂectance data in 350-2000 nm from Diﬀuse\\nReﬂectance Spectroscopy were considered for estimating Soil\\nArsenic content using Partial Least Squares (PLS) Regression\\nDeveloped PLS model achieved adjusted R-square value of 0.94\\nProduction Estimation using Regression Splines\\nDiameter at Breast Height (DBH) and Normalized Diﬀerence\\nVegetative Index (NDVI) were considered for estimating Biomass\\nusing B-Splines approach\\nAchieved adjusted R2 value of 0.98 against the Lowess Curve ﬁt\\nmodel with adjusted R2 value of 0.82\\nHACKATHONS & AWARDS\\nMcKinsey Young Optimization Crackerjack Contest\\n(Rank 5)\\nDemand Forecasting and Optimization was the crux of the Hackathon.\\nARIMA based demand forecasting with constraint optimization helped\\nme achieve Rank 5 on the Leaderboard\\nHackerEarth Deep Learning Challenge (Rank 17)\\nResnet50 Feature Extractor and K-means based Image Clustering to\\nclassify toys from baby products and Tesseract based OCR to extract\\nBrand names got me Rank 17 on the leaderboard\\nINTERESTS\\nML Hackathons\\nCricket\\nBadminton\\nMusic\\nAchievements/Tasks\\nRelevant Courses\\nRelevant Courses\\n',\n",
       " 'KUMAR SUBHAM\\nBachelor of Technology\\nNational Institute of Technology, Raipur\\n\\uffff krsubham48@gmail.com\\n\\uffff +91 7766 932528\\n\\uffff linkedin.com/in/krsubham48\\n\\uffff github.com/krsubham48\\n\\uffff Gurugram, India\\nPUBLICATIONS\\n• Paper published at IEEE ICCCIS 2019 titled \"SegNet – based\\nCorpus Callosum segmentation for brain Magnetic Resonance\\nImages (MRI)\"\\n• Paper published at IEEE ICECCT 2019 titled \"Corpus\\nCallosum Segmentation from Brain MRI and its possible\\napplication in detection of diseases\"\\nPROJECTS\\nMachine Translation\\n• Experiments with Encoder-Decoder and Attention Network\\narchitecture to translate French to English\\nCorpus Callosum Segmentation\\n• U-Net and SegNet based architectures for segmentation,\\nimage processing techniques for augmentation and\\npreprocessing\\n• Test set dice coe�cient of 0.967, best in domain\\n• Extensive parametric and semantic analysis on results\\nHuman Safety using Deep Learning\\n• CNN models for classi�cation, detection, segmentation and\\ninstance segmentation of humans in an image\\n• Experimentation on building a crowd-count system based on\\nCSRNet Model architecture\\nQuestion-Answer Mapping\\n• Experiments with 1-D CNN, BiLSTM and use of\\nTransformer Network to implement an Attention Model\\nto classify most relevant answer\\nText Prediction and Classi�cation\\n• LSTM based sequence models for next-word prediction and\\nexperiments with Count Vectorizer, Bag of Words, TF-IDF\\nand similar approaches for classi�cation using SVM\\nExploratory Data Analysis\\n• Classi�cation, Unsupervised Clustering and Continuous\\nvalue Prediction on common datasets\\n• Implementation of Machine Learning algorithms and\\ntechniques from scratch to understand underlying\\nmathematics on dummy data\\nEDUCATION\\nB.Tech, Electronics and Telecomm, 8.01/10.0\\nNational Institute of Technology, Raipur\\n\\uffff July 2015 – May 2019\\nAISSCE, 91.00 %\\nLoyola High School, Patna\\n\\uffff April 2012 – May 2014\\nEXPERIENCE\\nSoftware Engineer I\\nSnapdeal Private Limited\\n\\uffff Jan 2020 - present\\n\\uffff Gurugram, India\\n• Use of Deep NLP on user chat-messages to perform Intent\\nClassi�cation using LSTM based models for automatic\\nnavigation in chatbot, supported languages include English\\nand Hinglish\\n• Use of Sequence Models to predict user’s click behaviour\\nusing historical click-stream data, used for next-click and\\nsimilar product prediction\\n• Experimentation on Attention Network based\\nRecommendation System using Transformer Model variants\\n• Developed a Background Removal System based on U-Net\\nmodel architecture for product Image Segmentation from\\nnoisy images\\n• Development of Visual Search feature model by performing\\nmulti-class classi�cation on product catalog tree using CNN\\nbased architecture\\n• Extensive exploration and experimentation on development\\nof Product Graph - Knowledge Graph focused on Product\\nCatalog\\n• Core-development of Chat-Engine and Support-Panel project\\nProject Engineer\\nWipro Limited\\n\\uffff Aug 2019 - Dec 2019\\n\\uffff Bengaluru, India\\n• Working as a Python Developer in Data Analytics and AI\\ndomain\\nData Science Intern\\nFractal Analytics\\n\\uffff May 2018 - July 2018\\n\\uffff Mumbai, India\\n• Responsible for development of �rst Data-to-Text System\\nfor cuddle.ai, a subsidiary of Fractal Analytics\\n• Use of Statistics and Machine Learning algorithms for Time-\\nSeries data analysis and classical NLG to translate captured\\ninformation into human-readable summary\\nACHIEVEMENTS\\n• Award for Leading ML E�orts at Snapdeal\\n• Selected among Top 10% entrants in Microsoft AI Challenge\\nIndia 2018 and quali�ed for Final Round\\n• Finalist at Smart India Hackathon 2018, organised by MHRD\\n• First Prize in Vigyaan 2017, a National Level Project Exhibition\\nat NIT Raipur\\nSKILLS\\n• C, C++, Java, Python, Data Structures, Algorithms\\n• TensorFlow, Keras, PyTorch, Scikit-Learn\\n• Computer Vision, NLP, Attention Networks\\n• Data Science, Data Analysis\\n• Probability, Statistics, Linear Algebra\\n',\n",
       " \"SUNIL HULE\\n \\nHow to reach me:\\nCell: \\nEmail:\\n+917709308309\\nsunilhule4444@gmail.com\\nSkills\\nCoding Language:\\n Python\\nSKN Sinhgad Institute of Technology\\nTools:\\nPursuing B.E. in Computer Science\\nPersonal Profile\\nI am a final year computer science engineering\\nstudent. Extremely motivated to constantly\\ndevelop my skills.\\nI am a data science aspirant. I have experience of\\n15+ hackathons and secured podium in 4 of them.\\nAchievements\\nPlaying Chess\\nSpeed Cubing\\nBadminton\\nNumPy, Pandas, Matplotlib,\\nseaborn, scikit-learn.\\nData Analysis\\nMachine Learning\\nPredictive Modelling\\nGithub:\\nhttps://github.com/Sunil-Hule\\nLinkedIn:\\nhttps://www.linkedin.com/in/su\\nnil-hule-838418184/\\nProjects\\nAnomaly Detection  in Oil Pipeline:\\nhttps://github.com/Sunil-Hule/Anomaly-Detection-In-Oil-Pipeline\\nCross Sale Prediction (Rank 68 on Analytics Vidhya):\\nhttps://github.com/Sunil-Hule/Big-Mart-Sales-Prediction-Analytics-Vidhya\\nTinder Millennial Match (Rank 1 on DPhi):\\nhttps://github.com/Sunil-Hule/DPhi-Tinder-Millennial-Match\\nMerchandise-Popularity-Prediction-Challenge (Rank 19 on MachineHack)\\nhttps://github.com/Sunil-Hule/Merchandise-Popularity-Prediction-Challenge\\nHouse Price Prediction Challenge ( Rank 2 on MachineHack): \\nhttps://github.com/Sunil-Hule/House-Price-Prediction-Challenge---Machine-Hack\\nhttps://github.com/Sunil-Hule/Cross-Sell-Prediction\\nBig Mart Sales Prediction: \\nPredict-The-Cost-of-Sculpture (Rank 1 on HackerEarth):\\nhttps://github.com/Sunil-Hule/HackerEarth-Predict-The-Cost-of-Sculpture\\nHobbies\\nEducational Training\\n1st in Predict-The-Cost-of-Sculpture on HackerEarth.\\n1st in DPhi Tinder Millennial Match Prediction.\\n2nd in House Price Prediction Challenge on MachineHack.\\n2nd in Analytics Vidhya's Guided community Hackathon.\\nFeatured in AIM: https://analyticsindiamag.com/house-price-prediction-challenge-\\nmeet-the-winners-know-their-approach/\\n\",\n",
       " ' \\n \\n \\n \\nSUNIL SHARMA \\nPHONE: 425-647-0975 \\nE-MAIL ADDRESS: SUNILKSH@OUTLOOK.COM  \\n                                 \\n \\nSUMMARY \\n \\n\\uf0b7 \\nSeasoned Telecommunication/Data Communication professional with experience in companies \\nlike AT&T, TELUS, Acision/LogicaCMG, Ericsson-HP, Alstom, True Position, and HCL Perot \\nsystems. \\n\\uf0b7 \\nExperience in products design, development and deployment life cycle, right from identifying \\nbusiness requirements, design and implementation, testing and designing customized solutions \\nand handling complex integration projects. \\n\\uf0b7 \\nExperience in solution architecture with developing technical architecture and defining strategy \\nfor integrating products & deliver overall solution according to the customer requirement and \\nspecifications. \\n\\uf0b7 \\nExperience of customer interaction including requirement gathering and scoping, value \\npropositioning and negotiations. \\n\\uf0b7 \\nAbility to create, present, and implement competitive solutions by partnering with customers to \\nanalyze needs, gather specifications and situational requirements, and work with business \\ndevelopment personnel to create effective solutions. \\n\\uf0b7 \\nExcellent interpersonal and customer relationship management skills. \\n\\uf0b7 \\nProduced several research/white papers, case studies and other collaterals to generate sales leads \\n(Pre-sales) and responded to several RFIs/RFPs and prepared and gave product demonstrations. \\n\\uf0b7 \\nStrong team player with ability to handle multiple tasks and also works independently. \\n\\uf0b7 \\nPossess good analytical skills and problem solving capabilities along with good communication \\nskills. \\n \\nTECHNICAL SKILLS \\n \\nPROGRAMMING \\nLANGUAGES \\nPython, Java, C, C++ \\nDATABASE \\nSQL, INFORMIX, MySQL, Cassandra, MariaDB \\nDATA SCIENCE TOOLS \\nJupyter Notebook, Panda, Scikit-Learn, Numpy, SciPy, Matplotlib,  \\nElasticsearch, Logstash, Kibana, Grafana \\nOTHERS \\nHibernate, Ant, JUnit, CVS, SVN, Eclipse, XML, Visual Studio, XML, SOAP, \\nEclipse, Git \\nTELECOM/DATACOM \\nTECHNOLOGIES \\n \\nM2M/IOT,IP Messaging, LTE, HSPA/UMTS, GSM, E911, Pre-paid \\nTelecom billing, IN, OSS, ISDN, VOIP, IMS, Wi-Fi, BLUETOOTH, Network \\nManagement, International Roaming, OTA, Device Management \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nPROFESSIONAL EXPERIENCE  \\n \\n \\nMasters of Science in Machine Learning and AI \\n (Online)      \\n       \\n(Nov’ 2019– Present) \\n \\nPursuing MSc in Machine Learning and AI from Liverpool John Moores University \\n \\nProjects & Case Studies undertaken so far as part of the course: \\n- \\nTelecom Churn case Study - Predicting Customer churn for a telecom operator \\n- \\nLending Club case study - Decipher the types of customer default on loan \\n- \\nCar Pricing in US market - Understand the factors affecting the pricing of cars in US market, \\nhelping a Chinese company enter the US market \\n \\nMavenir ’USA - Senior Solutions Architect  \\n \\n \\n       \\n \\n(May 2019– Oct 2019) \\n \\nWorked as solutions architect for message store (mStore) product in IP messaging domain. \\n \\nSENTACA ’USA - Senior Solutions Architect \\n \\n \\n \\n       \\n(Jun 2015– May 2019) \\n \\nPROJECT- AT&T - Messaging Operational Certification –SAG, FDA, MWI, OTA, CPM and \\nAutomation/Data Analytics \\n \\nTechnologies/Protocols/Interfaces: Mavenir SMSC, SMS Aggregation Gateway & OTA     (FDA, \\nMWI, SAG & OTA), Charging gateway, Antispam server, ENUM server. SMPP, Sigtran, MAP, \\nDiameter, Simulators, Elastic search, Logstash, Kibana, MariaDB, YAML, Pandas, Numpy, Matplotlib. \\n \\nDESCRIPTION: As part of Messaging group operational certification, the scope of the project is to \\ncertify Messaging platforms in Pre-Production for operational readiness in production environment. \\n \\nROLE \\n\\uf0b7 \\nReview and finalize Customer requirement specification with Vendor and Client. \\n\\uf0b7 \\nTest cases planning, development and execution support, conduct Triages & troubleshoot issues.  \\n\\uf0b7 \\nProvide design support for the solution to be deployed in Pre-Production and Production \\nenvironment. \\n\\uf0b7 \\nInterface with end point nodes primes for the integration requirements for the implementation. \\n\\uf0b7 \\nProvide progress reports to both internal and external stakeholders. \\n\\uf0b7 \\nProject planning platforms pre-production validation/verification/implementation activities. \\n\\uf0b7 \\nDevelopment of tools and simulators for platform deliveries verification. \\n\\uf0b7 \\nTest case automation implementation for CPM platform using in-house Prism platform. \\n\\uf0b7 \\nInteract with vendor to coordinate the deployment and testing efforts and other onsite activities. \\n\\uf0b7 \\nData Analytics to gain insights into the network issues based on network Logs, TRL, and CDRs \\netc. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nEDUCATION \\n \\n \\nB.E (Electronics & Telecommunication) – GJ University’ India \\nPGD (ML & AI) – IIIT Bangalore ‘India \\nM.sc Machine Learning and AI (Pursuing) - Liverpool John Moores University’ UK \\nMBA -Global (Pursuing) – Deakin University’ Australia \\n \\nCERTIFICATIONS/COURSES: \\n \\nPMP Certification - PMI, PA, USA \\nEnterprise Management Certification - IIT Delhi \\nCorporate Finance Essentials - IESE Business School ‘ Coursera \\nCertified Openstack Administrator - COA \\n \\nDATA Analytics: \\n \\nIntroduction to Data Science in Python – University of Michigan (Coursera) \\nApplied Machine Learning in Python – University of Michigan (Coursera) \\nApplied Text Mining in Python – University Of Michigan (Coursera) \\nMachine Learning Foundations: A case study approach - University of Washington (Coursera) \\nMachine Learning: Regression - University of Washington (Coursera) \\nEssential Math for Machine Learning: Python Edition – EDX \\nMathematics for Machine Learning: Linear Algebra – Imperial College London (Coursera) \\nMathematics for Machine Learning: Multivariate Calculus– Imperial College London (Coursera) \\n \\nDS101: Introduction to Apache Cassandra - Datastax \\nDS201: Enterprise 6 Foundations of Apache Cassandra - DataStax \\nDS210: Enterprise 6 Operations with Apache Cassandra - DataStax \\n \\nTELECOM TRAININGS: \\n \\nMessage Store, SAG, FDA, MWI, RMS - Mavenir \\nSIMOTA DPA - Giesecke & Devrient, Canada \\nIMS & LTE-EPS Network Signaling Training Course - AWARD SOLUTIONS TX, USA \\nProduct Training/Exposure on INP, PSA, SMSC, MAG - ACISION/LOGICACMG \\nProduct Training on VMCC, DCG - ROAMWARE SYSTEMS CA, USA \\nProduct Training on Global Roamer Platform - KEYNOTE SIGOS GMBH, GERMANY \\nProduct Training workshop on RIA platform, GeoProbe - TEKTRONIX CA, USA \\nProduct training on AXE switch, TMOS, Mediation Platform - ERICSSON HP \\n \\nOTHERS \\n \\n \\nAwarded PATENT in Messaging \\nLORE Award for Best performance in Acision/LOGICACMG \\nBest Performance Award in Ericsson HP. \\nIEEE Member \\n \\n',\n",
       " 'V A I B H A V   M A T H U R                                          mathur06.vaibhav@gmail.com| 7217309517 \\n \\nEDUCATIONAL QUALIFICATION \\n Bachelor of Technology (CSE) \\n                 SRM UNIVERSITY (81.98%) \\n   2015 – 2019 \\n AISSCE (CBSE) \\n CENTRAL ACADEMY, JODHPUR (85.2%) \\n   2013 – 2014 \\n AISSE (CBSE) \\n CENTRAL ACADEMY, JODHPUR (9.4/10) \\n   2011 – 2012 \\n  \\n             PROFESSIONAL EXPERIENCE \\n \\n \\n  \\n \\n Data Scientist | Impact Analytics, Bengaluru                             \\n     Oct’20 - Present  \\n \\n• Developed in-house model using Pytorch to predict quantity sold; Calculated trend, seasonality, promo, price contributions  \\n• Used Fourier series to calculate seasonality; Added changepoints to calculate trend and include restrictions for coefficients   \\n \\n• Added upper-level seasonality for new articles; Improved overall WAPE by 15% compare to previous in-house models \\n \\nData Scientist | Impact Analytics, Bengaluru                                                                                                           Sept’19 – Oct’20 \\n• Developed a promotion optimization tool for a large warehouse club retailer that resulted in incremental revenue of 8%  \\n \\n• Predicted discount depths for articles in every cycle of year; LightGBM model used & Bayesian Opti for parameter tuning  \\n• Optimization of Margin done by Gurobi based on client’s conditions on like Spend, Revenue values compared to last year   \\n \\nData Science Intern | Impact Analytics, Bengaluru                                                                                                 Jun’19 – Sept’19 \\n• Implemented ML model to find similar product from client’s competitor website to compare their products prices with client  \\n \\n• Worked on product description to find similarity; Used NLP techniques to deal with text data cleaning and lemmatization   \\n \\n• Used TF-IDF to convert text to numerical format and created vectors for product descriptions and find cosine similarity \\n• Used prices for both type of products to get best matched products after applying cosine similarity between vectors of text \\n \\nData Science Intern | CapitalVia Global Research Limited, Indore                                                                            May’18 – Jul’18 \\n• Improved strategies of the sales department by forecasting employee’s sales using Time Series algorithm ARIMA model \\n \\n• Increased the lead by 21% using predicted lead score by Random Forest & XGBoost; Bayesian Opt for parameter tuning \\n \\n• Improved the quality of HR management by Predicted the employee attrition using LightGBM and got roc-auc of 0.88 \\n \\nData Science Intern | Veritas Techsoft Private Ltd, Noida                                                                                          Dec’17 – Jan’18 \\n• Increased the sales of a local shopping center by 20% through predicting the future sales of the different types of items \\n• Decision Tree, LightGBM algorithm; Final model contain ensemble of both best resulting models with rmse score 0.15/1  \\n \\n• Advanced the quality of content in a local magazine by predicting popularity of twitter celebrity using XGB; roc-auc metric  \\n \\n \\n \\n \\n \\n \\n \\n       PROJECTS \\n  \\nPredicting click on ad/promotion by user                                                                                                                SRM’19 \\n• Build a machine learning classifier to predict whether user will click on ad/promotion or not present on the company website \\n \\n• Used user demographics, user actions, product details as features; LightGBM algorithm used and got roc-auc score 0.92/1 \\n \\nAlzheimer Disease Detection by Machine Learning                                                                                                SRM’18 \\n• Implemented machine learning classification algorithm to build a classifier XGBoost and Random Forest algorithms used \\n \\n• Roc-Auc score 0.85/1; Medical features present such as mmse, year of education, nwbv ; used SMOTE to balance classes  \\n \\nTwitter Sentiment Analysis                                                                                                                                         SRM’17 \\n• The task was to classify racist or sexist tweets from others; cleaned tweets by tokenization, stemming, lemmatization; python  \\n• Bag-of-Words, TF-IDF to generate text features with the help of; model build using SVM with Grid Search to tune params \\n                                                                    ACHEIVMENTS AND AWARDS  \\n • Landed at Top 2% out of 6456 in WNS Analytics Wizard hackathon; organized by Analytics Vidhya    \\n • Ranked 19 out of 798 participants in Buyer’s Time Prediction Challenge; organized by Machine Hack \\n      2019 \\n      2020  \\n \\n • Achieved Rank 2 out of 6662 in HDFC Machine Learning Hiring Challenge; organized Hackerearth    \\n      2018  \\n \\n • Landed at Top 2% out of 3749 in India ML Hiring hackathon; organized by Analytics Vidhya                                  2019  \\n • Landed at Rank 34 out of 5000 in Affine Analytics ML Hackathon; organized by Hackerearth                                  2018                                                                            \\nSKILLS \\n  \\nTechnical – Python, Machine Learning, NLP, Time Series, Pytorch, Deep Learning \\nSoft Skills – Leadership, Conflict Resolution, Teamwork, Professional communication, Adaptability \\nEXTRA CURRICULAR ACTIVITIES \\nSRM \\nOrganized workshop on Machine Learning/Data Science for juniors to introduce them with these new \\ntechnologies and how they can learn and enhance their skills in machine learning; Got 2nd/6 in VolleyBall \\nCA \\nWon 2 times Bronze medal in Maths Olympiad held across India; Winner in science quiz out of 10 teams \\n \\n',\n",
       " ' \\n \\nVINCY SAGAR                         \\npaulsagar13vincy@gmail.com |+91 8800858216 | LinkedIn: https://www.linkedin.com/in/vincy-sagar-326099138/  \\nGitHub: www.github.com/Vincy13 | Blog: https://easyplacementguide.wordpress.com/ \\nPROFILE \\nOrganized and ambitious budding professional with experience in data science and business analysis with a thirst for learning \\ntop-notch skills, collaborating with great teams, sharing new ideas and leveraging my skills for the growth of the company. \\nSkills Summary \\nData Science                       Machine Learning R&D                      Statistics                       Python                         SAS                         Linux                                              \\nEXPERIENCE \\nDec’19 -\\nPresent \\nRisk Analyst II, American Express, Gurgaon \\n-Prevent monetary losses by capturing the risky transactions, high fraud rate merchants and risky customers using \\nGBM model and neural networks \\n-Increased the fraud coverage of the neural networks model from 34% to 45% that correspond to additional 100,000 \\nUSD fraud amount that can be saved by the company  \\n-Captured a fraud attack of 50000 USD on Amex cards, by generating and analyzing a weekly fraud network report \\n-Analyzed the fraud cases to evaluate patterns, suspicious activities, fraud applications and credit burst and come \\nup with new variable ideas that capture more information \\n-Personal research and development and collaborative brainstorming to improve the fraud coverage of existing \\nmachine learning models \\n-Collaborated with multiple teams in the same time period to deliver the projects in time; Held mentorship sessions \\n \\nSep’19 -\\nDec’19 \\nData Scientist, Impact Analytics, Bengaluru \\n-Predictive modelling using algorithm like linear regression and XGBoost to predict sales of the various categories of \\nproducts of a jewelry brand \\n-Improve upon existing promotions and recommend best promotions for maximum profit for a jewelry brand \\n-Worked in a team centered environment providing suggestions and assistance to fellow teammates to improve the \\nproject delivery and performance \\n \\nJun’19 -\\nSep’19 \\nData Science Intern, Impact Analytics, Bengaluru \\n- Promo recommendation and demand forecasting for a jewelry brand through EDA; Language used – Python \\n- Forecasting the weekly inventory for the products of a jewelry brand using ARIMA model; Language used - R \\n \\nEDUCATION \\nSRMIST (SRM University)                                   2015-2019                          Bachelor of Computer Science and Engineering - 7.98/10 \\nSophia Girls’ School, Meerut                             2013-2014                                                                   Higher Secondary (ICSE) - 73.40% \\nSophia Girls’ School, Meerut                             2011-2012                                                                              High School (ICSE) - 80.86% \\nPROJECTS \\nAnalysis of The Customer’s Banking Behavioral Scorecard using Machine Learning                                  SRM University 2019 \\nPredictive modelling using algorithms like logistic regression, random forest and XGBoost to predict the bad loan applications \\nPerformed data visualization for analysis and feature engineering to generate new features \\nKEY SKILLS AND CHARACTERISTICS \\nCornerstone, Hive, SAS, Linux, Jupyter, Predictive Modelling, Neural Networks, Python, SQL, Statistical Analysis, MS Excel \\nTeamwork, Timely delivery of projects, Adaptability, Effective Communication, Critical thinking, Problem-solving \\nACHIEVEMENTS \\nExpert guide on AntWak and Expertrons, providing career guidance and interview tips to the aspirants                                  2020 \\nScored 99.30% in Novartis Data Scientist Hiring Hackathon conducted on Hackerearth                                                                2020 \\n \\n',\n",
       " 'MAJELLA YUKTHA B \\n18PD17 \\n \\nOBJECTIVE \\n \\nTo obtain a position as a Student Intern for a period of six months from May\\n \\n  \\n \\n  \\n \\n \\n  \\n \\n \\n \\n \\n \\n \\n2021 to November 2021. \\n \\nACADEMIC QUALIFICATION\\n \\n \\n \\nCurrently pursuing 3\\u200brd year of 5 year Integrated M.Sc. Data Science at\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nDepartment of Applied Mathematics and Computational Sciences at PSG College of\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nTechnology. \\n \\nSKILL SET \\n \\n \\nAREAS OF INTEREST \\n \\nACADEMIC RECORD \\n \\n \\n \\n \\n \\n \\nFather’s name \\nGender \\n \\nDate of Birth \\nLanguages known \\nEmail \\nMobile \\n \\nFidelis Biju M \\nFemale \\n10\\u200bth\\u200b May 2001 \\nEnglish, Tamil \\nyukthabijuu@gmail.com \\n+91-81909-88815 \\nPermanent Address \\n3-273/5A, Green Street, \\nAnanthan Nagar, \\nGurukulam Road,  \\nNagercoil, Kanyakumari, \\nTamil Nadu – 629201. \\n \\nLanguages \\nPython, R, C, C++, Java \\nBack-End \\nOracle \\nPlatform \\nWindows, Linux \\nTools \\nMySQL \\n●\\nStatistics \\n●\\nDBMS \\n●\\nSupervised and Unsupervised Learning \\n \\nCourse \\nInstitution \\nBoard \\nCompletion By Marks (%) \\n \\nM.Sc. \\n \\nPSG College of Technology, \\nCoimbatore \\nAnna University \\n \\n2023 \\n8.22 \\n \\nXII \\nShree Sarasswathi Vidhyaah \\nMandheer, \\nMettupalayam \\nCBSE \\n2018 \\n90.6 \\n \\nX \\nAdarsh Vidya Kendra, \\nNagercoil \\nCBSE \\n \\n2016 \\n95 \\n \\nACADEMIC PROJECTS\\n \\n \\n \\n●\\nE-HumanAct, developed using \\u200bPython\\u200b, for the study of various Machine Learning\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nalgorithms to solve the problem of Human Activity Recognition and analyze the efficiency\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nof the various approaches used such as \\u200bDecision Trees, SVM, KNN and\\u200b \\u200bADA-Boost\\u200b. \\n●\\nDrug Analylab, \\u200bdeveloped using \\u200bPython\\u200b, for the analyses of various characteristics of\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\ndrug-drug interaction in the aspect of chemical similarity, target similarity and other\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nfeatures using ML algorithms\\u200b. \\n●\\nCondoQuest, \\u200bdeveloped using \\u200bHTML, CSS, JS and PHP\\u200b, is a Real Estate client-server web\\n \\n \\n \\n \\n \\n \\n \\n   \\n \\n \\n \\n \\napplication. With formulated constraints and considering human ideology, a web\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\napplication was developed that could be useful for both clients searching for homes and\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nselling homes. \\n●\\nUniSpotifer, developed using \\u200bPython\\u200b, for the purpose of analyzing and visualizing various\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nuniversities based on the quality of education, faculty and various other factors. The\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nproject uses libraries such as \\u200bTKinter,\\u200b \\u200bmpl_toolkits\\u200b and \\u200bplotly\\u200b.  \\n●\\nFuzzy-Matchy, developed using \\u200bC++\\u200b, to implement fuzzy string matching using the data\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nstructure, \\u200bBK Trees\\u200b. Given a file containing words, the contents are extracted and inserted\\n \\n \\n \\n  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nto the BK-Tree to enable searching, spelling corrections and similar word prediction.  \\n \\nEXTRA-CURRICULAR ACTIVITIES AND ACHIEVEMENTS \\n \\n●\\nCertified by ABRSM for completing fourth grade in Acoustic Guitar. \\n●\\nMember of Entrepreneur Club, PSG College of Technology. \\n●\\nMember of Pragma Club. \\n \\nDECLARATION  \\n \\nI, Majella Yuktha B, do hereby confirm that the information given above is true\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n  \\n \\nto the best of my knowledge. \\n \\nPlace: Coimbatore  \\nDate : 23/01/2021 \\n   (Majella Yuktha B)  \\n \\n',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '']"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d527122b",
   "metadata": {},
   "source": [
    "## String replace to remove unwanted characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "53a89536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CHARU BANSAL \\n \\n \\n \\n \\n \\n \\n \\n            5th Year Undergraduate \\nMb: +91-7080024445                                          Department of Mathematics & Scientific Computing, with Minor in Machine Learning and Applications \\nEmail: charub@iitk.ac.in \\n \\n \\n \\n \\n \\n             \\n    Indian Institute of Technology Kanpur \\nTechnical Skills \\n\\uf0b7 Programming languages  \\nC/C++, HTML/CSS, R, Python, MATLAB \\n\\uf0b7 Operating System  \\nWindows, Linux \\nEducational Qualifications \\n \\nYear \\nDegree/Board \\nInstitute/School \\nCGPA/% \\n2018-2019 \\nM.S. (MTH) \\nIIT Kanpur \\n8.0* \\n2014-2018 \\nB.S. (MTH) \\nIIT Kanpur \\n6.9 \\n2014 \\nAISSCE (CBSE XIIth Board) \\nJai Academy, Jhansi \\n92.8 \\n2012 \\nAISSE (CBSE Xth Board) \\nJai Academy, Jhansi \\n10.0 \\n*CGPA at the end of 8th Semester \\nExperience \\nSummer Intern @Accenture Digital (Project on Fraud Detection in Procurement Analysis) \\n \\n \\n      \\n          [May’18-Jul’18] \\n\\uf0b7 Performed Outlier Detection on Procurement Data using unsupervised k-NN density approach, Local Outlier Factor, k means Clustering and one \\nclass SVM in R. \\n\\uf0b7 Built a User Interface in R Shiny, to integrate all the processes for the user. The platform provides the user with the option of selecting and filtering \\ndata, choosing the model and its hyperparameters, and view interactive plots of the required variables. \\n\\uf0b7 Tested the program on six specific use cases of fraud, for comparing models and analyzing their effectiveness. \\nProbabilistic Machine Learning Project on ‘Zero-Shot Learning’ \\n \\n \\n \\n \\n \\n    \\n        [Aug’17-Nov’17] \\nProject under Prof. Piyush Rai, Computer Science Department, IIT Kanpur \\n\\uf0b7 Predicted the distribution parameters of unseen classes using regression on distribution parameters of seen classes. \\n\\uf0b7 Employed and compared Multivariate Regression Tree and Gaussian Process Regression for prediction. \\n\\uf0b7 Attempted Generalised Zero Shot Learning by sampling data from the predicted distribution of unseen classes. \\nNatural Language Processing Project on ‘Sentiment Analysis on Tweets’ \\n \\n \\n \\n \\n      \\n          [Jan’18-Apr’18] \\nProject under Prof. Harish Karnick, Computer Science Department, IIT Kanpur \\n\\uf0b7 Analyzed and classified sentiments of tweets into positive and negative. \\n\\uf0b7 Used the BBoW, tf, tfidf, Word2Vec, & GLoVE representations for feature extraction, after their tokenization and pre-processing. \\n\\uf0b7 Classified the tweets using Naïve Bayes, Logistic Regression, SVM, Feed Forward Neural Net (Multi-Layer Perceptron) and LSTM. \\nTime Series Analysis Project on ‘GDP Forecasting’  \\n \\n \\n \\n \\n \\n \\n \\n        [Aug’17-Nov’17] \\nProject under Prof. Amit Mitra, Mathematics and Statistics Department, IIT Kanpur \\n\\uf0b7 Analyzed for trend, seasonality, stationarity, and predicted GDP based on its time series, in R \\n\\uf0b7 Implemented the ARIMA model, Holt-Winters Smoothing, Augmented Dickey-Fuller, KPSS, & Ljung Box tests for further analysis. \\nRegression Analysis Project on ‘Statistical Modelling of Housing Prices’ \\n \\n \\n \\n \\n       \\n          [Jan’17-Apr’17] \\nProject under Prof. Sharmishtha Mitra, Mathematics and Statistics Department, IIT Kanpur \\n\\uf0b7 Designed a consistent Linear Multiple Regression Model to predict Housing Prices; employing a series of steps; fitting the usual Ordinary Least \\nSquares model, residual analysis, checking multicollinearity and variable selection; for the process. \\nStatistical Simulation and Data Analysis Project on ‘Identifying Authenticity of Currency Notes’  \\n \\n                            [Jan’18-Apr’18] \\nProject under Prof. Debasis Kundu, Mathematics and Statistics Department, IIT Kanpur \\n\\uf0b7 Modelled the Swiss bank data set on real and fake currencies with a two component Gaussian Mixture Model using Expectation Maximization \\nalgorithm, employed AIC/BIC scores to decide the shape and correlation structure of the clusters. \\n\\uf0b7 Checked and dismissed the requirement of k means parameter initialization and soft clustering classification. \\nMachine Learning Project on ‘E-mail Spam Filtering’ \\n \\n \\n \\n \\n \\n \\n \\n        [Aug’16-Nov’16] \\nProject under Prof. Piyush Rai, Computer Science Department, IIT Kanpur \\n\\uf0b7 Explored different classifiers for spam filtering and compared the results obtained to get a fair and comparative idea about the accuracy of various \\nlearning algorithms. \\n \\nScholastic Achievements \\n\\uf0b7 Secured AIR-1496 in JEE Advanced-2014 out of the top 150,000 applicants selected in JEE Mains 2014. \\n\\uf0b7 Secured AIR- 838 in GATE-2018 in Mathematics. \\n\\uf0b7 Selected among the top 1500 students qualified for the final interview round of NTSE 2010, out of the 300,000 applicants. \\n \\nRelevant Courses  \\n● Data Structure and Algorithm \\n● Machine Learning Techniques \\n● Probabilistic Machine Learning \\n  ● Regression Analysis \\n● Applied Stochastic Processes \\n● Applied Game Theory \\n \\n● Probability and Statistics \\n \\n  ● Inference \\n● Time Series Analysis \\n \\n● Natural Language Processing \\n● Statistical Simulation & Data Analysis   ● Data Mining* \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n*On-going Courses \\nPositions of Responsibility \\n● President, BloodConnect, Kanpur \\n \\n● Coordinator, Fine Arts Club, IITK \\n \\n● Student coordinator, Raktarpan, NSS \\n \\nExtra-Curricular Activities \\n\\uf0b7 Mentored 4 underprivileged students of class IX under the program NSS. \\n\\uf0b7 Was part of team of 12 members, who organized and went for a week-long trek to the Chandrashila peak (Uttarakhand) and back. \\n\\uf0b7 Arranged donation for underprivileged kids as social initiative by Inter IIT Sports Meet’16. \\n\\uf0b7 Performed and mentored Sand Art performance during various institute functions like Freshers’ night and Suicide Prevention Day. \\n',\n",
       " 'Ganti Sri Naga Sai Ajay Kamal\\najayganti3@gmail.com | +91 958-111-3007 | D.No 16-5-5, Sri Sai Surya Apartments, T-4, Sri Ram Nagar,\\nRajahmundry, Andhra Pradesh - 533105 | linkedin.com/in/ganti-ajay/ | github.com/ajayganti3\\nSUMMARY\\n• Experienced IT professional with 2.4 years of overall experience\\n• Experience in Web Application Development, Machine Learning and writing automation scripts for automating web\\napplications\\nEXPERIENCE\\nAssociate, Cognizant Technology Solutions\\nSep 2018 - Present\\n• Working for CVS client in developing and automating applications.\\n• Experience in developing Machine Learning model and creating API for the same to make it integrable with the\\nASP.NET web application\\n• Experience in developing Web Application with ASP.NET\\n• Experienced in each stage of software development life cycle Designing, Development, Testing and Documentation\\n• Experience in writing automation scripts for automating web application using selenium framework\\n• Able to deliver projects within deadlines with high quality\\n• Quick Learner, Good Team Player and have the ability to work independently\\nPROJECTS\\nNLP Classiﬁcation - CVS\\n• Classiﬁed contents / comments into speciﬁc categories using supervised machine learning techniques\\n• Used pre-processing techniques to remove unwanted combinations / words from data. Stop word removal, word tok-\\nenization and lemmatization we used to avoid data redundancy.\\n• Developed an API using Flask and integrated it with the existing web application\\n• Technology: Python, NLTK, NLP Preprocessing, Sci-Kit Learn, XGBoost, Flask\\nCassava Leaf Disease Classiﬁcation - Kaggle\\n• Developed a Deep Learning model (CNN) using Tensorﬂow and Keras for classiﬁying the images.\\n• Performed Cross Validation and achieved accuracy of 88.2%\\n• Technology: Deep Learning, Python, Tensorﬂow, Keras\\nMicrosoft Malware Detection\\n• Analysed and performed Feature Engineering on ASM ﬁles and byte ﬁles for identifying diﬀerent type of malware.\\n• Applied diﬀerent types of Machine Learning algorithms along with Grid Search and Random Search for hyperparameter\\noptimization and achieved log loss of 0.012\\n• Technology: Python, Sci-Kit Learn, XGBoost\\nScania Trucks APS Failure Prediction\\n• Performed Exploratory Data Analysis, Visualization, Feature Engineeting techniques\\n• Experimented with diﬀerent types of Machine Learning Classiﬁcation algorithms like Decision Tree, Random Forest,\\nSVM, Gradient Boost and Ensemble models like Stacking Classiﬁer on the data.\\n• Technology: Python, Sci-Kit Learn, XGBoost, SVM, Random Forest\\nEDUCATION\\nBachelor of Technology, Electronics and Communication\\nSep 2014 - Apr 2018\\nMVGR College of Engineering\\nPercentage: 79.9\\nSKILLS\\nProgramming Languages: Python, C#, Java\\nDatabase: MS SQL Server, My SQL\\nProject Management: TFS, GIT\\nFrameworks and Libraries: Pandas, Numpy, Sci-kit Learn, Tensorﬂow, Keras, Flask, Streamlit and PyTorch\\nOperating Systems: Windows, Linux\\nCERTIFICATIONS\\n• Applied AI Course\\n• Data Analysis, Visualization and Machine Learning from IBM Cognitive Class\\nPUBLICATIONS\\n• Published a paper in IEEE with title \"Interfacing of Matlab with arduino for face detection and Tracking Algorithm\\nusing serial communication\" with DOI: 10.1109/ICICI.2017.8365276\\n',\n",
       " 'Ajinkya Takawale\\n1-5-10 506, Sailor Komatsugawa, Edogawa-ku, Tokyo 132-0034\\n� ajinkya.takawale97@gmail.com\\n� ajinkyaT\\n� ajinkyaT\\nEducation\\n○ Indian Institute of Technology (IIT) Kharagpur\\n2015-2019\\nB.Tech. (Hons.), Department of Mechanical Engineering\\nThesis on Machine Learning applications in rural healthcare\\nTechnical Skills\\n○ Languages: Python, JavaScript, Java\\n○ Machine Learning: Scikit-learn, PyTorch, Tensorﬂow, Pandas, Numpy\\n○ Web Stack: React JS, Spring Boot, MySQL, Docker\\n○ AWS Services: Lambda, ECS, ECR, Batch, Step Functions, CloudWatch\\nWork Experience\\nMachine Learning Engineer - BizReach\\nTokyo, Japan\\nRecommendation Engines | ML infrastructure\\nOct 2019–Present\\n○ Identiﬁed target users for premium membership using BigQuery action logs and LightGBM. Improved f1 score by 15%\\n○ Designed and developed chatbot for internal use using Dialogﬂow and backend in python and hosted on GCP\\n○ Created an API endpoint using trained Deep Learning PyTorch model using SageMaker and AI-Sagify\\n○ Used interleaving as a better alternative to AB testing using AWS Batch, Step Functions, ECR\\n○ Created Spot instance management script using Step Functions for daily Sagemaker Jobs reducing cost up to 90%\\nInternships\\nEngineering Intern - Airbus India\\nBangalore, India\\nNatural Language Processing | Chatbot Development\\nMay 2018–July 2018\\n○ Developed an FAQ answering chatbot using open source chatbot framework to address customer care requirements\\n○ Worked on NLP components as intent classiﬁcation, named entity recognition and dialogue ﬂow management\\n○ Added a GUI dashboard for training data visualization and easy addition, editing and labelling of training data\\nMachine Learning Intern - Rorodata\\nHyderabad, India\\nComputer Vision | Deep Learning\\nDec 2017–Jan 2018\\n○ Developed Deep Learning models in the ﬁeld of Biomedical Image diagnosis such as segmentation and classiﬁcation\\n○ Implemented paper, \"Learning to Count Objects in Images\" by VGG group to count number of cells in an image by CNN\\n○ Utilized Gradient-weighted Class Activation Mapping (Grad-CAM) to highlight regions in the image for a prediction\\nSoftware Engineering Intern - Harbinger Systems\\nPune, India\\nNatural Language Processing | Software Development\\nJune 2017–July 2017\\n○ Studied Natural Language Processing and Information Retrieval methods for retrieving similar documents\\n○ Transformed text into tf-idf matrix followed by using cosine similarity to measure similarity of the matching documents\\n○ Surveyed the then existing chatbot as a service alternatives in the market for the task\\nIndustrial Training - Tata Motors\\nPune, India\\nAssembly Line Planning | Fish-bone Analysis | Engine Manufacturing\\nMay 2017–June 2017\\n○ Studied square pattern engine assembly line with the aim of reducing the number of engine rework done\\n○ Assisted in gathering data regarding cycle time, number of engine failures at cold and hot bed testing\\n○ Increased eﬃciency by 10% by analyzing and tracking key issues causing engine failure at testing\\nProjects\\nRunBook - Social media website | BizReach Training\\n○ Developed a complete website with front-end in ReactJS and MVC back-end in Spring framework and deployed on AWS\\nGoogle Summer of Code 2018 | Open Source Development | Audio Visual Speech Recognition\\n○ Developed a part of speech recognition system using both audio and lip video in scenario’s where audio is corrupted\\nSmart India Hackathon 2018 | Indian Space Research Organization (ISRO) | Deep Learning and Computer Vision\\n○ Led a team to ﬁnals to develop Content-Based Image Retrieval System (CBIR) for satellite images using Deep Learning\\nUltimate Student Hunt 2017 | Machine Learning Competition | Time Series Forecasting\\n○ Ranked 2nd in a competition to predict the footfall in the future given only limited past hourly timestamp data\\nExtra Curricular\\n• Qualiﬁed Indian National Chemistry Olympiad (INChO) ranked within top 1% out of 30,000 in the country\\n• Ministry of Science and Technology INSPIRE Scholar • Kishore Vaigyanik Protsahana Yojana (KVPY) Scholar\\n',\n",
       " 'Anant Mundra \\nPre-final year student \\nB.Tech (information technology (8.23 cgpa) \\nIndian Institute of Information Technology, UNA (IIIT Una) India\\nEDUCATION \\nSENIOR SECONDARY SCHOOL – [2016-2018] \\nMaheshwari Public School, Kota (79.8%, CBSE) \\nSECONDARY SCHOOL – [2014-2016] \\nThe Aditya Birla Public School, Chittorgarh (9.6 cgpa , CBSE) \\nResearch Intern at ALPHABETA INC  - A deep financial technology firm using  \\nVisualization, AI, DLT and Edge Computing.  \\nJune 2020-Aug 2020 (Certificate)  \\n• Worked on a set of problems faced by migrant labour in India during CoVID-19 lockdowns. \\nMy role was to build data driven models for most effective ways to: \\n• Disburse govt help to migrant labours, and  \\n• Bring them back to their home states, safely. \\n• Worked on to build a data driven model for govt incentives in commercial lending to MSME \\nstarting with stressed sectors facing CoVID-19 induced problems. \\n• Designing an appropriate model on raising capital for OLA keeping in mind the CoVID-19 \\nimpact on business. \\nMachine Learning – Teaching Assistant at ALPHABETA INC on Cocalc .  \\nSept 2020-Nov 2020 \\n• Tasked to build evaluation model including assignments, interactive quizzes for FinTech \\ncourses, to be delivered to Engineering and Management students at a leading university. \\n• Updated the CoCalc environment with relevant processes for evaluation \\nLEARNING & ACHIEVEMENT: \\n• Part of technical team at ALPHABETA, I worked on building multiple machine learning \\nmodels, doing various data analysis.\\nLANGUAGES\\nENGLISH\\nHINDI\\nCOURSES COMPLETED \\n• Joy Of Computing Using Python (NPTEL) \\nAchieved Silver Medal with 87.5% \\n• Complete Data Science Boot-Camp (UDEMY) \\nComplete Data Science Training: Mathematics, Statistics, Python, Advanced Statistics in Python,  Machine \\nand Deep Learning. \\n• Introduction to Investing and Portfolio Management (ALPHABETA) \\nBasics of stock market, looking at risk-return relationship, exploring asset classes, passive and active \\ninvesting using ETFs, and Index Funds.\\nACTIVITIES & AWARDS\\n• PARTICIPATED IN THE IIIT-JABALPUR \\n(GUSTO’20) SPORTS-FIESTA TABLE-\\nTENNIS TOURNAMENT. \\n• PARTICIPATED IN THE CBSE CLUSTER \\nWEST ZONE HANDBALL TOURNAMENT. \\n• IN MY FREE TIME I TAKE OUT TIME FOR \\nSPORTS (SWIMMING) AND ENJOY \\nWATCHING THRILLER/CRIME SERIALS.\\nSKILLS SUMMARY\\nCOMPETITIVE PROGRAMMER\\nPYTHON PROGRAMMING \\nLANGUAGE\\nDATA SCIENCE   \\nPYTHON | TABLEAU  \\nHTML/CSS\\nSUPERVISED ML\\nUNSUPERVISED ML\\nDEEP LEARNING\\nC/C++ LANGUAGE\\nanantmundra11@gmail.com\\n+91 63781.33616\\nCONTACT\\nWORK EXPERIENCE  \\nResearch Intern at  IIT MANDI \\nJan 2021 – ongoing (6 months) \\nROLE:  \\n•\\nPart of a team, working on the growth of plants. Building data driven models to \\nanalyse different factors affecting the growth.  \\n•\\nBuilding various regression models for weather forecasting. \\n•\\nUni-variate & Multi-variate time series analysis. (Prophet, ARIMA, LSTM) \\n•\\nCorrelating local weather data to the global dataset, with the actuals.\\n',\n",
       " 'ANURAG GUPTA\\nData Scientist\\nData scientist with experience in solving many real-world business problems across diﬀerent domains such as Retail, Banking and Financial sectors by\\nexecuting data-driven solutions.\\nanuragiitr9ag@gmail.com\\n8979546574\\nGurgaon, India\\nlinkedin.com/in/anuraggupta22\\nWORK EXPERIENCE\\nAssociate, Analytics\\nZenon\\n01/2019 - Present, \\nGurgaon, India\\nSenior Business Analyst, Analytics (01/2019 - 12/2020)\\nFor a Fortune 500 BFSI Firm, developed and implemented Machine Learning-\\nbased models such as XGBoost, GBM, and Ensembling techniques to enable the\\ndelivery of eﬀective and eﬃcient campaigns that enable the client to target the\\nappropriate audience within an actionable time frame.\\nIndustry Classiﬁcation Model and Market Basket Analysis for a leading ﬁnancial\\nservices company, Leveraged Natural Language Processing(NLP) assisted model in\\nclassifying target companies which were a critical input for the sales & marketing\\nteam to develop tailored strategies for each industry.\\nWorked on-site and closely with the client leadership teams apart from model\\ndevelopment by providing a commercial and economic interpretation of model\\ninsights that contributes to the overall strategic decision-making process.\\nWorked on developing the Economic Stress Score product for Businesses and\\nCustomers to predict the potential risk in the coming future.\\nInsights Library Project for a Healthcare Firm, Modularized capability to extract\\ninsights and present relevant, and personalized insights to end-user using\\nAdvanced ML.\\nAnalytics Specialist\\nOpera Solutions\\n09/2018 - 01/2019, \\nGurgaon, India\\nGross Sales Model for Prospect Targeting(Fortune 500 BFSI Firm) (09/2018 -\\n11/2018)\\nData Scientist\\nImpact Analytics Pvt. Ltd\\n06/2017 - 08/2018, \\nBangalore, India\\nPromo Eﬀectiveness for Large Scale Oﬃce Supplier (08/2017 - 11/2017)\\nMeasured eﬀectiveness of historical promotions against the baseline to identify\\ntoxic promotions and predicted their performance for the future by\\nDeveloping multi-linear regression to predict baseline sales, Various indirect\\neﬀects such as aﬃnity, halo, cannibalization, and pull-forward were calculated to\\ncapture promotion impact in a holistic manner\\nKey-value categories and Key-value items analysis to improve price perception\\nfor Fast Fashion Retailer(12/2017 – 05/2018) - Performance metrics and statistical\\nmethods used to identify KVCs and KVIs. Created a price elasticity model and\\nperformed aﬃnity analysis to be used as the key metric.\\nStrong understanding of advanced tableau features including calculated ﬁelds,\\nparameters, table calculations, join, dashboard action buttons, context ﬁlters and\\ndata blending. Worked on Promotion Management tool for promo optimization for\\nRetail client\\nEDUCATION\\nIDD B. Tech. (Chemical Engineering) and M. Tech.\\n(Hydrocarbon Engineering)\\nIIT Roorkee\\n2012 - 2017, \\nRoorkee, India\\nSKILLS\\nMachine Learning, Deep Learning, Statistics ,NLP,\\nData Visualization\\nPython, Tensorﬂow, Keras, R, SQL, GoogleBigQuery,\\nHive, Tableau\\nNeural Networks/CNN/RNN/Image Classiﬁcation\\nRegression,Decision Trees/Bagging/Boosting\\nPERSONAL PROJECTS\\nDeep Neural Network using Tensorﬂow\\n1. Build the 4-layer deep neural net to achieve the accuracy of 80%\\non the test dataset in classifying the cat images.2. Build a deep\\nneural network model to recognize numbers from 0 to 5 in sign\\nlanguage on the SIGNS dataset with 80% accuracy.\\nCNN Applications using Keras\\n1. Emotion Detection :Detect if someone\\'s emotion is classiﬁed as\\n\"happy\" or \"not happy.\"| 2. Objection detection using YOLO\\nalogorithm\\nTransfer Learning Applications using FaceNet\\n1.Build a face recognition system | Implement the neural style\\ntransfer algorithm andgnerate dnovel artistic images\\nCERTIFICATES & EXTRA\\nCOURSES\\nDeep Learning Specialization\\ndeeplearning.ai\\nNeural Network and Deep Learning\\ndeeplearning.ai\\nConvolution Neural Networks\\ndeeplearning.ai\\nMachine Learning\\nStanford University, Coursera\\nThe Analytics Edge\\nMIT University, Edx\\nStatistical Thinking for Data Science and Analytics\\nEdx\\nACHIEVEMENTS\\nIIT-JEE/AIEEE\\nAIR 3085/2455\\nCompetitions and Hackathons\\nParticipated in various data science competitive platforms and\\nsecure top 10 percentile rank.\\nINTERESTS\\nReading\\nTraveling\\nProjects at Zenon\\nProjects at Opera\\nProjects at Impact\\n',\n",
       " 'Aravind Pai\\nData Scientist\\nData Scientist with hands-on experience in solving real world\\nproblems. Solving some of the challenging research driven\\nproblems in daily life.\\naravindpai23@gmail.com\\n+91 8074101068\\nGurugram, India\\nwww.analyticsvidhya.com/blog/author/aravindpai/\\nlinkedin.com/in/aravind-pai\\ngithub.com/aravindpai\\nWORK EXPERIENCE\\nData Scientist\\nSpektacom\\n07/2020 - 04/2021, \\nBangalore, India\\nDeveloped computer vision-based products for cutting-\\nedge technologies in sports and deployed them in\\nproduction.\\nData Science Intern\\nAnalytics Vidhya\\n12/2019 - 06/2020, \\nGurugram, India\\nDeveloped Proof of Concept for Ball Tracking and Net\\nAssistant System for cricket using Computer Vision.\\nData Science Summer Intern\\nAnalytics Vidhya\\n05/2019 - 08/2019, \\nGurugram, India\\nExtracted highlights from a cricket video without using\\nMachine Learning and Deep Learning\\nGenerated one line summary for the long movie reviews\\nusing Attention based Encoder Decoder architecture\\nData Science Intern\\nProbyto\\n06/2018 - 11/2018, \\nCoimbatore, India\\nDevelops custom AI Solutions, provide fully managed AI services and\\nachieved AI innovation at scale\\nDeveloped a prototype for capturing the Adverse Drug\\nReactions from the user comments\\nDesigned and built streaming data pipeline for the social\\nmedia data using Amazon Web Services\\nEDUCATION\\nM.Sc 5 year Integrated Data Science\\nPSG College of Technology\\n2015 - 2020, \\nCGPA: 8.0/10\\nHigher Secondary Course\\nNarayana Junior College\\n2014 - 2015, \\n97.8%\\nSecondary Course\\nNarayana High School\\n2012 - 2013, \\nCGPA: 9.8/10\\nSKILLS AND TOOLS\\nMachine Learning\\nDeep Learning\\nComputer Vision\\nNatural Language Processing\\nPytorch\\nFastai\\nKeras\\nPython\\nSQL\\nGoogle Data Studio\\nACHIEVEMENTS\\nAuto No Ball Patent\\nOwn the inventorship rights on Auto No Ball patent\\nCOURSES\\nDeep Learning Specialization by Andew NG\\nStanford CS 224N: Natural Language Processing with\\nDeep Learning\\nPROJECTS\\nAugmented Reality for Sports\\nDeveloped an Augmented Reality application for the biopic of Rahul\\nDravid that brings covers to life.\\nCricket Commentary Analysis\\nAnalyzed the Indian team performance using the cricket commentary\\ndata\\nNeural Machine Translation system\\nDeveloped a model to convert text in one language to text in another\\nlanguage\\nLANGUAGES\\nEnglish\\nFull Professional Proﬁciency\\nKannada\\nFull Professional Proﬁciency\\nHindi\\nProfessional Working Proﬁciency\\nTelugu\\nProfessional Working Proﬁciency\\nTamil\\nProfessional Working Proﬁciency\\nAchievements/Tasks\\nAchievements/Tasks\\nAchievements/Tasks\\nAchievements/Tasks\\n',\n",
       " ' \\n \\n \\n \\n \\n \\nSENIOR DATA SCIENTIST \\n● Machine Learning ● NLP ● Deep Learning ● AI ● Semantic Search Engine ● RASA ● Python \\n \\nPROFESSIONAL SUMMARY \\n \\nSenior Data Scientist (Kaggle Competition Expert) with a strong math background and experience in Advanced \\nMachine Learning, NLP, RASA NLU, Deep Learning and Statistics. Total 8+ years of experience in Data Science. \\n \\nWhile working in current company, achieved value realization of 30 Million dollars through implementation of \\nvarious operations project in different towers for APAC zone. While working as APAC Zone lead and Commercial \\nTower lead, build a 20+ project pipeline for the team. \\nDeveloped Context Based Search Engine Product using fastText, NLP and Python. This includes a feature of real \\ntime Over the Top training of fastText model using event Queue (Redis) processing and Task Scheduler (Huey – \\nlight version of Celery). \\nTECHNICAL SKILLS SUMMARY \\n \\n• \\nMachine Learning: Classification, Regression, Clustering algorithms; Predictive and Statistical Modelling;  \\nEnsemble models; Advanced ML( LightGBM and Xgboost ), Hyperparameter Optimization; Natural \\nLanguage Processing(NLP) using Spacy and Gensim; Word2Vec, Word Embeddings;  Intent and Entity \\nextraction using RASA NLU; Context based Search Engine(Semantic and Syntactic) using fastText; \\nTimeseries Forecasting; Recommendation Engine, Collaborative Filtering; Deep Learning (CNN, LSTM, RNN), \\nKeras, Pytorch, Transformers, BERT, NER, Question and Answering Model; SparkNLP; H2O AutoML, \\nAnomaly Detection, Unsupervised Clustering(Kmeans and DBSCAN). \\n• \\nSoftware and Programming Languages: Python (scikit-learn, numpy, scipy, pandas), Huey (as task \\nscheduler), Redis (as event queue processor), Flask (Rest API), Linux, GitHub, Pie Git Cloud, Azure Dev Ops, \\nJava, SQL, Selenium WebDriver, Oracle. \\n• \\nTools: Power BI, SparkBeyond, Microsoft Excel, Macro. \\n• \\nDomain Knowledge:   Retail, Banking, Health Care and Financial. \\n \\nPROJECTS | HACKATHONS \\n \\nParticipated in various Machine Learning and NLP Hackathons which involves Data mining, Data Analysis, Data \\npre-processing, applying machine learning algorithms and doing predictive analysis. Performed well in these \\nHackathons and below are the details – \\n• \\n2019 Data Science Bowl – Kaggle Hackathon – Got Silver Medal -  \\nCreated a model to predict the performance of children on PBS Kids Measure Up app based on their \\nhistory learning records. Created advanced Feature engineering by using Business knowledge and did \\nASHISH KUMAR SINGH \\n \\nE-mail: ashishkr.30@gmail.com  \\n \\n \\n \\n               Contact: +91 9619794540 \\nstacking and Ensembling of LGBM models. Used techniques like pseudo labelling and Customized \\nStratified Group Kfold Cross validation technique. Kaggle id – Ashufet. \\n \\n• \\nNFL Big Data Bowl – Kaggle Hackathon – Got Silver Medal -  \\nCreated a model to predict how many yards will an NFL player gain after receiving a handoff. Our models \\nwere used to predict real time NFL games and then based on Continuous Ranked Probability Score (CRPS) \\nParticipants were ranked. \\nApplied geometry-based Feature engineering using X, Y coordinates of players and did Ensembling of \\nNeural network and LGBM models. Kaggle id – Ashufet. \\n• \\nHome Credit Default Risk – Kaggle Hackathon – Got Bronze Medal -  \\nCreated a model to predict the probability that a customer would be a defaulter in paying his/her loans as \\nper the last loan records and application data. Training data consists of 7 different files having data from \\ndifferent sources with more than 4lac records. \\nApplied various advanced ML algorithms and advanced Feature Engineering techniques like Target Mean \\nEncoding, Aggregation methods and so on. Kaggle id – Ashufet. \\n \\n• \\nLinguipedia Code Fest NLP conducted by IIT BHU – Analytics Vidhya Hackathon – Secured 6th rank - \\nUsing NLP, Spacy and ML algorithms created a model which can classify the sentiments of the tweets \\nwhether it is a positive or negative review comments for different products. Techniques used like \\ntfIdfVectorizer, CountVectorizer and Gensim Word2Vec.  Analytics Vidhya id – ashishkr.30@gmail.com \\n \\nWORK EXPERIENCE \\nABInBev \\n                            Jan 2020 — Present \\nSenior Data Scientist \\nCurrently, working as APAC Zone lead and Commercial tower lead where I am managing around 10+ projects \\nand have also worked as Individual Contributor for multiple projects. \\nProject    :    Sales Team Target Forecasting  \\n Client  :      Internal \\nDeveloped a Forecasting Model to predict the daily target for the China sales team for each POC/SKU/Shift. Total \\nadditional sales worth of 10 Million USD for 1 year. Productized the complete solution using Azure Dev Ops. \\nProject    :    Commercial Area Optimization  \\n Client  :      Internal \\nBuild a solution for Commercial Area Optimization project within 2 months which gave around 15 Million USD \\nsavings yearly. In this project, goal was to optimize the sales team commercial area so that ABInBev Market share \\nincreases. \\n \\nProject    :    Aged GR Prediction  \\n Client  :      Internal \\nDeveloped a Classification model to predict Aged GR (GRNI - Good Received Not Invoiced) to save Inadvertent \\npayments which affect EBIDTA and cash flows. Build LightGBM model to do the prediction with F1 score of 95% \\nand Recall of 98%. Value saving of 5 Million USD. \\nSopra Steria \\n                            Jun 2019 — Dec 2019 \\nLead Data Scientist \\nProject     :     INTELLIGENT SEARCH                                     \\nClient     :     Internal (across all projects) \\nManaged a team of 10 people to develop Context based Search Engine using fastText model which was trained \\non data available on different portals of over 200 projects. Data included documents (docx, pdf, xlsx, pptx, txt, rtf) \\nof SharePoint, content of WIKI, JIRA and Outlook Calendar. Provided the feature of Real time Over the Top \\ntraining of fastText model for document add, update and delete using event Queue (Redis) processing and Task \\nScheduler (Huey – light version of Celery). Used Context based Spell Checker (complete logic developed by me) \\nto correct the spelling mistakes and used tf-Idf vectorizer as a booster to get an accuracy of around 90%. \\n \\nTata Consultancy Services \\n                          Mar 2013 — May 2019 \\nData Scientist \\nProject     :     Custom Entity Extraction from Search Query   \\nClient     :     Apple \\nImplemented NLP search bar on the client dashboard so that instead of manual selection of filters on UI, user can \\ntype the query in NLP search box and get the filters set on UI. To solve this business requirement we have used \\nRASA NLU for customized entity training and entity extraction (NER-CRF algorithm) from the NLP query. Other \\ntools which we have used are CHATITO for training data generation. We have created end to end product which \\nincluded creating model, creating pipeline for production deployment using Flask and Gevent WSGIServer. \\n \\n                                              ACHIEVEMENTS \\n• \\nQualified Gate exam in 2012 with 97 percentiles. \\n• \\nGot 142nd rank in ISRO (Indian Space Research Organization) exam and 112th rank in NPCIL (Nuclear Power \\nCorporation of India Ltd) exam all over India in 2012. \\n \\nACADEMIC CREDENTIALS \\nB.Tech [ECE] \\n                                                2008 — 2012 \\nFaculty of Engineering and Technology, Gurukul Kangri Vishwavidyalaya, Haridwar - 79.20% \\n \\nHSC [12th] \\n                                                2007 — 2008 \\nRam-Eesh International School, Greater Noida, Uttar Pradesh, CBSE board - 86% \\n \\nSSC [10th] \\n                                                2005 — 2006 \\nRam-Eesh International School, Greater Noida, Uttar Pradesh, CBSE board – 84.6% \\n \\nPERSONAL DETAILS \\nFather’s Name          :   Ashok Kumar Singh                                      Date of Birth              :   3rd April, 1991 \\nCurrent Address        :   Bangalore, Karnataka                                 Permanent Address  :   Greater Noida, UP -201310                           \\nHobbies                       :   Coding and taking part in various Hackathons, Gaming, Playing chess and cricket.                  \\n',\n",
       " '                                    BALLA SAI HARSHITH \\nAddress: Door no:19-1-13,                                   Email: saiharshithballa99@gmail.com                                                                                                                                                                                 \\nChinamamidipalli village,                                    Mobile no: +91-9493105147 \\nNarsapuram mandal,                \\n \\n      Github: https://github.com/harshithballa                                                                                         \\nWest Godavari-534275.   \\n \\nCAREER OBJECTIVE: \\n To work in pragmatic way in an organisation where I can show my talent and enhance \\nmy skills to meet company goals and objectives with full intensity. \\nACADEMIA: \\nQualification: Name of the \\ninstitution: \\nBoard/University: \\nYear of \\npass: \\nscore: \\nB.Tech(ECE) \\nSRKR Engineering \\nCollege, \\nBhimavaram \\nAndhra University \\n   2020 \\n8.86 \\nIntermediate \\nSri Chaitanya \\nJunior College, \\nVijayawada \\nAP-Intermediate \\n   2016 \\n96.8% \\nUI  SSC \\nBhashyam EM \\nHigh School \\nBoard Of Secondary \\nEducation, \\nAndhra Pradesh \\n   2014 \\n9.7 \\n \\nWORK EXPERIENCE: \\nSep 2020- Present: \\n• \\nWorking as a Software engineer in TCS. \\nAREAS OF INTERESTS: \\n• \\nArtificial Intelligence \\n• \\nMachine learning   \\n \\nTECHNICAL SKILLS: \\n• \\nSoftware languages: C, Java and Python. \\n• \\nLabVIEW programming. \\n• \\nTensorflow and OpenCV. \\n• \\nArduino  programming \\n• \\nMS Word, MS Power Point, MS Excel. \\nCERTIFICATIONS: \\n• \\nDone Deep learning Specialization and Tensorflow in Practice Specialization from \\ndeeplearning.ai ( Coursera ). \\n• \\nCertified for completing the Fundamentals of Machine Learning A-Z course( by \\nUdemy). \\n• \\nCertified for completing the Cisco Introduction to IoT course. \\n• \\nCertified for completing the Arduino course by IIT Bombay. \\n• \\nCertified for completing the Java course by Spoken Tutorials(IIT B). \\n• \\nCertified with Grade C in the Cambridge Business English Certificate \\nVantage(BEC). \\n \\nCO-CURRICULAR ACTIVITIES: \\n• \\nPresented a circuit model of Solar Tracker and won 1st prize in CRUX-2K16, A \\nHardware context organised by IETE student forum, SRKREC. \\n• \\nPaper Presentations on Li-Fi and on Metallic Hydrogen. \\n• \\nAttended workshops on Embeded Systems, Cyber Attacks, Big data, Mind \\nControlled Robotics. \\n• \\nParticipated in the grand finals of  Smart India Hackathon (SIH-2019). \\nEXTRA-CURRICULAR ACTIVITIES: \\n• \\nStudent volunteer for TRANCE2k-18 organised by department of ECE, SRKR \\nEngineering College. \\n• \\nStudent volunteer for Spardha 2K18 (a cross departmental hackathon) conducted by \\nour college. \\n• \\nStudent Co-ordinator for the workshop conducted by Robokart in TRANCE 2K19.  \\nPROJECTS: \\n• \\nOn Arduino: Solar Tracker, Obstacle avoiding robot, edge sensing robot and CNC. \\n• \\nCircuits: Water level indicator, Controlling home appliances using RF transmitter      \\nand receiver. \\n• \\nSoftware: Chatbots for assisting passengers in railway stations and bus stands.(SIH) \\n• \\nAI & Ml:  Face detection using OpenCV, Object detection using SSD, Classification \\nmodel using Transfer learning and deploying it on to the web. \\n \\nHOBBIES: \\n• \\nPlaying Cricket, Chess. \\n• \\nMeditation. \\nPERSONAL PROFILE: \\n• \\nNationality:                     Indian \\n• \\nMarital status:                 Single \\n• \\nFather Name: \\n           B V Seshagiri Rao \\n• \\nMother Name: \\n           B Swathi \\n• \\nLanguages Known:         English, Telugu \\n \\nPERSONALITY TRAITS: \\n• \\nI am a self-motivated person. \\n• \\nBeing confident and determined. \\n• \\nAbility to cope up with different situations. \\n \\nPlace: Hyderabad                             \\nDate:   04-02-2021                                                                                     B.Sai Harshith                \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n',\n",
       " '                                                                                                     EDUCATION \\n                                                                                                     INTERNSHIP \\n                                                                                                          PROJECTS \\n                                                                                      RELEVANT COURSEWORK  \\n                                                                                                \\n        SKILLS  \\n                                                                      POSITIONS OF RESPONSIBILITY AND AWARDS \\n                                                                                              WORK EXPERIENCE \\n                                                                                                     COMPETITION \\n     AYUSH PALIWAL  \\n     AEROSPACE ENGINEER (B.Tech)                                                                                                                                                                                    \\n     ayushpaliwal2015@gmail.com|9800315999 \\n \\n       Year                                          Degree/Exam                                      Institute                                                                        CGPA/Marks \\n       2019                                         B.TECH                                                  IIT Kharagpur                                                                     7.21/10 \\n       2014                                         Senior Secondary-CBSE                      Tuli Public School, Nagpur                                               86.2% \\n       2012                                         High School-CBSE                                Center Point School, Nagpur                                           9.0/10 \\n \\n \\n       Reliance Jio       \\n \\n \\n \\n \\n         Data Scientist       \\n \\n \\n                    July 2019-Present \\n       • Working on development and maintenance of machine learning models integrated to an automation pipeline of the project \\n       • Trained multiple classifiers on historical ambient condition data to forecast potential pest and disease (p&d) attack on the crop  \\n       • Studied p&d lifecycle to identify key factors that facilitate or hinder their growth and use them to build features for the model \\n       • Responsible for updating models when new sensors are added in IOT device or new seasonal p&d training data is available  \\n \\nBen Gurion University of the Negev, Israel           Deutsche Telekom Innovation Labs  \\n \\n \\n                    May 2019 \\n• Worked on a novel method of unsupervised optimization of embedding dimension based on the stability of embedding  \\n• Created a custom metric, Node Pair Distance Correlation, to calculate embedding stability at varying dimensions \\n• Implemented the method to successfully obtain optimum dimensions of Android Applications (APK apps) embedding \\n• Received an appreciation for the work and was offered an opportunity to continue the research project as research assistant  \\n \\nDeloitte                                                         Consulting: Information Management and Analytics                                     \\n    May 2018    \\n• Exploratory Data Analysis: Analyzed sales data by plotting sales against customer demographics, product and store details \\n• Statistical Inference : Drew inference by performing parametric and nonparametric tests to review feature importance \\n• Handling Missing Values: Developed a versatile imputing method which improved imputed missing value accuracy by 11.9%    \\n• Machine Learning: Used Random Forest to achieve 1970 rmse on predicted sales value where the benchmark was 5050 \\n \\n• LTFS Data Science FinHack 3                                     \\n \\n \\n \\n \\n \\n \\n                          February 2021 \\n   Accomplished rank 11 out of 34415 registered in the competition; got a score of (0.62), highest (0.83) in F1 scale  \\n• JantaHack: Healthcare Analytics II  \\n \\n \\n \\n \\n \\n \\n \\n \\n                    July 2020 \\n   Accomplished rank 7 out of 19419 registered in the competition; got a score of (0.4364), highest (0.4390) in Accuracy scale \\n• JantaHack: Machine Learning in Agriculture  \\n \\n \\n \\n \\n \\n \\n \\n       September 2020 \\n   Accomplished rank 2 out of 15381 registered in the competition; got a score of (0.9605), highest (0.9610) in Accuracy scale \\n• JantaHack: Machine Learning for Banking  \\n \\n \\n \\n \\n \\n \\n \\n \\n    May 2020 \\n   Accomplished rank 6 out of 8341 registered in the competition; got a score of (0.5365), highest (0.5399) in F1 scale \\n• Gartner HackElite  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n       September 2019 \\n   Accomplished rank 10 out of 660 participants in the competition; got a score of (0.9217), highest (0.9432) in AUC-ROC scale \\n \\n \\nDrivers of Customer Loyalty in Airline        Vinod Gupta School of Management, IIT Kharagpur                                   November 2018 \\n• Worked on airline review data which comprised of inflight facility ratings, comments and if passenger recommended the airline \\n• Created new unbiased inflight facility ratings by calculating sentiment score of the facility related sentence in review \\n• Found major driving traits for each airline, flying class and overall data using linear regression and Random Forest model \\n \\nApplied Machine Learning | Exploratory Data Analysis | Linear Regression and Modeling | Statistical Methods | R Programming \\n \\nProgramming Languages: C, Python and R | OS: Linux (CentOS and Ubuntu) and Windows | Database: SQL, Cassandra, MongoDB \\n \\n \\n• Secretary, Aerospace Engineering Society, IIT Kharagpur \\n \\n \\n \\n \\n \\n \\n               August 2016 \\n• Won Best Fresher Volunteer Award, Radha Krishnan Hall of residence, IIT Kharagpur                                                             April 2017 \\n• Won a cash prize for best project presentation at Reliance Jio  \\n \\n \\n \\n \\n \\n \\nAugust 2019 \\n \\n',\n",
       " \"SUDARSHAN KUMAR | 13CE10048 \\n \\nCIVIL ENGG. (B.Tech 4Y) \\n \\nEDUCATION \\n \\nYear \\nDegree/Exam \\nInstitute \\nCGPA/Marks \\n2017 \\nB.TECH \\nIIT Kharagpur \\n7.77 / 10 \\n2012 \\nAll India Senior School Certificate Examination \\nDAV Public School, Patna \\n93.2% \\n2010 \\nAll India Senior Secondary Examination \\nPatna Central School, Patna \\n9.6 / 10 \\n \\nAWARDS AND ACHIEVEMENTS \\n \\n•Acquired 98.48 percentile score in Joint Entrance Examination Mains,2013 among 12 lakh students that had appeared for the examination \\n•Received Certificate of merit & Medal for exceptional performance in Class XII Boards & Joint Entrance Examination from my school  \\n•Received Rajeshwari Sahu Memorial Scholarship of INR 12,000 and Merit-cum-Means scholarship of INR 1,02,000 from IIT Kharagpur \\n \\nINTERNSHIPS \\n \\nDRG Analytics & Insights Pvt. Ltd., Bangalore \\nAnalytics \\n        May'16-July'16 \\n \\n•Understood Payer landscape and provided a solution for extracting plan level benefit design information useful for pharmaceutical firms \\n •Firm understanding and exposure to associated data for drawing meaningful and actionable insights from claims data using MySQL \\n•Analysed claims data generated at Pharmacies in the US and gained expertise around data nuances specific to my project \\n \\n•Developed an algorithm with discreet business rules which helped extract Patient copay and CoInsurance data for branded drugs which \\nis very valuable for Pharmaceutical firms struggling with drugs access within the current Payer market in the United States \\n•Wrote a modular and reusable algorithm which could be used for extracting copay data for any branded drug in the US market \\n \\nIIM Lucknow \\nMarketing Management \\n      May'15-June'15 \\n \\n• Developed pricing strategy for baseball matches based on willingness-to-pay survey data for maximizing revenues  \\n•Performed profit analysis for new product’s market entry and its effect on the net income of existing products and the company \\n• Mastered various concepts of marketing like segmentation, brand positioning, branding in web etc. through various mini-cases \\n \\nLarsen & Toubro Const. Ltd., Patna \\nIndustrial Training \\n       December'14 \\n \\n•Analyzed various sections of beam and column designs and subsequent load analysis using softwares like ABAQUS and STAAD \\n•Tested and verified the quality of aggregates and concerete mix (mainly M20 and M30) that were passed to the field  \\n•Performed in-depth analysis for optimal results in the Quality Assurance/Quality Control Lab based on Indian Standard Codes \\n \\nPROJECTS \\n \\nB. Tech. Project, Civil Engineering Department, IIT Kharagpur \\n       August'16-present \\n \\n•Prepared a documentation on chemical methods of synthesis and characterization of Magnetic Iron-OxideNano Particles(MNOPs)  \\n•Application of MNOPs on Water and Waste-water treatment based on the experimental outcomes \\n \\nTransportation Lab Project, IIT Kharagpur \\n        November'14 \\n \\n•Designed Depth-First-Search(DFS) Algorithm, Floyd's Algorithm and Prim's Algorithm excel sheet using macros (VBA) \\n•Applied the algorithms for minimum spanning tree and shortest path lengths in Transportation engineering problems \\n  \\nCOMPETITION/CONFERENCE \\n \\nAnalytics Vidhya : The Ultimate Student Hunt \\n       October'16 \\n \\n•Cleaned, processed and treated missing values  and applied various feature engineering after understanding the data   \\n•Developed SVM Model, Random Forest and Gradient Boosting (GBM) to predict number of people visit park on a particular day \\n•Created an ensemble model using Neural Network & GBM to achieve RMSE of 116.1 and securing a rank of 74/1494 on the leaderboard \\nHackerRank : Predict Email Opens \\n         August'16 \\n \\n•Processed training and test data, reduced number of features based on suitable parameters to prepare the data for prediction \\n•Achieved an accuracy of 43.8%(maximum achieved accuracy in the contest - 60% ) in prediction of whether or not every user will  \\n  open an email using Random Forest Algorithm \\n•Also applied Logistic Regression and Naive Bayes Classifier and achieved a ranked of 126 in the Machine Learning CodeSprint \\n \\nKaggle : E-Commerce Analytics \\n          March'16 \\n \\n•Developed an analytics model that would help buyers and sellers predict the sales success of a set of eBay listings for iPads  \\n•Developed Logistic Regression Model for the binary output to predict the sales of the ipads \\n•Random Forest model with Bag-of-Words, among various other models was found to be the optimal model \\n \\nSKILLS AND EXPERTISE \\n \\nProgramming Languages : SQL, Visual Basic for Application, R Programming, C \\nSoftware Packages \\n : MS Office, SolidWorks, AutoCAD, Staad.Pro \\n \\nCOURSEWORK INFORMATION \\n \\nMachine Learning, The Analytics Edge, Probability and Statistics, Statistical Methods in Hydrology, The Data Scientist's Toolbox, \\nRegression Models, Data Analysis and Statistical Inference, Construction Planning and Management \\n \\nEXTRA CURRICULAR ACTIVITIES \\n \\n•Awarded 1st Prize in an overnight event(CRIAR) for constructing tower crane prototype in Megalith-2014(Civil Engg. Fest of IIT Kharagpur) \\n•Awarded 4th Prize and a special prize for best fresher's performance in Case study over E-waste management and disposal in Megalith \\n•Volunteered for 2 years for the betterment of poor children in the nearby villages outside IIT Kharagpur and taught them with utmost \\ncare as a part of National Service Scheme \\n•Ensured overall development of 5 freshmen from the department of Civil Engineering as a student mentor to them \\n \\n\",\n",
       " \"| |  \\nMay-July 2017 \\nCOMPETITIONS \\nLord of the Machines | Text Classification | Data Science Hackathon | Analytics Vidhya \\nMarch 2018 \\nZS Data Science Challenge - 2018 | Sale Forecasting | HackerEarth \\nJuly 2018 \\nPOSITIONS OF RESPONSIBILITY \\nMedia Cell Sub-Head | Spring Fest | Annual Social & Cultural Fest of IIT Kharagpur \\nOct 2015 - Feb 2017 \\nSKILLS AND EXPERTISE \\nHITEC City, Hyderabad, Telangana 500081, India \\nABHIROOP KUMAR \\n+91-9932549291 | abhiroopkumar.iitkgp@gmail.com \\nACADEMIC QUALIFICATIONS \\nYear \\nExamination/Degree \\nInstitution/Board \\nPerformance \\n2019 \\nBachelor of Technology in Mining Engineering \\nIndian Institute of Technology, Kharagpur \\n7.64/10 \\n2014 \\nCentral Board of Secondary Education (CBSE) (Class XII) \\nD.A.V. Public School, B.S.E.B Colony, Patna \\n84.60% \\n2012 \\nCentral Board of Secondary Education (CBSE) (Class X) \\nD.A.V. Public School, B.S.E.B Colony, Patna \\n9.2/10 \\nAWARDS AND ACHIEVEMENTS \\n▪ Published a Research paper in IEEE 16th India Council International Conference 2019 on the topic “Emotion Recognition from EEG Signal” \\n▪ Earned 3 Winner, 4 Top 10, and 3 Top 25 badges for excellent performance in various Data Science hackathons on Analytics Vidhya \\n▪ Secured 3rd position in Analyze This 2018 organized by American Express and 5th Rank in the ZS Data Science Challenge by ZS Associates \\n▪ Bagged 2nd and 5th Position among 34K+ data enthusiasts in Talent Hunt Hackathon by L&T Financial Services for two consecutive years \\n▪ Identified patients who will be on the brink of a significant increase in health care expenditures to help care management programs  \\n▪ Predicted individual‘s medication state and Parkinson's Disease severity using raw sensor(accelerometer and gyroscope) time-series data \\n▪ Identifying Risk and Stratifying members based on their historical clinical data and taking actionable decisions to reduce ER costs \\n▪ Predicted estimated claim process time and check issue date and deployed it on the provider’s dashboard for Realtime prediction \\n▪ Improved ZestMoney credit fraud detection model by 6% by analyzing member behavior and integrating socio-economic features \\n▪ Analyzed the spending pattern of the account holder to discover fraudulent activities▪ \\n \\n▪ Developed an ensemble sale forecasting model employing Holt-Winters, ARIMAX, and TBATS achieving an accuracy of 91.58% \\n▪ Effectuated by the company, the model is presently being used to forecast sale for PSP of over 400 stores in 31 states in the USA \\n▪ Raised Gross Margin of a company by 2.6% by optimizing the car rental pricing model incorporating competitor price information \\n▪ Developed a customer retention model to identify the potential customers who are likely to book a car in future months \\n▪ Applied k-Nearest Neighbors algorithm for efficient selection of 100 stores for running promotional campaigns to maximize revenue \\n▪ Integrated Apriori algorithm of association rule learning incorporating Market Basket Analysis for promotional marketing strategy \\n▪ \\nLeading a project titled ‘Classification of EEG Motor Imagery (MI) Multi-Class Signals used in BCI’ under Prof. Debasis Samanta \\n▪ \\nImproving the classification accuracy by noise reduction of MI signals, optimizing features and, implementing an ensemble classifier \\n \\n▪ Secured 1st position among 1200+ participants in a two-month-long Hackathon predicting the performance of an enrollee on tests \\n▪ Built a 3-level stacked model with 8 different classification algorithms as base-classifier and trained GLM & GBM as meta-classifier \\n▪ Improved model accuracy by incorporating Matrix Factorization via Singular Value Decomposition (SVD) in the stacked model \\n▪ Predicted the click probability of links inside a mailer for email campaigns of 1.68M unique users with an imbalance ratio of 80:1 \\n▪ Optimized model predicting power by applying CatBoost, Light GBM, XGBoost algorithms getting an AUC-ROC score of 0.646 \\n▪ Ranked 5th among 10000+ competitors in Pan-India contest, forecasting sales of 5 different products in 6 countries \\n▪ Applied Ensemble forecasting technique employing multivariate ARIMA model, linear regression, SVR, holt’s winter and TBATS \\n \\n▪ Individually garnered funds worth INR 3.2 Lakhs through corporate deals and alumni contributions \\n▪ Coordinated the publicity drive in Northeast India leading to a 68% YoY increase in participation and 35% increase in media outreach \\n▪ Conducted the nationwide prelims of 4 events (140% participants increase) in Mumbai with Nukkad taking place for the first time \\n▪ Conceptualized and edited the promo for Spring Fest 2017 telecasted on a popular music channel VH1 with total views of 2 million \\n▪ Directed and edited the Official After-movie and the Social initiative of Spring Fest 2017, Masoomiyat which has total views of 50000+ \\nSoftware: Python | RStudio | Power BI | Tableau | Adobe Premiere Pro || Languages: C | C++ | R | Python | SQL \\nRelevant Coursework: Econometric Analysis | Probability and Statistics | Machine Learning | Programming and Data Structure \\n▪ \\nEXTRA-CURRICULAR ACHIEVEMENTS \\nCultural \\n▪ Part of the bronze winning Inter Hall Eastern Vocals team of Patel Hall of Residence in General Championship 2016-17 \\nSports \\n▪ Represented Patel Hall as part of its Athletics team in the 800-meter race for 2 years in General Championship Sports \\nMentorship \\n▪ Mentored 6 freshmen students on campus under the purview of the Dean of Student Affairs, IIT Kharagpur \\n \\nWORK-EXPERIENCE/PROJECTS \\nOptum, United Health Group | Hyderabad | Data Scientist                                                                                                                  July 2019 - Present \\nZestMoney | Bangalore | Data Science Intern                                                                                                                                                   May-July 2019 \\nImpact Analytics | Bangalore | Data Science Intern                                                                                                                                         May-July 2018 \\nDrivezy  | Bangalore |  Business Analyst Intern                                                                                                                                                  Dec -Jan 2018 \\nPeel Works |  Mumbai |   Data Science Intern                                                                                                                                                  May -July 2017 \\nBachelor's Thesis Project |  Guide: Prof. Debasis Samanta, CSE-IIT KGP                                                                                                    July -April 2019 \\nAug 2016 – Feb 2017 \\nCore Organising Team Member | Spring Fest | Annual Social & Cultural Fest of IIT Kharagpur \\nMay-July 2018 \\nThe Data Identity (Winner) | Performance Prediction | Student DataFest 2018 | Analytics Vidhya \\nReceived Pre-Placement Offer (PPO) | Project: Deploying sales forecasting model and promotional analysis on products \\n\",\n",
       " ' \\nSalim Shaikh \\nManager, HDFC Bank Ltd. \\n \\n \\nEDUCATION \\n \\nDegree \\nInstitution \\nCGPA \\nYear of \\nPassing \\nM.Sc \\n(STATISTICS) \\nUniversity Of Mumbai \\n \\n6.83/7 \\n \\n2017 \\nB.Sc (STATISTICS) \\nUniversity Of Mumbai \\n91.6% \\n2015 \\nSKILLS \\n \\nLanguages \\nML/DL Tools \\nBig Data/ETL \\nPython \\nPandas / Dask \\nPySpark \\nSQL \\nOpenCv \\nH2o Sparkling water \\nR \\nSklearn \\nHiveQL \\nScala \\nGPU – CuDF, CuML \\nTalend OS \\nSAS \\nH2o \\nPentaho \\nC \\nNLP, RNN, CNN, \\nNN \\nMapReduce \\nSPSS / Minitab \\nPytorch \\nHadoop \\n \\nEXPERIENCE \\nHDFC Bank Ltd, Manager – Risk Analytics \\nDec,2018 – Present \\n\\uf0b7 \\nDeveloped OCR framework to extract Name, Organization Name, \\nGross/Net Salary from Govt & Non-govt salary slips (response \\ntime=25secs per image)\\uf020\\n\\uf0b7 \\nAutocure collection scorecard for SI & ECS customers using SOTA Deep \\nLearning techniques\\uf020\\n\\uf0b7 \\nPredictive model to identify which are the potential customers who \\nswipe on our POS/PG but don’t have our credit card for sourcing new \\nCredit Cards. Used Neural Networks, XGBoost, etc. and improved the \\nKS.\\uf020\\n\\uf0b7 \\nIncome estimation for the people who qualify for getting new Credit \\nCards through the above predictive model. Point estimation as well as \\ninterval estimation using XGBoost, Multinomial Naïve Baye’s, etc. was \\nused. \\uf020\\n\\uf0b7 \\nMulti-Bureau Application scorecard for Auto Loan using Logistic \\nRegression, Decision Tree\\uf020\\n\\uf0b7 \\nAutomated the creation of entire Bureau Application scorecard and \\ndeployed the scorecards on Azure\\uf020\\n\\uf0b7 \\nSupervising team members to use Machine Learning Techniques in their \\nprojects to improve accuracy.\\uf020\\nVodafone Idea Limited, Asst. Manager – Advanced \\nAnalytics \\nJUL,2017 – Dec,2018 \\n\\uf0b7 \\nPropensity models to manage drop in usage & subscriber churn for \\nprepaid vertical. This required micro-level targeting through separate \\nmodels for each circle and sub-segment. \\uf020\\n\\uf0b7 \\nBuilt a Model Automation Tool in Python to automate the entire model \\nbuilding process and thereby reduced the TAT from months to days\\uf020\\n \\nProfile \\n  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n L\\n \\n \\n \\n  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n  \\n \\n \\n \\n \\n \\n \\nMumbai, India \\n \\nPHONE \\n(+91) 8286588859 \\n \\nEMAIL \\nstatsguysalim@gmail.com \\n \\nLINKEDIN \\nwww.linkedin.com/in/salim-\\nshaikh-a082a7162/  \\n \\nBlogs \\n \\nBanking Case Study - Part 1 \\n \\nS/S \\n \\nHackathons \\n \\nKaggle  \\n \\nZindi Africa \\n \\nAnalytics Vidhya \\n \\nMachine Hack \\n',\n",
       " 'DHRUV DEEP BISHNOI \\n Data Scientist \\n dhruvbishnoi0010@gmail.com         \\n  Gurugram, India         \\n +91-9728427702           \\n  linkedin.com/in/dhruvdeepbishnoi \\n         \\nEDUCATION \\nSKILLS\\n \\nMBA in Information Technology & Marketing \\nIIT Roorkee \\n   June 2015 - May 2017 \\n \\nB.E in Computer Science Engineering  \\nThapar University, Patiala \\n   July 2010 - July 2014 \\n \\nEXPERIENCES \\n \\nData Scientist, Zenon Analytics \\nMay 2019 – Present                     \\n•  Building predictive models to enable the delivery of effective and \\nefficient campaigns which enable the client to target the appropriate \\naudience within an actionable time frame  \\n•  Working closely with the client leadership teams apart from \\nmodel development; by providing commercial and economic \\ninterpretation of model insights which contributes to the \\noverall strategic decision-making process  \\n \\nData Scientist, Absolutdata Research and \\nAnalytics \\nMarch 2018 – May 2019            \\n•  Building unsupervised machine learning algorithm to detect \\nanomalies in Power generation industry \\n•  Developing multi-dimensional customer segmentation basis \\nusage and analyzing the population for personalized actionable \\ninsights using machine learning algorithms \\n \\nConsultant, Mazars UK \\n May 2017 – Feb 2018            \\n•   Collaborated with the offshore team in the development of various \\nML models \\n•  Formulating the objective function, exploring data, model \\nbuilding and communicating insights with the team members \\n \\n \\nACHIEVEMENTS   \\n \\n•   Current Rank 99 on Analytics Vidhya platform \\n•   Won Rising Star Awards, 2018 in Absolutdata Analytics  \\n•   Certifications: Deep Learning Specialization by Andrew NG \\n \\nINTERESTS \\n \\n•   Participation in Machine Leaning competitions and Hackathons \\n•   Playing basketball competitively \\n•   Watching movies \\n \\n \\n \\n•  Python, Hive, SQL, PySpark \\n•  Natural Language Processing (NLP) \\n•  Deep Learning – TensorFlow, PyTorch \\n•  Clustering/ Segmentation, Classification, \\nRegression, Anomaly Detection \\n \\n \\n \\nPROJECTS \\n \\nNRR Model \\n• Net Response Rate model, for one of the world’s \\nlargest card company, categorized the prospect \\npopulation based on the maximum likelihood of \\nconversion which optimized the sales strategy \\nleading to 47% increase in conversion rates  \\n• Insights from the model further enabled the \\nmarketing team to develop focused campaigns by \\ntargeting categories with higher response rates \\nwhich ultimately lead lower marketing spends \\n• Model validation was carried out through Gini \\n \\nRisk Model \\n• Built a risk model for a reputed American \\nmultinational financial service company; which \\npredicted the probability of default for potential and \\nexisting prospects based on firmographic data, risk \\nattributes and neighborhood intelligence \\n• Risk scores were generated for each prospect which \\nenabled the client to segment potential prospects \\nand adjust their product features, prices charged \\nbased on the assigned risk level \\nIndustry Classification Model \\n• Model assisted in classifying target companies \\nwhich was a critical input for the sales & marketing \\nteam to develop tailored strategies for each industry \\n• Model output helped the Risk, NRR model as \\nIndustry was one of the key variables in these \\nmodels \\n• Model validation was carried out through accuracy \\nand coverage \\n \\nSegmentation \\n• Behavioral Segmentation for 14 million prepaid \\nsubscribers using K-means clustering for one of the \\nlargest telecom players in Ghana  \\n• Model provided an entire map of the customer base      \\nof the client which enabled the client to design \\ncustomized campaigns aimed at retaining the \\npreferred customer segments \\n• Profiling of segments and silhouette score was used \\nto validate the model \\n \\n \\n',\n",
       " \" \\n \\nKanishka Kayathwal                                                                                                          \\nMale \\nPhone: +91 9833570896 \\nEmail: kanishka24sept@gmail.com \\n                                                                                                                                                                                \\nACADEMIC QUALIFICATIONS \\nYear \\nDegree/Board \\nUniversity/Institution \\n%/CGPA \\n2020 \\nPost Graduate Diploma in Business Analytics \\nIIM Calcutta, ISI Kolkata, IIT Kharagpur \\n9.06/10 \\n2014 \\nB. Tech (Chemical Engineering) \\nIIT Bombay \\n7.10/10 \\nAWARDS & ACHIEVEMENTS \\nData Science \\nCompetitions \\n▪ Won a sponsored trip to SAS US global forum for securing 1st rank in BADM Championship        ’18 \\n▪ Won Silver medal (top 5% out of 1832 worldwide) in Kaggle's Instant Gratification challenge        ‘19        \\n▪ Ranked 1st in Stat Wars by ISI Kolkata, Time Series Data Science Hackathon at Integration              ‘19 \\n▪ Ranked 3rd in Data Hackathon conducted by HSBC for detecting change in customer behavior      ‘19 \\n▪ Secured rank in top 5 in D'conStruct - PwC DIAC's annual simulated business case competition     ‘18 \\n▪ Finished in top 21 out of 10,200+ participants in Brainwaves hackathon by Societe Generale              ‘19 \\n▪ Ranked in top 1% in Intel Scene Classification (17th) & AV Computer Vision Hackathon (23rd)       ‘19 \\nAcademic Awards ▪ Cleared CFA Level 1; Secured AIR 798 (top 0.2%) in IIT-JEE ‘10; Conferred NTSE Scholarship (2007) \\n▪ Won Ethics in AI award by Facebook Research for work in Targeted Bias in Indian Media Outlets                    \\nWORK EXPERIENCE (58 months)                                                                                                                                         \\nData Scientist                                                                      Mastercard AI Garage                                       Gurgaon (Apr’20 – Present) \\nTransaction Fraud \\nDetection \\n▪ Evaluated efficacy of card & merchant embeddings learned via graph algorithms on fraud detection  \\n▪ Applied Word2Vec based entity representations in transaction fraud classification model to increase  \\ndetection rate by 27% & reduce FPR by 30% for transactions with high fraud score \\n        Merchant \\nOpen/Closed Status \\n▪ Predicted open/closed status of merchant in real time using transactional data for Newark city \\n▪ Formulated rules based on segments obtained using clustering on merchant’s transaction patterns \\n▪ Scaling the solution in Spark and integrating it with Deployment Platform for complete US market \\n       Enhancing \\n      ThreatScan \\n▪ Enhancing ThreatScan: Product to identify vulnerabilities & gaps in bank’s authorization network  \\n▪ Built approve/decline transaction model for one of the US bank’s with AUC-PR score of 0.96 \\n▪ Trained GAN model to achieve similarity score of 0.80 between synthetic & real fraud transactions \\n▪ Evaluated bank’s model on generated transactions to detect patterns in approved fraud transactions  \\n         Charity \\nRecommendation \\n▪ Recommended relevant charities for given news articles using semantic textual similarity solution  \\n▪ Built a 2 step-model: first to filter articles with no charity theme by fine-tuning pretrained XLNet \\nmodel with an accuracy of 71% & then tagged remaining articles to four charities based on similarity \\nbetween them & charity description using pretrained transformers with hamming loss of 0.3 \\n          Patents \\n▪ Four patents approved by the Mastercard AI committee: currently in filing stage in the US \\nData Scientist                                                                      Play Games 24x7                                                Mumbai (Nov’17 – May’18) \\nCash  \\nDeposit Propensity \\nPrediction \\n▪ Predicted probability of player’s first cash deposit in 14 days from registration via Logistic Regression \\n▪ Handled class imbalance with SMOTE ENN technique & performed feature selection using Boruta \\n▪ Increased user conversion by 5% & annual revenue by INR 15 million through model deployment \\nUser \\nRevenue Segment \\nClassification \\n▪ Identified high revenue players at start of the journey based on their game behavior & cash deposit \\n▪ Created 150+ features, performed likelihood encoding on high-cardinality categorical attributes \\n▪ Applied XGBoost post PCA to attain AUC score of 0.80; interpreted model predictions using LIME \\nConsultant                                                                           Fractal Analytics                                                   Mumbai (Apr’16 – Oct’17) \\n                                         Handled Pricing and Promotional Analytics for Fortune 500 Company in FMCG Domain \\nCompetitive Price \\nBenchmarking \\n▪ Designed analytical tool to determine pricing strategy for a specific product against competition \\n▪ Segregated products and competitors using K-means; recommended price index to maximize share \\n▪ Enhanced delivery time efficiency by 57%, saving 60 man-hours per month through automation \\nFeature Promotion \\nImpact Analysis \\n▪ Tested retailer’s promo strategies with focus on consumer’s stocking up/filling in first & last week \\n▪ Built Classification trees to understand the impact of above featuring strategy on retailer’s sales \\nSenior Analyst                                         Received an early promotion with highest possible rating                 Mumbai (Jun’14 – Nov’15) \\nMarket Mix \\nModelling \\n▪ Developed an algorithm in R to compute Baseline Sales; estimated ROI over Trade Promotions \\n▪ Performed Regression analysis (R-squared 82%) to identify the factors contributing to volume sales \\n▪ Recommended optimal mix of promotions for PPG’s to enhance total category/brand weekly sales \\nProfessional \\nAchievements \\n▪ Consistently received a certificate of excellence from the client for on-time execution and delivery \\n▪ Awarded ‘Team of the Quarter’ for showcasing innovation in building pricing & promotion tool \\nDATA SCIENCE INTERNSHIP \\n                                                                                                Mastercard AI Garage                                       Gurgaon (Oct’19 – Mar’20) \\n \\n  Revenue \\nForecasting \\n▪ Built scalable revenue forecasting engine for Mastercard products at monthly level for finance team \\n▪ Implemented DL time series forecasting models for major drivers to achieve MAPE under 5% \\n▪ Ensembled various model forecasts using geometric & regression-based methods; adopted \\nreconciliation approach to obtain set of coherent forecasts \\nACADEMIC PROJECTS \\n \\n \\nKanishka Kayathwal                                                                                                          \\nMale \\nPhone: +91 9833570896 \\nEmail: kanishka24sept@gmail.com \\n                                                                                                                                                                                \\nCustomer Loyalty \\nScore Prediction \\n▪ Predicted loyalty score to understand customer preferences using 29M transactions of 200K users         \\n▪ Engineered temporal & aggregate features; performed feature selection using Null Importances \\n▪ Ensembled stacked boosting regressors & LightGBM classifier in Python to achieve RMSE of 3.61 \\nBias Detection in \\nIndian News \\n▪ Scraped last ten-year news from media outlets & filtered political articles using Topic Modeling \\n▪ Built linguistic models using NPOV corpus to capture language bias & sentiment via SentiWordNet \\n▪ Quantified Hyperpartisan Bias using 1D CNN architecture & ELMo embedding to obtain F1 score of 0.61 \\nADDITIONAL PROJECTS \\nMulti-Class Image  \\nClassification \\n▪ Leveraged transfer learning from Places365 dataset to classify ~25k scenes into six different classes \\n▪ Used SGD with restarts for faster convergence; fine-tuned ResNet-50 with differential learning rate \\n▪ Implemented Mixup & Cutout with Test Time Augmentation in PyTorch to get an accuracy of 95% \\nRecommender \\nSystem in Retail \\n▪ Built a Recommender System to predict most relevant items for 600+ users using past transactions \\n▪ Used a hybrid model of LSTM & Collaborative Filtering to achieve best mean F1-Beta score of 0.19   \\nPOSITIONS OF RESPONSIBILITY \\nManager \\nTechfest’13, IIT-B \\n▪ Led team of 150 students to showcase 80 exhibits from 12 countries at zero cost (30% increase y-o-y) \\n▪ Pioneered TechConnect to ensure diffusion of research in IITB (Budget: INR 2 million;150 projects)  \\n\",\n",
       " 'Army Institute of Technology, \\nPune \\nB.E Computer Engineering \\n   G M O T H Y \\n        7780272224  \\ngobillamothy85@gmail.com \\nhttps://github.com/G-Slient \\nhttps://www.kaggle.com/marison \\n \\nCAREER OBJECTIVE                                                                                                                                                                 _  \\nRecently Graduated (2020) Computer Engineering student at Army Institute of Technology seeking a position \\nin Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the goal of \\ndiscovering significant information, suggesting conclusions and support decision making. Eventually to \\nbecome a successful Data Scientist. \\nEDUCATION                                                                                                                                                                                     \\n                                                                                                                                                                                            \\nYear \\nDegree \\nInstitute \\n% / CGPA \\n2016-2020 \\nB.E in Computer \\nEngineering \\nArmy Institute of \\nTechnology, Pune \\n9.25 \\n2015-2016 \\n12th Grade \\nSt Patrick’s Junior College  \\n97% \\n2013-2014 \\n10th Grade \\nKendriya Vidyalaya No. 1 \\nUppal  \\n9.8 CGPA \\n \\nEXPERIENCE                                                                                                                                                           \\n1. Quantiphi                                                                                                    Mumbai (July-20-Present) \\nMachine Learning Engineer (Full time) \\n• \\nComputer Vision based Project involving Object detection, Image Classification. \\n• \\nImplementing Machine Learning Algorithms on Aws Cloud and deploying using Aws Sagemaker. \\n2. TIAA Global Services India                                                                                Pune (June 19-Aug 19) \\nIntern                                        \\n• \\nAsset Management System for Internal Management of resources. \\n• \\nMEAN Stack Application with various features for handling resources. \\n3. Big DataMatica                                                                                             Hyderabad (Dec 18-Jan 19) \\nData Science Intern \\n• \\nPlant Disease detection using convolutional neural networks. \\n• \\nBuilding chatbot with movie dialogues dataset. \\n• \\nDeploying Machine Learning Models on Cloud (Heroku, Google Cloud, Aws). \\n4. Rise Lab IIT Madras                                                                                        Chennai (May 18-July 18) \\nData Science Intern \\n• \\nExperimenting various Machine Learning Models on Bigdata. \\n• \\nExploratory Data Analysis on various Datasets & Interactive Dashboards using Metabase. \\n5. Accops                                                                                                                    Pune (Jan 18-Feb 18) \\nRemote Internship \\n• \\nIOT project using Temperature, Humidity Sensor, Node-MCU and Thingspeak. \\n \\n \\nTECHNICAL SKILLS                                                                                                                                                                         \\n• \\nProgramming Languages/Scripting Languages: Python, HTML, CSS \\n• \\nExperience in Pandas, NumPy, Scikit-learn, Keras, Linear models, Tree based algorithms, Clustering \\nalgorithms, Convolutional neural networks, Recurrent neural networks, Rasa Framework, Exploratory \\nData Analysis, Web-frame using Flask, MongoDB, MySQL, PostgreSQL, Angular, Node.js, TensorFlow. \\n• \\nCloud Platforms: Experience in AWS, Heroku, GCP, Microsoft Azure, Alibaba. \\n• \\nSoftware Packages: Jupyter Notebook, R-Studio, Git, GitHub, Photoshop. \\n• \\nPlatform: Linux, Windows. \\n \\n \\n \\nHONORS AND ACHIEVEMENTS                                                                                                                                                     \\n  \\n▪ \\nInter/Intra College Technical Fests: \\n1. First Runners up in Paper Presentation in PCCOE, Spectrum’17. \\n2. Second Runners up in Best Design for Smart City in COEP, Mindspark’17. \\n3. First Position in Idea Presentation and Technical Quiz in ZION’18. \\n4. First Position in PredictX (Machine Learning Kaggle Competition) in CEOP, Minspark’18. \\n5. First Position in AIT Ugcon Amalgam 2018,2019 Business Plan Competition. \\n6. First Position in Machine Hack (Machine Learning Competition) Metal Grade Prediction \\nChallenge and Video Games Sales Prediction Challenge. \\n7. Third in Machine Hack (Machine Learning Competition) Glass Quality Prediction Hackathon. \\n• \\nNational Level Hackathons: \\n1. Second Runners up in Nec Open Innovation Hackathon held at Delhi (2019). \\n2. First Position in Angular Hackathon conducted by Techgig (2019). \\n3. First Position in Infineon AI/ML Hackathon held at Bangalore (2019). \\n4. Second Runners up in Alibaba Cloud India Hackathon by Alibaba (2020). \\n5. First Runners up in Voice Based Payments Hackathon by NPCI (2020). \\n6. First Position in Alibaba Shapeup the Ecommerce with Technology (2020). \\n7. First Runners up in Store Transaction Imputation by Nielsen (2020). \\n8. First Runners up in Automated Multi-Label Text Classification by Times Internet (2020). \\n• \\nEconomic Times Campus Stars Class 2018-19: \\n▪ \\nSelected to the final list of India’s Largest and most coveted list of India’s brightest engineers.ET \\nCampus Stars is an initiative for recognizing and rewarding India’s brightest engineering \\nstudents. 91 students were awarded among a pool of 35,000+ applicants from over 2,000+ \\nengineering colleges in India. \\n• \\nBest Technical Performer (BE), 2019-20 at Army Institute of Technology, Pune. \\n   CERTIFICATIONS                                                                                                                                                                      \\n• \\nAWS Certified Solutions Architect – Associate \\n• \\nGoogle Cloud Certified Associate Cloud Engineer \\n \\nPERSONAL SUMMARY                                                                                                                                                                 \\n  \\nI am self-motivated person who believes in achieving the target with dedication and constant work. I will not \\nleave any work mid-way at any cost and will give the smallest of the tasks my best shot. I don’t leave any \\nstone unturned to grab opportunities that boost personality and make me a better individual. \\nLEADERSHIP                                                                                                                                                                                   \\n  \\n• FE and SE Technical Board member. \\n• Part of organizing team in Technical Aakriti’17, Solutions’17 and Solutions’18.   \\nINTERESTS                                                                                                                                                                                      \\n   \\n• My hobbies include playing Hockey, Football and running to refresh myself.  \\n• Interests include trying different types of dishes. \\n \\nREFERENCE                                                                                                                                                                                    \\n \\n• Dr. S R Dhore (HOD Computer Engineering Department), hodcomp@aitpune.edu.in \\n',\n",
       " 'Huzefa Lohawala\\nData Scientist, Bengaluru (IN)\\n� huzefa.lohawala.che14@iitbhu.ac.in\\n� LinkedIn\\n� Github\\n� +91-8602242352\\nWORK EXPERIENCE\\nAssociate Technical Lead\\nSep 2019 – Present\\nZycus Infotech, Bengaluru\\n◦ Currently implementing a quote validation engine using NLP techniques to extract details pertaining to the supplier and\\nthe items from quotes, and match them with their respective purchase requisition.\\n◦ Implemented search-based supplier recommendation engine using Elasticsearch and K-Nearest Neighbors Algorithm.\\n◦ Developed a supplier risk analysis engine to ﬂag suppliers facing bankruptcy, low credit ratings and suppliers who are\\nlaying oﬀ or furloughing their employees during the current pandemic.\\n◦ Implemented a 10-way email classiﬁcation algorithm using classical NLP techniques such as Word Sense Disambiguation\\nusing WordNet and Feature Extraction using Chi-Square Statistic.\\nTeaching Consultant - Data Science\\nAug 2020 – Present\\nupGrad, Bengaluru\\n◦ This a contractual position wherein I’m teaching students from upGrad’s B.Tech. and P.G. programs, fundamental\\nconcepts of Data Science.\\n◦ The curriculum includes training in SQL, Tableau, Python, it’s libraries essential for data science (such as Pandas, NumPy,\\nMatplotlib and seaborn), EDA, statistics and predictive analysis using scikit-learn.\\nProject Engineer\\nJun 2018 – Aug 2019\\nWipro Ltd, Bengaluru\\n◦ Implemented NLP techniques to extract key information from contracts, such as project start date, project end date, project\\ntype, termination clauses, warranty clauses etc.\\n◦ Used boosting to predict term days for collection using the invoice data. The end goal was to forecast days sales outstanding\\n(DSO) using the output from this model.\\n◦ Automated the process of computing loan advances for active and withdrawn employees. The automation reduced the\\neﬀorts of the Controllership team by 50 man-days.\\nEDUCATION\\nIndian Institute of Technology (BHU), Varanasi, India\\nJun 2014 – May 2018\\nB.Tech. in Chemical Engineering and Technology\\nDATASCIENCE HACKATHONS\\nInnoplexus Online Hiring Hackathon: Saving lives with AI (Analytics Vidhya)\\nMar 2019\\n◦ Created a CRF model to extract entities which were an indication towards a probable disease.\\n◦ Final leaderboard position was 1 out of 1998 participants.\\nLMG Analytics Data Science Hiring Challenge (Hackerearth)\\nNov 2018 – Dec 2018\\n◦ Used Gradient Boosting to predict whether or not an existing customer of a retail store will shop at it’s newly launched\\nstores.\\n◦ Final leaderboard position was 21 out of 4400 participants.\\nEricsson Foresight ML Hiring Challenge’19 (Hackerearth)\\nMay 2019\\n◦ Used Bidirectional LSTM to predict the material type of to-be-published research.\\n◦ Used NLP techniques to predict the overall rating for the review provided by a reviewer on a job portal.\\n◦ Final leaderboard position was 11 out of 2586 participants.\\nPractice Problem: Time Series (Analytics Vidhya)\\nOngoing\\n◦ Created a Time Series model using FbProphet to forecast customer footfall for a transportation ﬁrm.\\n◦ Current leaderboard position is 91 out of 12689 participants..\\nCOURSES & SPECIALIZATIONS\\nMachine Learning (Coursera)\\nIssued: Feb 2018\\n◦ Achieved a score of 96.9% for the course.\\nDeep Learning Specialization (Coursera)\\nIssued: May 2019\\n◦ Achieved an average score of 93.1% for the specialization.\\nTECHNICAL COMPETENCIES\\n◦ Tools: TensorFlow, Keras, PySpark, Elasticsearch, Docker, Tableau\\n◦ Interests: NLP, Time Series Analysis, Deep Learning, Statistics, Data Visualization\\n◦ Languages: Python, JAVA, MySQL, Bash\\n',\n",
       " \"Jaswinder Singh\\nLinkedin | jassican300@gmail.com | +[91] 7054122875\\nEDUCATION\\nIIT KANPUR\\nB.Tech in Chemical Engineering\\n2019 | CPI: 7.8 / 10\\nCOURSEWORK\\nProbability & Statistics\\nMachine Learning\\nComputational Methods in Engineering\\nIntroduction to Computing\\nNeural Network & Deep Learning\\nStructuring ML Projects\\nConvolutional Neural Network\\nSequence Models\\nPLATFORM STANDING\\nKaggle Notebooks, Highest Rank- 544\\nAnalytics Vidhya, Global Rank -170\\nMachineHack, Global Rank- 48\\nTECHNICAL SKILLS\\nSOFTWARES\\nJupyter Notebook | Matlab |\\nAdvance Excel | Tableau | Hadoop\\nLIBRARIES\\nNumpy | Pandas | Matplotlib | Seaborn |\\nSklearn | Keras | Pytorch | Tensorﬂow\\nPROGRAMMING LANGUAGES\\nPython | SQL | SAS |C | C++\\nACHIEVEMENTS\\nACADEMIC\\n99.97%ile in JEE MAINS’15\\nAIR-2382 in JEE ADVANCED’15\\nAIR-1097 in KVPY Scholar’14\\nHACKATHONS\\nRank 20-Demand Forecasting\\nRank 25-Customer Segmentation\\nRank 30-ML in Agriculture\\nRank 34-Healthcare Analytics II\\nRank 38-Topic Modeling I\\nSPORTS AWARDS / MEDALS\\nBest Outgoing Sportsperson’18\\nSports Performer of the Year’17\\nBest Incoming Sportsperson’15\\nInterIIT Medals- 2 Gold, 7 Silver\\nInter College Medals- 11 Gold, 7 Silver\\nWORK EXPERIENCE\\nEXL SERVICES, SPECIALIST DATA SCIENTIST\\nJUN'19-PRESENT\\nMODEL DEVELOPMENT TEAM - SCORECARD MONITORING AND MODIFICATION\\n• Working in a cross geographical team, building, monitoring and modifying models\\n• Monitored behavioral scorecard model for reliable out of time validation score\\n• Modiﬁed mobile lending application scorecard model from Logistic regression in SAS\\nto Random forest and Gradient boosting in python with complete automation.\\n• Performed reject inference with fuzzy augmentation and cutoff analysis.\\n• Results in 3% increase of Accuracy with roughly same predicted bad percentage.\\nMODEL DEVELOPMENT TEAM- PROBABILITY OF DEFAULT FORECASTING\\n• Performed exploratory data analysis and refreshed previous Linear regression models\\n• Analyzed macroeconomic factor along with their business impact in model building.\\n• Refreshed previous linear regression models and build new PD-12 linear regression\\nand Time series models in SAS with signiﬁcantly improved R-square\\nSTRATEGY IMPLEMENTATION AND INNOVATION TEAM\\n• Independently handle monthly audits and reporting of collection strategies across\\nmultiple clients that includes data manipulation using SQL, advanced Excel.\\n• Automated 3 Excel based strategies in Python for efﬁcient execution\\n• Initiated quarterly hackathon and manage complete organisation process.\\nPROJECTS\\nPRICE PREDICTION CHALLENGE , REGRESSION PROBLEM, MACHINE HACK\\n• Predicted house prices in India using address and 11 other factors to get least RMSLE\\n• Analyzed given data and extracted useful features such as city and locality name\\n• Generated features by Grouped frequency encoded and Mean encoded variables\\n• Achieved Rank 9, by using the ensembled model of XGBoost, LGBoost, and CatBoost\\nTOPIC MODELLING FOR RESEARCH ARTICLES\\nMULTILABEL CLASSIFICATION & NATURAL LANGUAGE PROCESSING, ANALYTICS VIDHYA\\n• Classiﬁed research articles of 4 different topics to 25 tags using text data\\n• Analyzed most occurring words with word cloud and countvectorizer\\n• Used models such as Logistic Regression, SVC, LSTM with onevsrest classiﬁer\\n• Achieved Rank 14, by tuning Threshold for max F1 Score with ensembling of models\\nCROSS-SELL PREDICTION , BINARY CLASSIFICATION, ANALYTICS VIDYA\\n• Predicted the interest of healthcare policyholders in Vehicle Insurance policy\\n• Generated multiple features and performed feature selection using LOFO\\n• Model used- Neural Network, Random Forest, & Advanced Grading Boosting\\n• Achieved Rank 24, using stratiﬁed K-fold by maximizing the AUC-ROC score\\nPOSITIONS OF RESPONSIBILITY\\nCAPTAIN, ATHLETICS TEAM, IIT KANPUR\\nJAN'18-DEC'18\\n• Spearheaded daily practice sessions of 50 player with coordination of 3 coaches\\n• Organized Student Athlete Program sessions for all round development of team\\n• Led team to Gold in Udghosh’18 (IITK), and Bronze Medal in InterIIT’18 (IITG)\\n1\\n\",\n",
       " \"                                                                                                     \\n1 \\n \\n____________________________________________________________________________________ \\n \\n                                                                    JYOTI SHARMA \\nMOBILE: 425-829-8544 \\n                                EMAIL:  erjyoti@gmail.com   \\n                                LINKEDIN: https://www.linkedin.com/in/jyotisharma1978/      \\n____________________________________________________________________________________ \\n \\nSUMMARY \\n\\uf0b7 \\nAround 16+ years of extensive IT experience in Database, Business Intelligence Technologies, software \\ndevelopment developing Web, Windows client-server architecture. \\n\\uf0b7 \\nProficient at building & maintaining analytics infrastructure, maintaining scalable data pipelines, and \\ndeveloping & testing architecture for data generation. Highly skilled in creating data set processes and \\naccelerating the performance of ETL processes. \\n\\uf0b7 \\nProficient in data mining & data visualization to deliver compelling business value to clients & successfully \\nexecute projects. \\n\\uf0b7 \\nAdept at performing deep dive for gaining actionable insights to benefit key stakeholders & facilitate sound \\ndecision-making.  \\n\\uf0b7 \\nProficient in developing complex machine learning and statistical modeling algorithms/techniques for \\nidentifying patterns and extracting valuable insights for key stakeholders and leadership. \\n\\uf0b7 \\nWorking knowledge in Agile/Scrum Environments. \\n\\uf0b7 \\nGreat management and People skills. \\n\\uf0b7 \\nPursuing Post Graduate Diploma in Management. \\n\\uf0b7 \\nPursuing Master in Machine Learning & AI. \\n \\nKEY SKILLS: \\nBusiness Intelligence | Data Analytics | Data Visualization | Inferential Statistics | Hypothesis Testing & A/B Testing | \\nData Modelling | Trend Analysis | Quantitative Analysis | Process Optimization & Development | Predictive Modelling | \\nSentiment Analysis | ML Algorithms | Model Development | Exceptional Interpersonal communication | Efficient multi-\\nTasker | Customer service-oriented | Deadline- oriented | Project Management  \\n \\nEDUCATION: \\n\\uf0b7 \\nBachelor of Tech. and Engineering (Computer Science)-Kurukshetra University \\n\\uf0b7 \\nPost Graduate Diploma, Machine Learning & AI–International Inst. Of Information Technology, IIITB  \\n\\uf0b7 \\nPursuing Masters in Machine Learning and AI (2019-2021)- Liverpool John Moore’s University \\n\\uf0b7 \\nCertified Scrum Master (Scrum Alliance) \\n\\uf0b7 \\nPursing Post Graduate Diploma, Management – IMT \\n \\nKEY ML/AI PYTHON PROJECTS: \\n\\uf0b7 \\nTELECOM CHURN: Help a telecom company to Predict telecom customers likely to churn with more than 90% \\naccuracy by analyzing 7000+ customers data, identifying the best models out of KNN, Naïve Bayes, \\nlogistic, Decision Tree, Random Forest, and SVM Model. Used exploratory data analytics to identify the \\nfeatures that convey whether the customer will churn.  \\n\\uf0b7 \\nCAR PRICING: The predicted price of the car with more than 80% accuracy on a small dataset using multiple \\nlinear regression. Also provided the feature importance by analyzing the features which impact the car \\nprices. \\n\\uf0b7 \\nRASA CHATBOT: Implemented the chat Bot using the rasa framework to provide the best restaurants within the \\nmentioned budgets, in an area for an asked cuisine.    \\n\\uf0b7 \\nGESTURE RECOGNITION: Gesture recognition using Deep Learning techniques for smart television. Tried 3D \\nCNN, LSTM, and GRU \\n\\uf0b7 \\nCREDIT APPROVAL MODEL: Predicted the likelihood of approval of credit card customer applications with \\naccuracy by building and evaluating them using Logistic Regression and Decision Tree models \\n \\nPROJECT DETAILS: \\nCompany: Teksystems Inc. USA                                                                                                 Jun 2018- Till Date \\nPROJECT – Office 365 CXP FastTrack \\nClient - Microsoft \\n                                                                                                     \\n2 \\n \\nDescription:  Office 365 is a set of cloud services available on a subscription basis from Microsoft. FastTrack Team \\nhelps the tenant onboarding and user adoption resources and guidance. \\nResponsibilities: Working with cross-functional teams, collaborate with external partners/stakeholders to drive core \\ninsights, trend analysis assisting with data collection, reporting, and A/B test development, and help drive customer \\nsatisfaction.  \\n\\uf0b7 \\nOversee the design and delivery of reports and insights that analyze business functions and key \\noperations and performance metrics. \\n\\uf0b7 \\nResponsible for translating business requirements into specifications to implement the required reports \\nand dashboards, from multiple data sources. \\n\\uf0b7 \\nProviding technical assistance and cross-training to other team members.  \\n\\uf0b7 \\nAdvanced data modeling and Statistical Analysis performing hypothesis Testing, A/B testing using T-\\nTest to analyze different designs and scenarios.  \\n\\uf0b7 \\nSubject matter expert in Setup Guide Wizards Usage for all Office related Service data, Automation \\nTags, and BOT Data. \\n\\uf0b7 \\nInvolved in trend analysis, variance analysis, findings, insights, and working with stakeholders to \\narticulate what actions are being taken in each scenario to drive or improve performance. \\n\\uf0b7 \\nCreated power pivot, PowerBI reports for data analysis based on the Tabular Model. Providing the ad \\nhoc reports as per different requirements from stakeholders. \\n\\uf0b7 \\nCreated NLP model on sentiment analysis specific to the feedbacks/ratings given by different users. \\nEnvironment: Visual Studio 2015/Visual Studio 2017, Team Foundation Server, GIT, SQL Azure, SQL Server \\nManagement Studio 2017, SSIS, TSQL, Power Pivot, Tabular Model, DAX, Power BI, Azure Data Explorer (Kusto), \\nPython  \\n \\nCompany: Inspur, USA                                                                                                                  Mar 2018- Jun 2018 \\nPROJECT – MSAnalytics \\nClient - Microsoft \\nResponsibilities: \\n\\uf0b7 \\nCreated POWER BI Visualizations and Dashboards as per the business requirements. \\n\\uf0b7 \\nCreated effective reports using visualizations such as Bar chart, Clustered Column Chart, Waterfall \\nChart, Gauge, Pie Chart, Treemap, etc. in POWER BI \\n\\uf0b7 \\nCreated Roles and implemented Row-level security for user Authentication in POWER BI \\n\\uf0b7 \\nImplemented logic to mask the data as per the GDPR compliance.  \\n\\uf0b7 \\nWorked on Data Analysis Expressions (DAX) for accessing data directly from the tabular SSAS \\ndatabase. \\n\\uf0b7 \\nImplemented the changes in the tabular model including the new tables/views and creating measures in \\nDAX. \\n\\uf0b7 \\nProvided POC to embed the POWER BI Reports in the Web application using C# REST API. \\n\\uf0b7 \\nWritten PowerShell script and scheduled a batch process to get the updated data from Share Point to \\nSQL Server Database.  \\nEnvironment: Visual Studio 2017, SQL Server 2016, SSIS, TSQL, POWER BI, Power Pivot, Tabular Model, DAX, \\nKusto, Azure Data Lake \\n \\nCompany: HCL America Inc., USA                                                                                               Feb 2016- Mar 2018 \\nPROJECT – Office 365 CXP FastTrack \\nRole – Technical Lead           \\nClient - Microsoft \\nDescription:  Office 365 is a set of cloud services available on a subscription basis from Microsoft. FastTrack Team \\nhelps the tenant onboarding and user adoption resources and guidance. \\nResponsibilities: \\n\\uf0b7 \\nInvolved in the creation of facts, dimensions, and star schema representation for the Datawarehouse. \\n\\uf0b7 \\nWriting and updating the stored procedures, Views, SQL scripts for performing analysis of data quality \\nas per business requirement. \\n\\uf0b7 \\nInvolved in daily batch loads (Full & Incremental) into Staging and Data Warehouse, troubleshooting \\nprocess, issues, and errors using complex Queries and stored procedures. \\n\\uf0b7 \\nInvolved in developing recommendation engine which provides different recommendations on Office \\nportal dashboard.  \\n\\uf0b7 \\nInvolved in data analyzing, data validation, and data cleanup. \\n\\uf0b7 \\nCreated power pivot reports for data analysis based on the Tabular Model. Providing ad-hoc reports \\nbased on different stakeholders. Design Percentage, Month over Month, and Year over Year measures \\n                                                                                                     \\n3 \\n \\nafter importing data into Pivot.  \\n\\uf0b7 \\nCreated the Data Pipelines in Azure Data Factory that gets and updates the data for each Tenant from \\nSql OnPrem to Azure SQL Storage Table. Installed On-premise data gateway and scheduled daily data \\nrefresh in Pipeline. \\n\\uf0b7 \\nResponsible for providing queries from SQL Azure database related to wizard usage based on various \\nparameters selected by tenants on wizards for analytical purposes. \\nEnvironment: Visual Studio 2015/Visual Studio 2017, C#, ASP.net, AngularJS, Source Depot, GIT, SQL Azure, SQL \\nServer 2012/2016, SSIS, TSQL, Typescript, JavaScript, CSS, PowerBI, Power Pivot, Tabular Model, DAX \\n \\nCompany: Walmart, Canada                                                                                                  Aug 2011- Nov 2015 \\nPROJECT – CrossCap MMS \\nRole – Technical Lead               \\nDescription: Reporting solution which gets the item files provides by Crosscap and generates the weekly Sales \\nbased on the Date Range provided in the file. Also, to generate weekly item file and create Category File for that \\nweek. \\nResponsibilities: \\n\\uf0b7 \\nInteracted with Business Users, analyzed user requirements, and Created technical design document \\nbased on the Requirement. \\n\\uf0b7 \\nInvolved in Data Modeling to develop the database design and created database diagram in MS Visio \\nand SQL Server Management Studio (SSMS) \\n\\uf0b7 \\nDesigned and implemented a variety of SSRS reports such as Parameterized, Drill Down, Snapshot, \\nCached, Drill Through, Ad-hoc, and Sub-Reports using Report Designer and Report Builder based on \\nthe business requirements using both Tabular and Matrix report formats. \\n\\uf0b7 \\nWorked on several types of Chart Reports such as Lines, Columns, Bars with both 2D and 2D views to \\ndisplay cumulative totals based on the day-to-day requirement. \\n\\uf0b7 \\nDesigned complex T-SQL queries, User Defined Functions \\n\\uf0b7 \\nStored Procedures and Triggers followed by thorough analysis and testing of those database objects \\nbefore deployment to the production server. \\n\\uf0b7 \\nMaintained Change Control and Release Management process for all database objects like Tables, \\nViews, Procedures, and Triggers.  \\n\\uf0b7 \\nCreated an ETL Process using SSIS that will get the file from sFTP and generate various .xlsx files \\nbased on the items and date range provided in the files for that week. \\n\\uf0b7 \\nThe process includes creating a weekly item file Report and upload the file on sFTP. \\n\\uf0b7 \\nProvided POS Sales for different items in different date ranges. \\n\\uf0b7 \\nArchive the inbound and outbound files in a specific directory using the SSIS package. \\nEnvironment: SQL Server 2012, Enterprise Edition, SQL Server Business Intelligence Development Studio (SSIS, \\nSSRS), Teradata, Team Forge. \\n \\nPROJECT – Yearly Bonus \\nDescription: Web-based Analytics tool used by Compensation Team, Store Managers, and Home Office Associates \\nto view the Bonus eligibility. The compensation Team can view the report and can make necessary changes to \\ncalculate the Bonus amount and generate the file for the payroll team. \\nResponsibilities: \\n\\uf0b7 \\nInteracted with Business Users, analyzed user requirements, and Created technical design document \\nbased on the Requirement. \\n\\uf0b7 \\nCreated the program specifications based on the technical design document. \\n\\uf0b7 \\nDesigned and developed ETL Process to import data from Teradata and HR Data Warehouse and \\ntransform and load data into SQL Server database \\n\\uf0b7 \\nInvolved in complete SSIS life cycle in creating SSIS packages, building, deploying, and executing the \\npackages in both the environments (Development and Production) \\n\\uf0b7 \\nMonitored performance and optimized SQL queries; and created and modified T-SQL stored \\nprocedures.  \\n\\uf0b7 \\nCreated Database and Database Objects like Tables, Stored Procedures, Views, Triggers, Rules, \\nDefaults user-defined data types and functions.  \\n\\uf0b7 \\nCreated UI for the associates to login and view the reports for any employer and generate the Bonus \\nletter. \\n\\uf0b7 \\nCreated web pages for Store Manager/Home Office Associates to login and view based on their access \\nrights. \\nEnvironment: MS SQL Server 2008, SQL Server Business Intelligence Development Studio, PeopleSoft, DSN2, \\nVisual Studio 2010, ASP.NET, C#, Team Forge, HP Quality center, TSQL. \\n \\n                                                                                                     \\n4 \\n \\nPROJECT – People Analytics \\nDescription: This is an analytical web-based tool for the Talent Management Technology Team as a part of \\ncentralizing and automating the Talent Management. \\nResponsibilities: \\n\\uf0b7 \\nInteracted with Business Users, analyzed user requirements, and Created technical design document \\nbased on the Requirement. \\n\\uf0b7 \\nCreated and reviewed the program specifications based on the technical design document. \\n\\uf0b7 \\nETL Process to extract data from PeopleSoft (DB2) and Teradata. \\n\\uf0b7 \\nConducted and automated the ETL operations to Extract data from multiple data sources, transform \\ninconsistent and missing data to consistent and reliable data, and finally load the data. \\n\\uf0b7 \\nCreated Dashboard to show different reports based on different matrix-like Headcount, Termination, \\nPromotion, Demotion, Performance matrix, etc in different regions, markets, and stored. \\n\\uf0b7 \\nDeveloped reports using SSRS with subreports, dynamic sorting, defining data source and subtotals for \\nthe report. \\n\\uf0b7 \\nCreating Stored Procedures to handle complex calculations and wrote logic to maintain the purging and \\nhistory of data. \\n\\uf0b7 \\nCode Review of the offshore team. \\n\\uf0b7 \\nUnit testing for the modules implemented by other team members.  \\nEnvironment: SSIS, MS SQL Server 2008, Teradata, PeopleSoft, Visual Studio 2010, ASP.NET, C#, Team Forge, \\nHP Quality Center, TSQL. \\n \\nPROJECT – Manitoba Holiday Pay \\nDescription: This is a web-based project for Canada Payroll to support them to generate the payroll file for the store \\nassociates who work in Manitoba and are entitled to the Holiday Pay. \\nPayroll associates can view the report based on store, WIN, First and Last Name along with the hours and pay for all \\nthe holidays in Manitoba. \\nResponsibilities: \\n\\uf0b7 \\nCreated Technical design document and program specification based on the requirement. \\n\\uf0b7 \\nDesigned all the modules based on the requirement using ASP.Net, C#, SQL Server 2008. \\n\\uf0b7 \\nImplemented SSIS Package that will get the hours worked for all the associates working in stores in \\nManitoba and perform the calculation based on Manitoba labor regulations. \\n\\uf0b7 \\nOnce the calculation is done for the hours/pay the file will get generated and put on sFTP wherein the \\nPayroll team will download and process the file.  \\n\\uf0b7 \\nCreated reports related to Associates store, First and Last Name, Holiday, Hours/Pay  \\nEnvironment: SSIS, MS SQL Server 2008, DSN2, Teradata, Visual Studio 2010, Team Forge, HP Quality center, \\nTSQL. \\n \\nPROJECT – SumTotal LMS \\nDescription: This is a backend project for Walmart Canada wherein data files will be generated and send to the \\nSumTotal (third party vendor) providing e-learning courses.  \\nResponsibilities: \\n\\uf0b7 \\nResponsible for the determination of the requirements of the module and apply those requirements. \\n\\uf0b7 \\nCreated technical documentation of the project based on the Business Requirement document. \\n\\uf0b7 \\nWritten program specifications for generating the files to upload it on the sFTP server along with the \\nDelta feeds to be sent daily. \\n\\uf0b7 \\nConstructed ETL Process using SSIS packages that get the data from PeopleSoft and manipulates as \\nper SumTotal Requirement and then generate the files and upload it to Sum Total sFTP. Schedule the \\nprocess to run daily. \\n\\uf0b7 \\nDesigned and developed the Triggers, Functions, and Stored procedures. \\nEnvironment: SSIS, MS SQL Server 2008, Team Forge, HP Quality center, TSQL. \\n \\nPROJECT – BIMS \\nDescription: The project is being used by both store's home office associates to create and resolve the issues \\nrelated to stores. Managing tickets based on Department group and sending email notifications was a major \\nfunctionality of this project. One ticket will be created, an email will be sent to the concerned department group \\nregarding the description of the issue. If the ticket has been created and is unassigned for more than 24hrs, an email \\nwill be sent to the category manager and to the associates associated with that group. For all overdue tickets which \\nhave been created more than 48hrs and are still unresolved, an email will be sent to the category manager with \\ndetails of who has been assigned to the ticket. Store associates can search the ticket to find out the status of the \\nticket. \\n                                                                                                     \\n5 \\n \\nResponsibilities: \\n  \\n\\uf0b7 \\nInvolved in Requirements gathering, Conceptual Design, Analysis, and Detail design, Development, and \\nSystem Testing. \\n\\uf0b7 \\nDeveloped core functionality with the .NET Framework. \\n\\uf0b7 \\nImplemented Agile methodology for the development of the application. \\n\\uf0b7 \\nDatabase designing with the implementation of stored procedures.  \\n\\uf0b7 \\nExpertise in creating complex Stored Procedures, DTS packages, triggers, cursors, tables, views, and \\nother SQL joins and statements for applications.  \\n\\uf0b7 \\nCreated ETL process that gets the updated data related to the store, department, category, etc. \\n\\uf0b7 \\nDesigned and developed SSIS Packages (ETL) to import data from Teradata and SQL Server sources \\n(heterogeneous sources) and transform and load data into SQL Server database \\n\\uf0b7 \\nInvolved in complete SSIS life cycle in creating SSIS packages, building, deploying, and executing the \\npackages in both the environments (Development and Production) \\n\\uf0b7 \\nImplemented Event Handlers and Error Handling in SSIS packages \\n\\uf0b7 \\nAdvanced extensible reporting skills using SQL Server Reporting Services (SSRS). \\n\\uf0b7 \\nUsed SSIS for each loop to implement data updates from various sources to the data warehouse. \\n\\uf0b7 \\nScheduled jobs to run daily and along with sending emails to the category manager if any ticket is \\noverdue.  \\n\\uf0b7 \\nUsed Visual Team Foundation Server for version control, source control, and reporting. \\n\\uf0b7 \\nFixing bugs. \\nEnvironment: SQL Server, SQL Server Business Intelligence Development Studio (SSIS, SSRS), .Net framework, \\nMicrosoft Office Share Point Server, XML, MS Visual Source Safe and Windows Server 2008, Visual Studio \\n2008/2010, C#, Entity Framework. \\n \\nCompany: The Marketing Store, Canada                                                                                   Sep 2010- Aug 2011 \\nPROJECT – Nissan CRM \\nRole - Application Developer                   \\nDescription: This is a multilingual web-based project for Nissan North America dealership for Corporate as well as \\ndealer level users. \\nResponsibilities: \\n\\uf0b7 \\nCreated technical documents for the performance Review Report, RO Analysis report, Inner Circle, \\nOuter Circle, Express Service.  \\n\\uf0b7 \\nProvided Production support along with new development based on the change request. \\n\\uf0b7 \\nCreated class diagrams and sequence diagrams using Visio. \\n\\uf0b7 \\nSolely responsible for the implementation of code and designing database based on the requirement \\ndocument.  \\n\\uf0b7 \\nCreated reports related to the customer invoice, Coupon redemption for the dealer and corporate users \\nusing grid view Control. Also written code to export the report to excel sheet. \\n\\uf0b7 \\nWritten stored procedures and queries to get the data related to reports.  \\n\\uf0b7 \\nCreated SQL jobs related to reporting data. \\n\\uf0b7 \\nResponsible for creating test/production build. \\n\\uf0b7 \\nFixing bugs related to all modules. \\nEnvironment: \\nASP.Net 3.5, C#, .Net Framework 3.5, Visual Studio 2008, JavaScript, MS SQL Server 2008, Windows XP \\nProfessional, Team Foundation Server- 2010, T-SQL, Visio, Microsoft Enterprise Library.                                  \\n \\nCompany: Mobiroo Inc., Canada                                                                                               Oct 2009 – Sep 2010                                       \\nPROJECT – Mobiroo \\nRole: Software programmer/Lead \\n                    \\nDescription: Web-based Mobile application that is used for the promotional industry to download the apps from \\nBlackberry. These downloaded apps will have customized branded banner ads and splash pages. The company \\nplaces the brand within paid apps as a banner ad and/or a landing page. These branded apps can then be given \\naway as promotional items to the customers in the form of customizable prepaid branded app cards. Apart from this it \\nalso allows users that do not have a Blackberry to download songs instead of an app. \\nResponsibilities: \\n\\uf0b7 \\nDesigned technical document of the Website along with the Mobile application. \\n\\uf0b7 \\nDesigned program specifications for all the modules like Admin, Distributor, Developer, and Mobile \\nApplication and created UI and business logic layer using Asp.Net and C#.Net. \\n                                                                                                     \\n6 \\n \\n\\uf0b7 \\nWritten code for blackberry and Android as well as unit testing, technical analysis, debugging, and \\nintegration. \\n\\uf0b7 \\nCreated Tracking reports using ASP.Net server controls using C# and JavaScript. Also created \\ndistributor and customer reports using SSRS. \\n\\uf0b7 \\nWritten SQL Queries, Stored Procedures, Views, and User-defined functions in SQL Server 2005 and \\nSQL Server 2008 using T-SQL. \\n\\uf0b7 \\nThe transition of the solution and database from the old server to the new server and from Visual Studio \\n2005 to Visual Studio 2008 version. \\n\\uf0b7 \\nCreated web service for sending dynamic banners to the mobile application. \\n\\uf0b7 \\nPerforming root cause analysis and rectified issues accordingly. \\n\\uf0b7 \\nDeployment on the server for each build. \\nEnvironment: \\nASP.Net 2.0/3.5, C#, .Net Framework 2.0/3.5, Visual Studio 2005/2008, JavaScript, MS SQL Server 2005/2008, \\nWindows XP Professional, TortoiseSVN, T-SQL, SSRS, SSIS. \\n \\nCompany: Summitworks Technology Inc., USA                                                                    Mar 2008 – Aug 2009                                \\nPROJECT – CORE \\nClient: Infosys, USA      \\nRole: Software Lead                                                     \\n                           \\nDescription: CORE is a web-based project being implemented by Infosys together with Health ways Inc \\nResponsibilities: \\n\\uf0b7 \\nDesigned documents of the Core including Sequence and Deployment Diagram.  \\n\\uf0b7 \\nPlayed a major role in data migration for the Activity, Membership, and Enrollment for the Forever Fit, \\nSilver Sneakers, and Prime Products. It includes Terminated Enrollees, Active Enrollees, and True \\nguest. Also contributed to functional design and specification, table mapping, and program coding for \\nthe data extraction. \\n\\uf0b7 \\nMigrated the data related to the swipe system from legacy to the new system. \\n\\uf0b7 \\nRequirement gathering, data cleansing, de-duplication, program coding to extract data from the legacy \\nsystem and provided various audit reports for data verification using SSIS. \\n\\uf0b7 \\nCreated scripts in T-SQL to migrate the data using SSIS. \\n\\uf0b7 \\nPerformance tuning of the application by doing modifications in SQL Server 2005. \\n\\uf0b7 \\nCreated UI layer and written business logic layer using ASP.Net and C#.Net as code behind for Activity \\nModule. Consumed WCF service in the Payment and Contract Module. \\n\\uf0b7 \\nPerformed code reviews, database design, and unit testing. \\n\\uf0b7 \\nCreated Reports using Crystal Report for Payment and Location modules. \\n\\uf0b7 \\nResponsible for the entire archival project for Legacy Core System.  \\n Environment: \\nASP.Net 3.0, C#, .Net Framework 3.0, WCF, Visual Studio 2005/2008, JavaScript, MS SQL Server 2005, Windows \\nXP Professional, Clear Case, SQL Server Integration Services 2005(SSIS), T-SQL, MS-Access, VBA, Visual Source \\nSafe, Crystal Reports. \\n \\nCompany: LogicaCMG Pvt. Ltd                                                                                             Aug, 2004 – Jan, 2008                                \\nPROJECT – i-Net \\nRole: IT Consultant \\nClient: Netherlands Government                                                                               \\nResponsibilities: \\n\\uf0b7 Window-based application for the Minister of affairs department. \\n\\uf0b7 Implementation of Database Design and Writing Stored Procedures using SQL Server 2000. \\n\\uf0b7 Handled AMF module by creating designing documents and implementing use case diagrams using C#. \\n\\uf0b7 Written various classes and used style sheets to maintain the uniformity in look and feel. \\n\\uf0b7 Maintain existing code modules as well as design and implement new Modules to face new challenges. \\n\\uf0b7 Implemented a security module based on rights assigned to the user. \\n\\uf0b7 Designed data assess layer using ADO.Net datasets & data adapters. \\n\\uf0b7 Used FxCop for validating .NET code and nDoc to generate documentation. \\nEnvironment: C#, Windows-based Forms, .Net Framework 2.0/1.1, WinForms, ADO.Net, MS SQL Server 2000, \\nApplication Data Blocks, Windows XP Professional, Infragistics Controls, FxCop, nDoc. \\n \\nPROJECT – FCSC \\nClient: UK Government                                   \\n                \\n \\n \\nRole: IT Consultant \\nResponsibilities: \\n                                                                                                     \\n7 \\n \\n\\uf0b7 \\nLow-level designing for the modules implemented. \\n\\uf0b7 \\nBatch Payment, Batch Printing and Registration Module. \\n\\uf0b7 \\nCreated tables, views, and relations using SQL Enterprise Manager. \\n\\uf0b7 \\nQuerying the database tables to obtain the required results. \\n\\uf0b7 \\nBuild Prototypes for both client and server applications in C#. \\n\\uf0b7 \\nCreated a Windows Service in C# which would act as a request listener. \\n\\uf0b7 \\nDesign XML (using XSD) to be used at the Server / Client side for storing terminal configuration details \\nand application process state. \\nEnvironment: ASP.Net 1.1, C#.net, VB.Net Framework 1.1, ADO.Net, MS SQL Server 2000, Visual Source Safe \\n6.0, IIS, Windows XP Professional       \\n \\nTECHNICAL SKILLS \\n \\nLanguages \\nASP.Net, C#, VB.Net, ADO.Net, Visual Basic 6.0/5, C++, XML, Python \\nDatabases \\nMS-SQL/T-SQL, SQL Azure, MS Access, MySQL, Teradata, ORM: ADO.Net Entity \\nFramework, Tabular, SSAS, SSIS, DAX, Azure Data Factory, Cosmos \\nPackages \\nSciKit-Learn, NumPy, SciPy, Plot.ly, Pandas, NLTK, Beautiful Soup, Matplotlib, Stats \\nModels \\nStatistics/ML/AI \\nLinear/Logistic Regression, SVM, Ensemble Trees, Random Forests, Clustering, Gradient \\nBoosted Trees, Neural Networks, Deep Learning, NLP \\nREPORTS \\nSQL Reporting Services (SSRS), Crystal Reports, Power Pivot, Power BI, Tableau \\nInternet Technologies \\nASP.Net, HTML, XHTML, SOAP, WCF, WPF, VBScript, JavaScript, Active Server Pages, \\nWCF, WPF, PowerShell \\nOperating Systems \\nMS-DOS, Windows- 95/98, Windows NT 4.0, Windows 2000, Windows XP \\nVersion Control \\nClearCase, Tortoise svn, Visual Studio Team Foundation Server \\nDevelopment IDE \\nVisual Studio 6.0, Visual Studio.Net 2017/2015/2008/2005/2003/2002, MS Interdev 6.0, \\nOthers \\n.Net Framework 3.0/2.0/1.1, Visio, ClearCase, ClearQuest, Microsoft Data Application \\nBlocks, SourceSafe, VBA, Web Forms, Win Forms, ODBC, ADO, DAO, ADO.Net, LINQ, \\nMultithreading, Remoting, XML Web Services, ActiveX, COM, IIS, Win32 API, T-SQL, \\nKusto, Infragistics Controls, Telerik Controls, Axure RP Pro \\n \\nCERTIFICATIONS: \\n\\uf0b7 \\nMCAD CERTIFICATION  \\n70-315 Developing and Implementing Web Applications with Microsoft Visual C# .NET and Microsoft Visual \\nStudio .NET \\n70-316 Developing and Implementing Windows-based Applications with Microsoft Visual C# .NET and \\nMicrosoft Visual Studio .NET \\n70-320 Developing XML Web Services and Server Components with Microsoft Visual C# .NET and the \\nMicrosoft .NET Framework. \\n\\uf0b7 \\nCertification in Analyzing and Visualizing Data with Power BI- Powered by Microsoft. \\n\\uf0b7 \\nStatistics Essentials for Data Science – Simplilearn \\n\\uf0b7 \\nData Visualization with Python – IBM Cognitive Class \\n\\uf0b7 \\nPython for Data Science – IBM Cognitive Class \\n\\uf0b7 \\nPython for Data Science – Simplilearn \\n\\uf0b7 \\nTableau Desktop 10 - Simplilearn \\n\\uf0b7 \\nA-Z Machine Learning using Azure Machine Learning (AzureML) – Udemy \\n\\uf0b7 \\nProgramming with Python - Simplilearn \\n\\uf0b7 \\nMaster DAX Fundamentals: Power BI, Power Pivot & SSAS – Udemy \\n\\uf0b7 \\nMachine Learning Advanced Certification Training - Simplilearn \\n\\uf0b7 \\nNeural Networks and Deep Learning – Coursera \\n\\uf0b7 \\nDeep Learning Interface with Azure ML Studio – Coursera \\n\\uf0b7 \\nIntroduction to TensorFlow for Artificial, Machine Learning, and Deep Learning – Coursera \\n\\uf0b7 \\nMachine Learning with Python – Coursera \\n                                                                                                     \\n8 \\n \\n\\uf0b7 \\nCertified Scrum Master  - Scrum Alliance \\n \\n \\n\",\n",
       " 'K Krishna Chaitanya\\nM.Sc.(Integrated) Physics, IIT Kanpur\\nMobile No. +91 8105442793\\nAlt. Mobile No. +91 9491418897\\nEmailid – kkchaitu27@gmail.com\\n                                                                                                                                                                \\nExperience Summary       \\n \\n                                                                                                                     \\n➢ Have 9+ years of experience in IT industry including 7+ years of experience in Data \\nSciences.\\n➢ Worked on developing data science solutions for low-latency systems in Adtech domain for \\nmore than 4.5+years.\\n➢ Exceptional ability to translate abstract business problems to Data Science Solutions.\\n➢ Provided scalable data science solutions to opportunities available in data available with the \\ncompanies.\\n➢ Lead a team of 2 members and worked as a key point of contact for data sciences.\\n➢ Helped companies to improve their revenue by detecting patterns in data available with \\nthem.\\n➢ Worked on projects that ensure brand safety of companies that advertise.\\n➢ Practical knowledge in Python, R, Scala, Java, MySql, Spark, AWS, Docker, Aerospike, \\nHTML, CSS etc.\\n➢ Proven knowledge in Machine Learning, Deep Learning, Artificial Engineering, Data \\nAnalysis, etc.\\n➢ Have Certifications in Algorithms, Data Sciences, Deep Learning and Scalable Machine \\nLearning.\\n                                                                                                                                                                \\nCertifications     \\n \\n                                                                                                                                     \\n\\uf0d8 Deep Learning Nano Degree  - Udacity\\n\\uf0d8 Scalable Machine Learning, University of California, Berkeley – Edx\\n\\uf0d8 Big Data Analytics and Optimization from INSOFE, Hyderabad\\n\\uf0d8 Statistical Learning, Stanford University\\n\\uf0d8 Data Analytics and Statistical Inference, Duke University – Coursera\\n\\uf0d8 Algorithmic Toolbox, University of California San Diego – Coursera\\n\\uf0d8 Data Structures, University of California San Diego – Coursera\\n\\uf0d8 Algorithms on Graphs, University of California San Diego – Coursera\\n\\uf0d8 Natural Language Processing Specialization,  Deeplearnig.AI – Coursera\\n                                                                                                                                                                \\nData Science and Software Skills \\n \\n                                                                                                        \\n\\uf0d8 Python, R, Java, Scala, HTML and CSS\\n\\uf0d8 Big Data - Hadoop, Pig, Hive, and Spark\\n\\uf0d8 Deep Learning, Machine Learning, Statistics, Probability, and Data Analysis\\n                                                                                                                                                                \\nPositions of Responsibility             \\n \\n                                                                                                       \\n\\uf0d8 Team Lead – Airpush India Pvt. Ltd., Xangars Solutions Pvt. Ltd.\\n\\uf0d8 Coordinator – Vivekananda Samiti, Students Gymkhana, IIT Kanpur\\n                                                                                                                                                                \\nAchievements         \\n \\n                                                                                                                                \\n\\uf0d8 Competitions Expert on Kaggle\\n\\uf0d8 KVPY Scholar, Indian Institute of Science, Bangalore\\n\\uf0d8 NIUS Scholar, HBCSE and TIFR, Mumbai\\n\\uf0d8 Undergraduate Associate, Saha Institute of Nuclear Physics, Kolkata\\n                                                                                                                                                                \\nProfessional Experience                                                                                                                       \\nPokkt                                                                                                                   Mar 19 – Present\\n➢ Click Through Rate Prediction – Predicted and ranked hundreds of advertisement \\ncampaigns so that maximum revenue is achieved in the bidding process using Python, and \\nSpark generating thousands of dollars revenue..\\n➢ Conversion Rate Prediction – Built machine learning models for predicting conversion \\nrate of conversion based advertisement campaigns to increase overall revenue.\\n➢ Timeout Determination Model – Architected and implemented adative timeout \\ndetermination model for Demand Side Platform(DSP) for Pokkt.\\n➢ Bid Call Decision Process – Architected and implemented decision process to call third \\nparty Demand Side Platform(DSP).\\nAirpush                                                                                      Oct14 – Apr16 and Feb17– Aug17\\n➢ Gaming Algorithm Applied to CTR Prediction – Used Xbox’s Gaming algorithm \\nTrueSkill Predictor to match players that are close on skill based on their playing history and\\nrank players based on the players they have played with in the context of CTR prediction in \\nadvertising domain.\\n➢ CTR and Conversion Rate Prediction – Worked on different algorithms for predicting \\nclicks and conversions that happen in Mobile advertisement domain.\\n➢ Fraud Detection – Detecting Fraud that happens at request level, impression level, click \\nlevel and app level that happens in Advertising Domain\\n➢ Discerning validity of patterns in data. For eg. Does users who installed sports apps have \\nmore click through rate when sports advertisements are shown to them.\\n➢ Adult Content Detection Model – for preventing adult content to Kids Apps using deep \\nlearning techniques for Airpush to ensure brand safety for advertisers.\\nArimaResearch                                                                                                  Nov17 – Mar19\\n➢ Worked as consultand and helped Arima Research for doing various POCs.\\n➢ Sentiment Classification, Ranking different Resumes with reference to Resume query, \\nClustering various resumes based on content of resumes.\\nPersonagraph                                                                                                     Apr16 – Dec16\\n➢ User profiling – Worked on User profiling extracting User demographics and interests from \\napps installed in one’s mobile.\\nXangars Solutions Pvt. Ltd.                                                                               Apr14 – Oct14\\n➢ Have done Sales Forecasting for revenue of top 50 stores of Aditya Birla’s more stores in \\nBengaluru.\\nPlanetSoft(Now Ebix)                                                                                         Dec11 – Nov13\\n➢ Worked in a core framework team which is the basis for various products that get built in the\\ncompany. It comprises of Google Web Toolkit. Primarily used Java, HTML and CSS.\\nGitHub Profile - https://github.com/kkchaitu27\\nLinkedin Profile - www.linkedin.com/in/kkchaitu27\\nKaggle Profile - https://www.kaggle.com/kkrishnachaitanya\\n',\n",
       " 'KRISHNA PRIYA \\nEmail: krishnapriyakejriwal@gmail.com                                                                               GitHub: https://github.com/krishnapriya-18 \\nMobile No: +917856048599                                                                                 LinkedIn: https://www.linkedin.com/in/krishnapriya18 \\n \\nEDUCATION:                                                                                                                                                             \\n \\nIndian Institute of Technology, Roorkee, India - M.Tech in Geophysical Technology                                  June 2015 – May 2020 \\n• \\nCGPA: 7.76 out of 10 \\n \\nDelhi Public School, Siliguri, India (CBSE)                                                                                                     May 2010 – Apr. 2014 \\n• \\nHigher School Certificate (Class 12 - Science): 81.4 out of 100 \\n• \\nSecondary School Certificate (Class 10): 9 out of 10 \\n \\nWORK EXPERIENCE: \\n \\nData Science Associate | ZS Associates – Bengaluru, India                                                                            Jul. 2020 – Present \\n \\n• \\nNamed Entity Recognition (NLP): Developed a weak supervised named entity recognition framework for carpet industry products. The \\nmodel achieved an F1-Score of 78 percent. Modules: Pandas, Numpy, spaCy, Snorkel \\n \\n• \\nSemantic Textual Similarity (NLP): Developed a framework for predicting similar clinical trials using unsupervised, weak supervised and \\ntransfer learning methods. Deployed the model in a DataBricks production environment. Achieved an F1-Score of 81 percent for weak \\nsupervised (Snorkel) Model and 91.5 percent for supervised transfer learning (Roberta) Model.  \\nModules: spaCy, scispacy, K-Means, Snorkel, Pytorch, Transformers, Pyspark, DataBricks \\n \\n• \\nMulti-Channel Marketing Touchpoint Optimization (Mixed Integer Non-Linear Programming): Optimized the number of times a Health \\nCare Professional (HCP) should be contacted through a marketing channel, to help a pharmaceutical company market their retail and non-\\nretail brands. Modules: Pandas, Numpy, Pyspark, PuLP, DataBricks \\n \\nData Science Intern | Happay – Bengaluru, India                                                                                             May 2019 – July 2019 \\n \\n• \\nCustomer Review Analysis (NLP): Web Scraping + Sentiment analysis of reviews from Google play store, Apple app store and G2 \\nCrowd. Generated insights on Happay’s shortcomings and their competitor’s weakness which led to better marketing pitch of their product.  \\nModules: Beautiful Soup, Selenium, TFIDF, Word2vec, Random Forest \\n \\n• \\nPython Interactive Dashboard (Analytics): Development of Analytics Dashboard for Visualization and Exploration of employee level \\nInsights. Modules: Dash, Plotly, HTML, CSS \\n \\n• \\nMarketing Campaign Click through rate prediction (Predictive Modelling: Classification): Suggested important insights on campaign \\nrunning-time and day, banner positions for display campaigns, etc. Modules: Matplotlib, Dask, Xgboost \\n \\n• \\nMulti-Class Text Classiﬁcation (NLP): Classiﬁcation of employee expenses from the reimbursement bills under predefined categories like \\ntravel, food, hotel etc. Modules: K-Means, TFIDF, Word2vec, XGBoost \\n \\n• \\nMulti-Channel Marketing Attribution Analysis (Optimization): Developed a workﬂow for analysing and predicting marketing channel that \\nleads to better conversions, worked on various approaches including First Touch, Last Touch, Linear, and Markov Models. Delivered \\ninsights on which model is better for the company’s business. \\n \\nMachine Learning Research Intern | CSIR - National Institute of Oceanography – Goa, India                  May 2018 – July 2018 \\n• \\nDeveloped an end to end machine learning framework to identify rock types (facies) under the earth surface. Transformed the problem into \\na machine learning multiclass classification with features based on wireline log measurements of the wells at a fixed depth interval. The \\nfeatures used were radioactivity, porosity, photoelectric effect, sound velocity, resistivity, depth and relative position. Oversampling \\ntechniques like SMOTE and ADASYN together with a random forest model resulted in a weighted F1-Score of 88 percent. \\nModules: Pandas, Numpy, Random Forest, Matplotlib, Imblearn \\n \\nAWARDS / DATA SCIENCE HACKATHONS: \\n  \\n Top 1% (Rank 7 out of 776) - AirQo Ugandan Air Quality Forecast Challenge (Zindi)                                                June 2020 \\n• \\nProblem Statement: Predict future air quality levels and empower communities to plan and protect their health. (Forecasting) \\n \\n Winner (Rank 1 out of 216) - COVID-19 Tweet Classification Challenge (Microsoft)                                                       May 2020 \\n• \\nProblem Statement: Develop a machine learning model to assess if a Twitter post is about COVID-19 or not. (NLP) \\n Winner (Rank 2 out of 272) - Akeed Restaurant Recommendation Hackathon (Zindi)                                                    May 2020 \\n• \\nProblem Statement: Build a recommendation engine to predict what restaurants customers are most likely to order from given \\nthe customer location, restaurant information, and the customer order history. (Predictive Analytics: Classification) \\n Winner (Rank 3 out of 314) - The Zimnat Insurance Assurance Challenge (Zindi)                                                          May 2020 \\n• \\nProblem Statement: Develop a predictive model that determines the likelihood for an insurance customer to churn - to seek an \\nalternative insurer or simply drop out of the insurance market altogether. (Predictive Analytics: Classification) \\n Top 0.5% (Rank 9 out of 2555) - A Data Science Hackathon by Bain & Company (Analytics Vidhya)                            Apr 2020 \\n• \\nProblem Statement: Time Series Sales Forecasting. (Multivariable - Multiple Time Series) \\n Top 0.5% (Rank 24 out of 6300) - LTFS Data Science FinHack 2 (Analytics Vidhya)                                                        Jan 2020 \\n• \\nBusiness Forecasting: Predict number of loan applications on a daily basis for different loan segments. (Regression) \\n Winner (Rank 5 out of 650) - Chartbusters Prediction: Foretell the Popularity of Songs (MACHINE HACK)                 Jan 2020 \\n• \\nProblem Statement: Predict how popular a song will be in future. (Predictive Modelling: Regression) \\n Winner (Rank 2 out of 1400) - Predicting Food Delivery Time - Hackathon by IMS Proschool (MACHINE HACK)       Nov 2019 \\n• \\nProblem Statement: (Predictive Modelling – Multiclass Classification) \\n Top 0.5% (Rank 11 out of 7000) - National Finalist - ZS Data Science Challenge 2019 (InterviewBit)                           July 2019 \\n• \\nReceived a PPO – Pre-Placement Offer for Data Science Associate Role. \\n• \\nProblem Statement: Probability of Ronaldo scoring a goal, Classification of tweets (Healthcare Professional or not) \\n \\nSKILLS: \\n \\n• \\nLanguages / Tools / Databases: Python, SQL, PySpark, DataBricks \\n• \\nWeb Scraping: BeautifulSoup, Selenium \\n• \\nData Pre-Processing and Visualization: Pandas, Numpy, Matplotlib, Seaborn, Plotly, Dash \\n• \\nMachine Learning / Deep Learning: Scikit-learn, Pytorch, Keras, Regression, Classiﬁcation, Clustering, Bagging, Boosting \\n• \\nNatural Language Processing: TF-IDF, Word2vec, Gensim, Transformers, spaCy, NLTK \\n• \\nWeak Supervised Learning: Snorkel \\n• \\nOptimization (Linear and Non-Linear Programming): PuLP, Scipy, Gekko \\n',\n",
       " '                                               MANASI SINGH \\n8130921709/manasisingh11@gmail.com \\n \\nAcademic Qualifications  \\nCourse Name  \\nCollege/School/University \\nYear of Passing \\nMarks \\nObtained (%) \\nMA Economics  \\nDelhi School of Economics, \\nDelhi University, New Delhi \\n2017 \\n63.4 \\nBA(HONS)   \\nEconomics \\nSri Venkateswara College, Delhi \\nUniversity, New Delhi \\n2015 \\n82.84 \\n12th Class  \\nArmy Public School, Dhaula \\nKuan, New Delhi \\n2012 \\n93.4 \\n10th Class  \\nHansraj Public School, \\nPanchkula \\n2010 \\n95 \\n \\nWork Experience \\n• \\nData Scientist at Fractal Analytics                                                      ( July 2017 – Feb 2020) \\no Growth Driver Analysis for a leading CPG firm  \\n▪ \\nIdentified the Key Performance Indicators (KPIs) \\n▪ \\nUsed Bayesian Belief Networks to leverage direct and indirect relationships, \\ncapturing lead-lag relationships, objectively making trade off decisions between \\nmarkets, functions and categories and making informed decisions at the correct \\ngranular level \\no Financial data forecasting for a leading CPG firm \\n▪ \\nForecasted key indicators such as GSV, TTS, Turnover  \\n▪ \\nBuilt a scalable machine forecasting architecture with approximately 25 models \\nachieving accuracies of greater than 90% on a consistent basis \\n▪ \\nUsed techniques like ARIMA, GARCH, ETS, Prophet, XGBoost, Bayesian etc \\no Automation of model generation for Growth Driver Analysis \\n▪ \\nAutomated the model generation process using Bayesian Belief Networks  \\n▪ \\nIncreased efficiency, eliminated manual modelling errors and achieved a \\nsignificant decrease in the time taken to generate models (From 3-5 hours to \\napproximately 1 hour) \\n \\nSkills & Certifications \\n• \\nTools: Proficient in R, Python, SQL, Stata, MS Word, MS Excel, MS PowerPoint \\n• \\nTechniques: Supervised learning techniques (Linear, Logistic, LDA, QDA, KNN, Bayesian \\nBelief Networks, SVM), Unsupervised learning techniques (PCA, K Means), Forecasting \\ntechniques (ARIMA, GARCH, ETS, Prophet), Decision Trees, Random Forest, XGBoost, \\nBagging, Boosting and Ensemble learning \\n• \\nCertifications: Practical Time Series Analysis by The State University of New \\nYork (Coursera), Neural Networks and Deep Learning by deeplearning.ai (Coursera), \\nStructuring Machine Learning Projects by deeplearning.ai (Coursera), Improving Deep \\nNeural Networks: Hyperparameter tuning, Regularization and Optimization by \\ndeeplearning.ai (Coursera) \\n \\nExtra-Curricular Activities & Academic Projects \\n• \\nFinal Year MA Project on estimating poverty lines for rural and urban India through the        \\nuse of the Linear Expenditure System                                                                                  2017                \\n',\n",
       " \"\\uf0e1 https://www.linkedin.com/in/nandini-\\nb-b4baaa178/\\n\\uf09b http://www.github.com/nandinib1999\\n\\uf0e0 nandinib1811@gmail.com\\nKEY SKILLS\\nPython\\nDeep Learning\\nTensorFlow\\nMachine Learning\\nMySQL\\nNLP\\nHTML\\nCSS\\nJavaScript\\nDjango\\nFirebase\\nSOCIAL LINKS\\nVOLUNTEERING\\nContent Writer for Women in ML \\nand Data Science, Delhi. \\nResponsible for curating the content \\nthat goes on social media platforms, \\ncreating posters for various events, \\nand blogs about different events that \\nare conducted.\\nVolunteer at Social Library ASU\\nIt is an initiative to provide the under-\\nprivileged hostel helps and workers \\nwithin the university premises with an \\nequal right to education. We \\nconducted classes to teach basic \\nEnglish and maths.\\nOPEN SOURCE\\nParticipant | Girl Script Summer of \\nCode\\nCERTIFICATIONS\\nPROFESSIONAL EXPERIENCE\\nRAPIDKEN.AI\\nJan '21- Present\\nAI ENGINEER\\nWork From Home\\nRAPIDKEN.AI\\nMay '20- Jan '21\\nSOFTWARE ENGINEER INTERN\\nSCIENTIA INNOVATION PVT LTD\\nFeb '20- May '20\\nSOFTWARE ENGINEERING INTERN\\nWork From Home\\nTHIRD EYE INC.\\nJun '19- Feb '20\\nDATA SCIENCE INTERN\\nNoida\\nEDUCATION\\nPOSTGRADUATE PROGRAM IN ARTIFICIAL \\nINTELLIGENCE AND MACHINE LEARNING\\nMay '20- Present\\nNIT Warangal\\nPost-Graduation\\nB.TECH CSE\\nAug '17- Present\\nApeejay Stya University\\nGurgaon\\nBachelor\\nGPA: 4.21/4.3\\nNandini Bansal\\nAI ENGINEER\\nKaggle \\nhttps://www.kaggle.com/nandini19\\n99\\nMedium \\nhttps://nandinibansal1811.mediu\\nm.com/\\nCodeChef \\nhttps://www.codechef.com/users/n\\nandinib1999\\nLeetCode \\nhttps://leetcode.com/nandinib1999\\n/\\nHealth Check - Added a new sub-\\nproject of Fetal Health \\nClassification\\nNeoAlgo - Added minimum \\nparenthesis removal & playfair \\ncipher algorithm \\nIntroduction to Deep Learning with \\nPyTorch | Udacity | Bertelsmann \\nAI Track Scholarship | November \\nDesigning rigorous algorithms for the task of extracting candidate phrases from the \\ntext documents and performed excessive processing on the text. These algorithms \\nhave reduced the bad looking or less meaningful candidates phrases by 10%.\\nWorking with supervised and unsupervised algorithms for linking similar documents. \\nWorked with BiLSTM-CRF and Sentence Transformers to develop a model for \\nkeyphrase extraction and document similarity. \\nFinetuned transformer models and worked on state-of-the-art methods such as \\nDAPT, TAPT to generate more meaningful embeddings of the text documents.\\nDesigned custom loss and evaluation functions for the embeddings generation.\\nDeveloped a prototype solution for automated evaluation of student answer sheet \\nusing Google Vision API and Mask RCNN.\\nAlso worked on fine-tuning the BERT for the STS task.\\nIdentified opportunities and derived insights through the use of algorithmic and \\nstatistical data mining & visualization techniques. \\nBuild predictive models and machine-learning algorithms and combine models \\nthrough ensemble modelling, and produce results. \\nExecute analytical methods while conveying the results to technical and business \\nteams.\\nCoursework includes Predictive Analytics, ML, CNN, RNN, Sequence Models, NLP, GANs \\nand Reinforcement Learning.\\nWorked on various projects such as Emotion Detection using CNN, Handwritten text \\nrecognition, Denoising the images using autoencoders, etc.\\nMember of Institutional Innovation Council (IIC) by AICTE & MHRD\\nFounding member & secretary of the technical club, Proxima - organized various \\nworkshops, seminars & hackathons as a part of it.\\nDr Stya Paul Memorial Scholarship Recipient \\nMember of Annual Technical Fest Organizers of ASU - TechShield\\nPROJECTS\\nDiabetic Retinopathy Severity Detection \\nusing EfficientNet & Attention Layer\\nApr '21- Present\\nhttps://github.com/nandinib1999/aptos19-diabetic-retinopathy\\nAs a part of the GSSOC'21 Health Check project, I am working on this project. The dataset \\nused was obtained from Kaggle. It consists of fundus scans of the eyes which are \\ncategorized into 5 classes based on the severity of DR. The dataset is highly imbalanced. \\nI have so far created a baseline model with EfficientNetB5 with a validation accuracy of \\n0.8449. I am currently working on improving the performance of the model further by \\nadding the Attention layer and TTA.\\nChest Abnormalities Detection using \\nXRay Scans and Tensorflow Object \\nDetection\\nJul '20 - Present\\nhttps://github.com/nandinib1999/vinbigdata-chest-abnormalities\\nAn automated system that could accurately identify and localize findings on chest \\nradiographs would relieve the stress of busy doctors while also providing patients with a \\nmore accurate diagnosis. In this project, I have worked with a dataset of 18000 diacom \\nscans to automatically localize and classify 14 types of thoracic abnormalities. For \\nlocalization & classification, I have used Tensorflow Object Detection API.\\nQuotes Generation using GPT2 Language \\nModeling\\nNov '20- Feb '21\\nhttps://github.com/nandinib1999/gpt2_quotes_generation | https://huggingface.co/nandinib1999/quote-\\ngenerator\\nUsing the language modelling scripts provided by HuggingFace, I finetuned the GPT2 \\nmodel on a custom dataset of inspirational quotes to synthetically generate quotes. I used \\nGoogle Colab's GPU for the fine-tuning task and the perplexity of the model after the first \\nepoch was 15. I have deployed the model on the HuggingFace model hub.\\nCar License Plate Scanner\\nMay '20-\\nhttps://github.com/nandinib1999/license-plate-scanner\\nUsing darknet Yolo, I have trained a custom YOLO object detector for detecting a car \\nlicense plate. The dataset used is a combination of the Kaggle dataset and some images \\nscraped from the internet. Image processing techniques are applied to the detected \\nlicense plate to extract the text using PyTesseract.\\nFake or Real Tweet using BERT & \\nTensorFlow\\nMar '20-\\nhttps://www.kaggle.com/nandini1999/fake-or-real-tweet-bert-nlp\\nThere are countless sources of fake news nowadays that continue to spread false \\ninformation 24/7. Social Media has become one of the biggest platforms to spread such \\nlies. Using the BERT model, I trained a model over a dataset of disaster tweets to identify \\nwhich one is fake. The model achieved a public score of 0.83 on the Kaggle Leaderboard.\\n2019 – March 2020\\nPL/SQL Oracle Certified Associate \\n| Oracle | September 2019 \\nPractical Machine Learning with \\nTensorflow | IIT Madras | Google | \\nAugust 2019 – November 2019\\nMachine Learning | Stanford \\nUniversity | May 2019 – August \\n2019\\nThe Joy of Computing with Python \\n| IIT Madras & IIT Ropar | August \\n2018 – November 2018\\n\",\n",
       " 'NITHILAA U\\n18PD22\\nFather’s name\\nGender\\nDate of Birth\\nLanguages known\\nEmail\\nMobile\\nUma Sankar E\\nFemale\\n28th September 2000\\nEnglish, Tamil, Telugu\\nnithilaau28@gmail.com\\n+91-89408-02277\\nPermanent Address\\n11, Kavitha Layout,\\nIswarya Nagar, Extn,\\nUdumalpet,\\nTamil Nadu – 642154.\\nOBJECTIVE\\nTo obtain a position as a Student Intern for a period of six months from May\\n2021 to November 2021.\\nACADEMIC QUALIFICATION\\nCurrently pursuing 3rd year of 5-year Integrated M.Sc. Data Science at the\\nDepartment of Applied Mathematics and Computational Sciences at PSG College of Technology.\\nSKILL SET\\nLanguages\\nPython, C++, R, C\\nBack-End\\nOracle, SQL\\nPlatform\\nWindows, Linux\\nTools\\nTableau\\nAREAS OF INTEREST\\n●\\nSupervised and Unsupervised learning\\n●\\nProbability and Statistics\\n●\\nData Visualization\\nACADEMIC RECORD\\nCourse\\nInstitution\\nBoard/University\\nCompletion By\\nMarks\\nM.Sc.\\nPSG College Of Technology,\\nCoimbatore.\\nAnna University\\n2023\\n8.49\\nXII\\nSrinivasa Vidhyalaya Mat. Hr.\\nSec. School, Udumalpet\\nState Board\\n2018\\n93.25%\\nX\\nDISHA - A Life School, Pollachi\\nICSE\\n2016\\n93%\\nINDUSTRY BASED PROJECT EXPERIENCE\\nChamber of Products (August 2020 - September 2020) - Machine learning intern\\nWorked on developing an efficient text summarizer for any given news article\\nwith different text summarizing techniques like TFIDF, Gensim, and found BERT to be more\\nefficient. Also, created a simple website to demonstrate the proper working of the summarizer\\ncreated.\\nNON-ACADEMIC PROJECT\\nDesignable (September 2020 - Till date) - Software Designing Intern\\nWorked with PSG alumni in creating a product development software. The\\nwebsite aims in exploring the market survey, by formulating the hypothesis of a particular\\nproblem, finding the user and the competitor of the product using SWOT analysis. It has a\\nsurvey template, a product roadmap, and a dashboard to visualize the market segmentation\\ncriteria. The front end of the website was developed using HTML5, CSS, and JavaScript, and the\\nback end using Flask.\\nACADEMIC PROJECTS\\n●\\nStockLock, the study of the impact of COVID-19 in Indian Stock Market and GDP Response,\\na research paper, implemented in Python to study and analyze effects of a pandemic on\\nthe stock market using various ML regression algorithms like linear, multiple linear,\\npolynomial, and SVM regression, and statistical models of ARIMA.\\n●\\nSmartTweet, a Twitter sentiment analysis tool, developed using Python, which classifies\\nthe tweets according to the sentiment expressed in them - positive, negative, or neutral\\nusing the open-source python package, Tweepy to get the Twitter data. The Naive\\nBayesian model has been deployed to classify the tweets, along with graphical analysis.\\n●\\nH1B-VisualBay, Analysis of H1B visa approval, developed using Python, using the H1B visa\\npetitions data from 2011 to 2016, from Kaggle. This project helps to find out which\\nlocations, employers, job titles and salary range makes up most of the H1B petitions and\\nhelps in graphical analysis of data using lollipop, donut plot, and violin plots.\\n●\\nHealry, a website hosted on google drive about COVID-19, to study the various metrics and\\nKPIs using Google Analytics. The website aims in creating awareness about the COVID-19\\npandemic through blogs and self-check quiz and helps in visualizing the real-time data\\nusing interactive and live COVID-19 updates. The front end was developed using HTML5,\\nCSS, and JavaScript.\\n●\\nSoccerStats, Analysis of international football results from 1872 to 2019, developed using\\nR. This project presents interesting results for football lovers by analyzing head-to-head\\nbalances in football games and which countries are currently showing the greatest\\nprogress in official football games, using various plots and data visualization techniques.\\nEXTRA-CURRICULAR ACTIVITIES AND ACHIEVEMENTS\\n●\\nCertified by Coursera for the completion of the courses Deep Learning and Neural\\nNetworks And Deep Learning offered by deeplearning.ai.\\n●\\nCertified by Coursera for the completion of the course Exploratory Data Analysis offered\\nby Johns Hopkins University.\\n●\\nKaggle contributor.\\n●\\nWinner (1st), District (Coimbatore) Level Drawing Competition, hosted by the Tamilnadu\\nForest Department.\\n●\\nParticipant, State (Tamilnadu) level drawing competition, hosted by the Electricity Board\\nOf India.\\n●\\nWas a part of the Entrepreneur Club, PSG College Of Technology.\\nDECLARATION\\nI, Nithilaa U, do hereby confirm that the information given above is true to the\\nbest of my knowledge.\\nPlace: Coimbatore\\nDate : 03/05/2021\\n(Nithilaa U)\\n',\n",
       " 'Omkar Pathak\\nSOFTWARE ENGINEER · FULL STACK PYTHON DEVELOPER\\nPune, Maharashtra, India\\n\\uf10b (+91) 8087996634\\n|\\n\\uf0e0 omkarpathak27@gmail.com\\n|\\n\\uf015 www.omkarpathak.in\\n|\\n\\uf092 OmkarPathak\\n|\\n\\uf08c omkar-pathak-94473811b\\n“Make the change that you want to see in the world.”\\nExperience\\nSchlumberger\\nPune, Maharashtra, India\\nDATA ENGINEER\\nJuly 2018 - Present\\n• Responsible for implementing and managing an end-to-end CI/CD Pipeline with custom validations for Informatica migrations which\\nbrought migration time to 1.5 hours from 9 hours without any manual intervention\\n• Enhancing, auditing and maintaining custom data ingestion framework that ingest around 1TB of data each day to over 70 business\\nunits\\n• Working with L3 developer team to ensure the discussed Scrum PBI’s are delivered on time for data ingestions\\n• Planning and Executing QA and Production Release Cycle activities\\nTruso\\nPune, Maharashtra, India\\nFULL STACK DEVELOPER INTERN\\nJune 2018 - July 2018\\n• Created RESTful apis\\n• Tried my hands on Angular 5/6\\n• Was responsible for Django backend development\\nPropeluss\\nPune, Maharashtra, India\\nDATA ENGINEERING INTERN\\nOctober 2017 - January 2018\\n• Wrote various automation scripts to scrape data from various websites.\\n• Applied Natural Language Processing to articles scraped from the internet to extract different entities in these articles using entity\\nextraction algorithms and applying Machine Learning to classify these articles.\\n• Also applied KNN with LSA for extracting relevant tags for various startups based on their works.\\nGeeksForGeeks\\nPune, Maharashtra, India\\nTECHNICAL CONTENT WRITER\\nJuly 2017 - September 2017\\n• Published 4 articles for the topics such as Data Structures and Algorithms and Python\\nSofttestlab Technologies\\nPune, Maharashtra, India\\nWEB DEVELOPER INTERN\\nJune 2017 - July 2017\\n• Was responsible for creating an internal project for the company using PHP and Laravel for testing purposes\\n• Worked on a live project for creating closure reports using PHP and Excel\\nProjects\\nPyresparser\\nAPI/Python Package\\nPERSONAL PROJECT\\nJuly 2019 - Present\\n• A simple resume parser used for extracting information from resumes\\n• Extract information from thousands of resumes in just a few seconds\\n• Author and maintainer of this project\\nGarbage Level Monitoring System\\nIoT\\nTEAM PROJECT\\nOctober 2017 - May 2018\\n• To find a economical and smarter alternative to current garbage problems\\n• Users can monitor levels of all garbage bins from a global dashboard provided\\n• Was responsible for Django backend development\\nNOVEMBER 3, 2019\\nOMKAR PATHAK · RÉSUMÉ\\n1\\nPygorithm\\nAPI / Python Package\\nPERSONAL PROJECT\\nJuly 2017 - Present\\n• Author and maintainer of this project\\n• An educational library to teach all the major algorithms\\n• Got covered in Fosstack, FullStackFeed, Kleiber and Tagged under Hotest Github Project on ITCodeMonkey\\nSmart Surveillance System using Raspberry Pi and Face Recognition\\nIoT\\nPERSONAL PROJECT\\nJanuary 2017 - February 2017\\n• Face Recognition using OpenCV and Python\\n• Raspberry Pi was used as the data server\\n• User notified if any suspicious activity detected in real time\\nPassword Strength Evaluator using Machine Learning\\nMachine Learning\\nPERSONAL PROJECT\\nMarch 2017\\n• SVM algorithm used for training and classification\\n• Flask framework used\\n• Self-generated dataset\\nEducation\\nMarathwada Mitra Mandal’s College of Engineering\\nPune, Maharashtra, India\\nB.E. IN COMPUTER ENGINEERING\\n2014 - 2018\\n• Aggregate 74%\\nSkills\\nProgramming Languages:\\nPython, C, PHP, C++, Shell Script\\nFrontend Technologies:\\nHTML, CSS, JavaScript, Angular 6/7\\nBackend Technologies:\\nDjango, Flask (Python), Laravel (PHP)\\nOperating Systems:\\nLinux, Unix, Windows\\nDatabases:\\nMySQL, SQLite, MongoDB\\nOther:\\nGit, NLP, Scikit-Learn, OpenCV, Cloud (GCP, Azure, DigitalOcean)\\nHonors & Awards\\n2018\\nTop rated Python developer, in Pune and Fifth in India at Github\\nIndia\\n2018\\nQuora Top Writer,\\nIndia\\n2018\\nAwarded ‘The Best Outgoing Student Award 2017-18’,\\nMMCOE, Pune\\n2018\\nWon 2nd Prize, in an Hackathon organized by MIT-ADT Persona Fest 2018\\nPune\\n2018\\nBest Paper Award, in National Level Conference on “Emerging Trends in Computing , Analytics\\nand Security - 2018”(NCETCAS-2018)\\nMMCOE, Pune\\nExtracurricular Activities\\nContributor in Pune PyCon 2018\\nPUNE, MAHARASHTRA, INDIA\\n2018\\n• Was a part of Website Designing and volunteering committee\\nNOVEMBER 3, 2019\\nOMKAR PATHAK · RÉSUMÉ\\n2\\nMentor at GirlScript Summer of Code 2019\\nPUNE, MAHARASHTRA, INDIA\\n2019\\n• Mentored 4+ teams in various domains\\nOrganizing head for the National level technical event -\\nInnovatus\\nPUNE, MAHARASHTRA, INDIA\\n2018\\n• Organized project competitions\\nWorkshop on IoT and Python\\nMMCOE, PUNE\\n10 Jan 2017\\n• Conducted a workshop for second year students to give them a brief overview about IoT by completing three mini projects and taught\\nthem basics of Python programming language\\nPublications\\nSmart Surveillance System using Raspberry Pi and Face\\nRecognition\\nDOI10.17148/IJARCCE.2017.64117\\nGarbage Level Monitoring System\\nInterests\\n• Competitive Programming\\n• Photography\\n• Sketching\\n• Reading/Writing on Quora\\n• Contributing to Open Source projects\\nNOVEMBER 3, 2019\\nOMKAR PATHAK · RÉSUMÉ\\n3\\n',\n",
       " 'Prashant Arora\\nMachine Learning Enthusiast\\nMentoring machines to perform better .\\nprashantarora998@gmail.com\\n08630831390\\nAmroha, Delhi, India\\nlinkedin.com/in/prashant-arora-3063b5155\\ngithub.com/neyoxdrago\\nWORK EXPERIENCE\\nMachine Learning Engineer\\nMyWays\\n07/2020 - 08/2020, \\nWork From Home\\nTask was to debug some existing codes of recommendation systems.\\nTeaching Assistant\\nCoding Blocks\\n05/2020 - Present, \\nWork From Home\\nClearing Doubts of students raised during online and oﬄine classes .\\nMachine Learning Intern\\nAnalysed.in\\n07/2019 - 10/2019, \\nWork From Home\\nUsing Natural Language Processing to build a resume builder . Working\\non analysis of different kinds of resumes.\\nMachine Learning Intern\\nAvanov solutions\\n04/2019 - 06/2019, \\nWorking on different computer vision problems like person counter\\nsystem , detecting users mood , detecting damages on vehicles,etc.\\nEDUCATION\\nBachelor of Science ( Hons. ) Computer\\nScience\\nKeshav Mahavidyalaya , University Of Delhi\\n2017 - 2020, \\nCLASS XII\\nDelhi Public School , Moradabad\\n2017, \\n91%\\nCLASS X\\nDelhi Public School , Moradabad\\n2015, \\n9.4 CGPA\\nSKILLS\\nMachine learning\\nPython\\nDeep Learning\\nComputer Vision\\nNatural Language Processing\\nMySQL\\nFlask\\nData Analytics\\ndjango\\nPERSONAL PROJECTS\\nDetecting Brain Tumor with Brain MRI scans\\n (08/2019 - 08/2019)\\nDetermining the probability of brain tumor to be predicted on the\\nbasis of brain MRI scans.\\nEmail Answering Bot (07/2019 - 07/2019)\\nA bot to automatically reply all new unread emails with a deﬁned\\nformat and text.\\nSkin Lesion detection to predict skin cancer\\n (06/2019 - 06/2019)\\nDetecting and segmenting Skin lesions through skin images for\\npredicting skin cancer\\nSentiment Analysis of Movie reviews\\n (05/2019 - 05/2019)\\nClassifying user review as positive , neutral or negative .\\nAchieved 90+ % accuracy.\\nACHIEVEMENTS\\nGlobal Rank 17 on Machine Hack.\\nLANGUAGES\\nEnglish\\nProfessional Working Proﬁciency\\nHindi\\nFull Professional Proﬁciency\\nINTERESTS / HOBBIES\\nReading\\nPlaying Football\\nTravelling\\n',\n",
       " 'RAJAT RANJAN \\nMobile No: +919886043959 (IN) \\n+61432028099 (AU) \\n \\n \\nEmail: rajat.ranjan24@gmail.com \\nCurrent Location: Melbourne AU \\n \\n \\n \\n \\n \\nObjective: \\nTo pursue a challenging career and be a part of progressive organization that gives a scope \\nto enhance my knowledge and utilizing my skills towards the growth of the organization and \\nself. \\n \\nProfessional Summary \\n \\nExperienced System Engineer with a demonstrated history of working in the information \\ntechnology and services/product industry. Skilled in J2EE Web Services, Python, React, Web \\nDevelopment and MySQL Database along with insights in Machine Learning. \\nStrong and productive in Front end Applications. Experience in agile as well as waterfall \\nmodels along with CI/CD using bamboo with a background in Banking Industry. Machine \\nlearning with experience in data wrangling, analytics and pre-processing and gaining insights \\nusing data visualisations with keen to build model that best fits the problem statement. \\n \\n \\nExperience Summary: \\n1. Deputee : ANZ (Melbourne AU) (Nov, 19 – Present) \\nDesignation: Developer (Australia) \\n \\nCompany: EdgeVerve (An Infosys Company) \\nDesignation: Senior Systems Engineer (Pune) \\n \\nProjects \\n1. Distributed and loosely coupled application development of Internet banking \\nusing microservices. Maintaining release activities and restful API contracts. \\n2. Development in React/Redux, Spring boot, J2EE applications, python scripting, \\ntest API automation. \\n3. Evaluate effective risk management on the borrower’s overall ability to repay \\nloan with domain specific knowledge and insights using customer data with \\nvisualisations. \\n4. Analytics based solution for Customer Churn by recommending customer centric \\nproducts using recommendation engines. Dashboarding done in React JS with \\nbackend data from model pipeline in Python. Documented findings. \\n5. Data analysis of used services in Banking industry, its feasibility and optimisation \\nfor new use cases. \\n \\n \\n2. Designation: Senior Systems Engineer (Pune, India) \\nCompany: EdgeVerve (An Infosys Company) (Nov, 16 – Nov,19) \\n \\nProjects \\n \\n1. Customisation in Finacle Product development for ANZ internet banking. \\nDeveloping microservices architecture in Java application and maintaining SQL \\nscripts for release activities. \\n2. Performing Unit test, UAT for newly added services and creating proof of concept \\nto integrate applications with third party services. \\n3. Information Extraction of important clauses in an ISDA document and to develop \\npredictive modelling so as to reduce human effort. More than 60% of human \\neffort was reduced and the combined accuracy of trained models skyrocketed to \\n77% for selected clauses. \\n4. Maintaining JIRA and updating user stories for better understanding of \\nrequirements aligned with Agile work flow. \\n \\n \\n3. Company: Infosys Limited (Jun, 16 – Nov, 16) \\nDesignation: Systems Engineer Trainee (Mysore, India) \\n \\nProjects \\n \\n1. Web Based customer friendly clothing E commerce website using Angular, JAVA \\nand web services as a full stack developer incorporation Agile workflow. \\n2. Python terminal Application for Cricket tournament Management system \\nintegrated with MySQL (OOP Framework) \\n3. Learning new technologies with Python, Machine learning and participation in \\nhackathons. \\nCore Competencies: \\n \\n \\n \\nTechnologies \\nPython, Flask, Keras, TensorFlow, Data Pre Processing, Data \\nManipulation, Data Visualisation, Classification & Regression, \\nPredictive Modelling, Natural Language Processing, \\nCompetitive \\nProgramming, Microservices, Java, Gradle, React JS,Redux \\nDatabases \\nMy SQL , Mongo DB \\nTools \\nJupyter Notebook, Anaconda, Postman, JIRA, PyCharm, IntelliJ \\nIdea, MS Office, VS studio code \\nOthers \\nGIT, Jenkins, Bamboo, GitHub, Bitbucket, JIRA \\nFunctional \\nClient Engagement, Showcase activities, Sprint Planning and \\nreview, Retrospectives, Creative Thinking, Design thinking. \\n \\n \\n \\n \\n \\n \\nAwards & Achievements: \\n• 7th Rank in AWS Deepracer League organised by ANZ. \\n• 1st at CRISIL Machine Learning Hiring Challenge’19 for Information Text Extraction \\n• Intel Edge AI Scholarship Winner 2019, Nanodegree Udacity \\n• Microsoft Azure Scholarship Winner Udacity \\n• Top 21 finalist in YES Bank Datathon for providing Machine learning based Banking \\nSolution. \\n• 3rd in Brainwaves 2019 Machine Learning Hackathon by Societe Generale. \\n• 11th in LTFS FinHack Machine Learning Hackathon among thousands of participants \\n• Top 5 in MachineHack Global Leaderboard \\n• Top 10 in Analytics Vidya Global Leaderboard Rankings \\n• 3rd in Edelweiss Hackathon Machine Learning organised by Hackerearth \\n• Top 25 ranking in Crowd Analytix Hackathon - Propensity to fund Mortgages \\n• Ranked under 25 in many Hackathons organised by Analytics Vidya \\n• Gold Level Badge in Hackerrank Problem Solving and Python sections \\n• Appreciations for Client Showcase activities \\n• Ranked among top 5000 best coders in Techgig Code Gladiators and Code Chef \\n• Competitions Contributor in Kaggle \\nCertifications: \\n• Python for Data Science & Machine Learning BootCamp – Udemy \\n• Machine Learning by Andrew Ng - Coursera \\n• Machine Learning Developer Summit 2019 \\n• Intel Edge AI Scholarship Nanodegree Udacity 2019 \\n• AMCAT Certified \\n• Certificate of Excellence by Techgig for Machine Learning Hackathons (Code Gladiators 18) \\n• EV Super league – Best Performer Certificate consecutively for two quarters and High \\nPerformer award at EdgeVerve \\n \\n \\nProjects Undertaken – Self \\n \\n• Sentiment Analysis - Dynamic UX (Full stack Machine Learning) embedded with ML \\nmodels to predict customer feedback sentiment over web. Web Server using Flask, \\nModel Deployment using Heroku, Github. \\nTools: Python, NLTK, RESTful Web Services \\n \\n• ML Models as Scalable Micro Services - Generic API for regression and classification \\nusing embedded models over web to cater different datasets with integrated \\npipeline. \\nTools: Python Scripts, CircleCI, joblib, RESTful, Heroku, Git \\n \\n• Keras Transfer Learning - Keras pretrained ResNet Model deployed on GCP with \\nFront end in React to display the label of Images with its probability. \\nTools: GCP, Python Flask, React, ML \\n \\n \\nQualifications: \\n \\n \\nS. No \\n \\nBoard \\nUniversity/ \\n \\nInstitution \\n \\nYear \\nPercentage/ \\n \\nCGPA \\n \\nClass \\n \\n1. \\nB. Tech in Computer \\n \\nScience & Engineering \\nG.I.T.A. \\nBhubaneswar \\n \\n2016 \\n \\n8.49 \\n \\nFirst \\n2. \\nXII Board – CBSE \\nD.A.V. Hehal , Ranchi 2012 \\n89.2% \\nFirst \\n \\n3. \\n \\nX Board – CBSE \\nSurendra Nath \\nCentenary School, \\nRanchi \\n \\n2010 \\n \\n9.6 \\n \\nFirst \\nLinks: \\n• Github: https://github.com/rajat5ranjan/ \\n• Analytics Vidya: https://datahack.analyticsvidhya.com/user/profile/rajat5ranjan \\n• MachineHack: https://www.machinehack.com/members/rajat5ranjan/ \\n• Hackerearth: http://www.hackerearth.com/@rajat5ranjan \\n• Portfolio Website: https://rajat5ranjan.github.io \\n• LinkedIn: https://www.linkedin.com/in/rajat-ranjan24/ \\n• Kaggle: https://www.kaggle.com/rajatranjan/ \\n \\n \\n \\n \\n \\n \\nDeclaration: \\n \\nI hereby declare that all the above information mentioned regarding me & my credentials \\nare true to the best of my knowledge. \\n \\n \\nPlace: Melbourne AU \\n(Rajat Ranjan) \\n',\n",
       " 'RAVI NIRALA\\n+919661276718 | ravinirala123@gmail.com | linkedin.com/in/ravinirala | github.com/ravinirala\\nEDUCATION\\nInstitute\\nDegree/Certificate\\nYear\\nIndian Institute of Technology, Delhi\\nB.Tech in Chemical Engineering\\n2017 - Present\\nKrishak College Dheodha, Bihar\\nClass XII (BSEB)\\n2017\\nGyan Bharti Model R.C.\\nClass X (CBSE)\\n2015\\nTECHNICAL SKILLS\\nLanguages: Python, SQL, MongoDB, C, C++, HTML\\nSoftwares: Jupyter Notebook, Tableau, MS Excel, MS Powerpoint, Adobe Photoshop\\nLibraries: Pandas, NumPy, Matplotlib, Seaborn, Scikit-Learn, NLTK, TensorFlow\\nPROJECTS\\nFace Mask Detector | Python, OpenCV, CNN\\nJune 2020 - July 2020\\n• Used OpenCV to capture, resize and modify colormap of images\\n• Developed Convolutional Neural Network with a fully connected layer to classify images as with or without mask\\n• Tested the model in conjunction with Cascade Classifier to detect human faces through webcam feed\\nTwitter Sentiment Analysis | Python, NLTK, NLP\\nJune 2020\\n• Performed techniques like Tokenization and Stemming from Natural Language Toolkit library to prepare dataset\\n• Implemented Term frequency-Inverse document frequency to represent text into numerical features after\\npreprocessing the dataset\\n• Applied Logistic Regression and Random Forest Classifier for sentiment analysis classification\\nWhatsApp News Bot | Python, Flask, Twilio\\nDecember 2019\\n• Built a WhatsApp Bot to provide news for a query using Twilio\\n• Used NewsAPI to fetch news from different sources for a given query\\n• Developed the web application using Flask and deployed it on Heroku for end users\\nMachine Learning Model for prediction of Loan Defaulters | Python, ML\\nMay 2020\\n• Predicted loan Defaulters for a given dataset using machine learning models like Logistic Regression, Decision Tree and\\nRandom Forest. Random Forest model gave the highest accuracy\\n• Applied and tested various techniques like imputation of missing values, feature engineering and preprocessing of data\\nSCHOLASTIC ACHIEVEMENTS\\n• Joint Entrance Examination (JEE) Main, 2017 : Secured rank in top 1 percentile among 12 lakh+ candidates\\n• Joint Entrance Examination (JEE) Advanced, 2017 : Secured rank in top 5 percentile among 2.5 lakh+ candidates\\n• Certificate of Merit : Awarded by MHRD, Govt. of India for outstanding performance in all subjects\\nCERTIFIED COURSES\\n• Machine Learning A-Z : Hands On Python In Data Science - Udemy\\n• Data Visualization and Communication with Tableau - Coursera\\n• Introduction to SQL and Intermediate SQL for Data Science - DataCamp\\nPOSITIONS OF RESPONSIBILITY\\nCreative Activity Head, Rendezvous\\nJuly 2018 - Oct 2018\\n• Created Posters and Banners for promotion of multiple events of the festival\\n• Designed Advertisement published in the newspaper, The Hindu, for promotion of the festival\\nPublicity Coordinator, Spic Macay\\nSep 2019 - Oct 2019\\n• Achieved 50% increment in registrations by coordinating with 10+ colleges in Delhi\\n• Achieved 100% YOY increment in online outreach, managed all official communication\\n',\n",
       " '/\\nSIDHARTH SACHDEVA\\n sidharth19.sachdeva@gmail.com\\n 09650302606\\n www.linkedin.com/in/sidharth-sachdeva-5035b598/\\n sidharth19s\\n\\uf0e0\\n\\uf095\\n\\uf041 H-173, DLF Skycourt, Sector-86, Gurgaon\\n\\uf0e1\\n\\uf09b\\nSUMMARY\\nExperienced Data Scientist with hands-on skills in Python, SQL, Statistical Analysis, Machine Learning, Deep Learning & Model Building with good\\nvisualization and presentation skills. 8+ yrs of experience across shipping and healthcare.\\nAdept in Data-driven decisions help to maximize business development by deriving valuable insights and building predictive models using endless relevant\\ndata.\\nComing with hands-on experience in data wrangling, exploratory data analysis, and Predictive modelling to extract actionable information from huge\\ndatasets.\\nExpertise in solving analytical problems using quantitative and statistical approaches and developing a predictive statistical model through supervised and\\nunsupervised machine-learning techniques.\\nEMPLOYMENT\\nPlunes\\nAssociate Product Manager · Mar. 2020 to Oct. 2020 · Gurgaon\\nLed a team of 12 contributors to complete Delhi-NCR onboarding with Tier-1 hospitals.\\nNegotiated and established exclusive long-term business agreements at slashed rates.\\nLaunched a Covid response team, which helped receive Series B funding.\\nMaersk\\nMarine Engineer · Jan. 2011 to Mar. 2015 · Copenhagen\\nAmong the first engineers to work on the largest container ship in the world.\\nLed design and operation changes across an entire fleet of vessels.\\nQNET\\nConsultant Executive Sales Analyst · Mar. 2015 to Nov. 2019 · \\nGurgaon\\nLed a team of 30+ contributors to expand distribution channels across 4 metropolitan cities.\\nAlong with my team, responsible for revenue generation of over 6 Cr. \\nEDUCATION\\nSpringboard · June 2020 to Mar. 2020\\nData Science Certification/2020/Springboard. Currently working on deep learning along with two capstone projects.\\nAMET · June 2006 to Mar. 2011\\nB.E. Marine Technology 2011\\nStudied marine engineering and nautical science in a dual course and got placed with the largest container shipping company in the world, Maersk Line.\\nPROGRAMMING SKILLS: Python , Pandas, Numpy, Sklearn, Matplotlib, Seaborn\\nMACHINE LEARNING / DEEP LEARNING: K-nearest neighbors, Random Forests, Naive Bayes, Regression Models, PyTorch\\nDATABASE MANAGEMENT/ DATA SOURCING: MySQL, PostgreSQL, Spark, PySpark\\nPROBABILITY & STATISTICS: Probability Basics and Random Variables, Probability Distributions, Hypothesis Testing, Modelling\\nDATA VISUALIZATION: Tableau, PowerBI,MS Excel, Plotly\\nSKILLS\\nPROJECTS\\nFootball Match Result Prediction\\nApr. 2020 to Nov. 2020\\nUsing 10 years of football match data on the EPL, I have built a multi-class classification model.\\nHaving applied machine learning I am able to predict football matches with better probability than the bookies.\\nTechnology Used: Python, Beautiful Soup, Machine Learning – Random Forest Model, XGBoost, Platform – Jupyter Notebook\\nDescription: Data sourced from a combination of different websites using Beautiful Soup. Data cleaning, preprocessing followed by EDA of each match’s\\nattributes across 10 years. Hypothesis testing done for 20+ hypothesis for better understanding of attributes along with outliers.\\nModel selection done using various ML algorithms with cross validation and hyper-parameter tuning. \\nPersonal Loan Campaign prediction\\nAug. 2020 to Nov. 2020\\nUsing data for 5000 existing customers, built a classification model to predict success in a personal loan campaign.\\nHaving applied machine learning, good recall value has been achieved, and along with lift chart analysis, will help with customer onboarding.\\nTechnology Used: Python, Machine Learning – Logistic Regression, Decision Tree, Pruned Decision Tree, Random Forest Model, Gradient Boosting along\\nwith \\nH2o from AutoML, Platform – Jupyter Notebook\\nDescription: EDA on data reveals certain very interesting patterns in customer behaviour, which combined with prediction from ML models along with the\\nlift chart will result in a successful targeted marketing campaign.\\nModel selection done using various ML algorithms with cross validation and hyper-parameter tuning. \\n',\n",
       " 'ANJALI SINGH \\n  Business Analytics Specialist, \\n      Advanced Analytics \\n \\n \\n \\nEmail: anjali9singh@gmail.com \\nPhone: +91-8884544016 \\nAddress: Hyderabad, Telangana \\nSOCIAL MEDIA \\nWORK EXPERIENCE \\n \\nhttps://www.linkedin.com/in\\n/anjali-singh-08316123/ \\n \\nPROFESSIONAL SKILLS \\nMachine Learning Algorithms \\nStatistical Modelling \\nPython \\nHive \\nPostgreSQL \\nAzure DataBricks \\nProgram Management \\nStakeholder Management \\n EDUCATION \\nExperienced and Data Driven Data Scientist with approx. 8 years of experience in the \\nIndustry. Creative, and highly organized with the ability to integrate out-of-the-box \\nthinking and problem-solving analysis to improve processes.  \\n \\n \\n \\n \\nBusiness Analytics Specialist, Advanced Analytics & Insights \\nMicrosoft / October 2018 - Present \\n• \\nDeveloped and delivered Delivery Excellence and Global Capacity Management \\npredictive solutions  \\n• \\nApplying statistical & Machine Learning techniques to problems such as Customer   \\nSatisfaction analysis, Demand Forecasting using Prophet, Consulting Projects \\nBurn Forensics to understand usage trends, Insights, and drivers \\n• \\nEnd-to-end execution of the Data Science process: understanding business \\nrequirements, data discovery and extraction, model development and evaluation, \\nto production pipeline implementation \\n• \\nDesign, Develop and code scripts \\no \\nTo cleanse, integrate and evaluate large datasets from multiple disparate \\nsources including structured, and unstructured data \\no \\nTo embed Machine learning algorithms such as logistic regression, \\nregularized models, random forests, clustering, XGBoost, Catboost, stacking \\netc., and automate processes \\n• \\nDevelop Black Box Interpretability solutions using Shapley values SHAP \\nframework, InterpretML framework \\n Business Analyst II – Advanced Analytics, Data Science \\nVMware / October 2016 – August 2018 \\n• \\nDeveloped Propensity Statistical Models across areas such as Sales and \\nMarketing - cross-sell/up-sell analytical models, and Global Support & Services \\nand Customer Success Solutions \\n• \\nGenerate insights to conceptualize and drive development of analytical \\nsolutions. Build statistical models for renewals-cross-sell, support services, \\ncustomer satisfaction – for upgrade, product satisfaction (PSAT) etc. \\n \\nCONTACTS \\nData Analyst \\nFlipkart- ekart Analytics, Consultant/ September 2015 – October 2016 \\n• \\nWorked extensively on huge datasets to perform data analysis to design metrics \\n(KPIs) to measure the performance of the business processes, and to provide \\ncompelling management reporting on a regular basis \\n• \\nDeveloped solution for Perfect Customer Delivery, and Perfect Customer \\nExperience \\nSoftware Engineer – Web Analytics Developer \\nHCL Technologies Ltd / September 2010 – July 2013 \\n• \\nAnalyzed web traffic by using conversion and traffic variables, and captured the \\ncustomer journey on the online stores to generate insights, and identify issues \\nsuch as bounce rate \\n• \\n \\nSoftware Developer – Business Intelligence \\nTeam Computers Pvt Ltd / July 2010- September 2010 \\n• \\nDeveloped Data Models, and created reports and dashboards for business \\npartners \\n \\nHackathons \\n3rd Rank  - Women Data Science \\nHackathon by Bain & Company \\n \\nMobility Analytics -Rank 1st \\nhttps://datahack.analyticsvidhya.com/cont\\nest/janatahack-mobility-\\nanalytics/#LeaderBoard \\nRecommendation Systems- \\nRank 2nd \\nhttps://datahack.analyticsvidhya.com/cont\\nest/janatahack-recommendation-\\nsystems/#LeaderBoard \\n \\nMLADS Speaker  \\n \\n \\n \\n \\n \\n \\n   \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nBusiness Analytics- ISB, 3.32/4  \\nB. Tech (IT)- UPTU, 78.5% \\n12th Std- CBSE, 79.2% \\n10th Std- CBSE, 84.6% \\n \\n \\nBusiness Analytics Graduate, \\nISB Hyderabad \\nPredictive Intelligence for \\nAutomatic Global Support Ticket \\nRouting \\nMicrosoft- Machine Learning, AI and Data Science Conference, Fall \\n2020 \\n',\n",
       " 'SUNIL KUMAR\\n+91 7397419727\\nsunilkumarn@protonmail.com\\nlinkedin.com/in/sunil-kumar\\ngithub.com/DragonPG2000\\nEDUCATION\\nBachelor of Technology | Software Engineering\\nAug. 2017 – May 2021\\nSRM Institute of Science and Technology\\nChennai, India\\nWORK EXPERIENCE\\nAssociate Researcher\\nJuly 2018 – August 2019\\nNextTech Minsky AI Lab.\\nChennai, India\\n• Worked on building SOTA models for Kaggle competitions.\\n• On NLP related subtasks I was able to break current SOTA scores.\\nResearch Intern\\nSeptember 2020 – October 2020\\nOffNote Labs\\nBangalore, India\\n• Worked on text detection and recognition for low resource Multi Lingual Languages.\\n• Primary Focus was on building OCR models for better text identification for Indic Languages.\\nDeep Learning Intern\\nDecember 2020 – Present\\nStealth Mode Startup\\nGurgaon, India\\n• Working on using Deep Neural networks for Video analytics.\\n• Built models incorporating multi modal fusion of Audio and Video related data for emotion recognition from\\nlive video streams.\\nCOMPETITIONS\\nSIIC Melanoma Detection\\n2020\\nBronze Medal\\nKaggle\\n• I participated in the Kaggle Competition hosted by SIIC to detect the presence of Melanoma A very common\\ntype of Skin Cancer.\\n• With an F1 score of 0.938 I won a bronze medal for my performance.\\nCharacter Recognition in ancient Japanese Paintings\\n2019\\n11th Place\\nNishika\\n• Competed in the Nishika Challenge to identify the different types of people In ancient Japanese paintings.\\n• My model had an F1 score of 0.91 and placed me in the 11th place.\\nSRM Fablab Hackathon\\n2019\\nWinner\\nSRM Fablab\\n• My team proposed an interconnected system to track the productivity and usage of lab equipment.\\n• I built a Facial recognition system based on Facenet in order to prevent unauthorised access.\\n• As an additional feature I built a productivity tracker for tracking if lab equipment was used properly.\\nPROJECTS\\nMechanism of Action Prediction\\n2020\\nUsing the data from Kaggle competition Mechanism of Action. I built a model that can predict the\\nMechanism of action of a drug given the tabular properties of a subject. My model had a log loss score of\\n0.19\\nYoutube 8 million Video Understanding\\n2020\\nUsing the open source Youtube 8 million I built and iterated various attention based paradigms for video\\ncategorisation.\\nThe modifications led to a Global Average Precision score of 0.81, an improvement from the competition LB top score.\\nSKILLS\\nLanguages: English (Full Professional fluency), Tamil (Native), Hindi (Native)\\nProgramming: Python (NumPy, SciPy, Matplotlib, Pandas,Tensorflow, Scikit Learn), Scilab, C/C++, Java\\n',\n",
       " \" \\nNITESH KUMAR \\nContact   : +91-9641575820                               Email id  : niteshkumardmk1@gmail.com                                 LinkedIn : linkedin.com/in/niteshiitkgp \\n  \\nEDUCATION \\n  \\n  Year \\n               Degree/Exam \\n             Institute \\n                 CGPA/ % \\n  2018 \\n               Integrated MSc (5 Yr) in Applied Geology \\n             IIT Kharagpur \\n                 7.45 / 10 \\n  2012 \\n               Intermediate Examination \\n             BSEB, Patna \\n                 71% \\n  2010 \\n               Secondary School Examination \\n             BSEB, Patna \\n                 74.8% \\n   \\nINTERNSHIPS / WORK EXPERIENCE \\n  \\nAssistant Manager (Fraud Prevention), Paytm Payments Bank, Noida                                                                             (August'20 – Current) \\n   • Working on improving the predictive power of existing classification model to identify fraudulent accounts by using different modelling    \\n      techniques, ensembling output from different models and by incorporating new features into the model \\nBusiness Analyst (Pricing & Revenue), OYO Rooms, Gurgaon    \\n(June'18 – July’20) \\n• Successfully tested and implemented 'Inverted Pricing' & 'Gears based Pricing' in pricing algorithm which lead to shift in demand trend   \\n   by 2 days and received ‘Highest Business Impact’ award for the same in Q1'19  \\n• Ideated, executed and concluded various pricing related experiments and shared the results with different stakeholders   \\n• Automated the micro and macro level surges / dips in pricing based on predicted occupancy and other factors of clusters / cities  \\n• Identifying the cluster and city level peaks in advance and setting the base prices accordingly over which dynamic pricing happens  \\n• Created automated dashboards at cluster and city level to track the performance on metrics like occupancy and avg room rate (ARR) \\n  • Worked upon migrating the dynamic pricing algorithm from R to a modular, scalable and more efficient codebase in Python \\nQuantium Analytics, Hyderabad \\nSales Forecasting for New Products \\n(May'17 - July'17) \\n• Retrieved and pre-processed the dataset using Teradata database and identified the new products which were launched \\n• Predicted percentage of pre-period sales of existing products which will be switched to a new product after its launch \\n• Used GBM in R for prediction using features product attributes, pricing, promotion, customer behavior, loyalty, segmentation etc. \\n• Interpreted the model using Partial Dependence Plots and analyzed the marginal effects of features on response variable \\n \\nTrendwise Analytics, Bangalore \\nAudio, Text and Image Analytics \\n (May'16 - July'16) \\n• Introduced two new features to the existing framework, Tone Analyzer to detect different types of tones (joy, anger, fear, sadness \\netc.) present in an audio file and Document Classification to categorize the text retrieved from same audio file using IBM Watson \\n• Developed an API to extract text written on an image using IBM Watson Visual Recognition \\n  \\nAWARDS AND ACHIEVEMENTS \\n     \\n• Received ‘Highest Business Impact’ award for Q1-2019 and ‘Employee of the Month’ award for Aug’19 and May’20 at OYO Rooms  \\n• Winner of McKinsey Analytics Sales Excellence Hackathon organized on Analytics Vidhya (Jan’18) \\n• Winner of AbInBev Data Science Talent Hunt Hackathon organized on Analytics Vidhya (Feb’18) \\n• Winner of data analytics event of Impulse (annual fest of electrical department, IIT Kharagpur) (March'17) \\n• Runner-up in LTFS Data Science FinHack 2, organized by L&T Financial Services on Analytics Vidhya (Jan’20) \\n• Secured a position in Silver category in Shell.AI Hackathon for Sustainable and Affordable Energy (Nov’20) \\n• Currently hold rank 8 out of 0.3 million data science enthusiasts and former student rank 1 on Analytics Vidhya \\n• Secured national rank 1 and worldwide rank 15 in 17th edition of Data Mining Cup 2017 among 200+ teams of 48 countries \\n• Among top 30 finalists out of approx 1000 teams from all over the world in International Data Analysis Olympiad (IDAO) 2018 \\n• Among top 10 finalists after 2 elimination rounds in 4th edition of IndiaHacks Machine Learning Challenge 2017 on HackerEarth  \\n• Ranked 10/1000+ participants in a data science competition organized by National Stock Exchange and ISB on HackerRank  \\n  \\nCOMPETITIONS / HACKATHONS \\n  \\nHackerRank: OLX- Code and the Curious (July'17) \\nAds recommendation  \\n Rank 2/1,1000+ \\n \\n• Analyzed the record of around 7,000 OLX visitors who have visited 18,000+ unique products on sold in 1 month \\n• Recommended 10 ads to buyers whose sellers are likely to be contacted by buyers based on the OLX browsing history of buyers \\nAmerican Express: Analyze This (Oct'16)                                                   Election Poll Prediction                                                        Rank 9/1,500+  \\n  • Predicted the party to which a new set of citizens will vote for based on their previous votes and engagement with the party  \\n  • Used a custom evaluation metric to optimize the Xgboost model performance and fine tune the hyperparameters \\nEXL Analytics: EXL Excellence Quotient (Jan'17 - Feb'17) \\nCustomer Churn Prediction \\n   Top 25/500+ \\n \\n• Developed a customer churn prediction model to identify the customers who are likely to churn from broadband service \\n• Used Bag-of-Words technique in R to derive features from enquiry statement of the customers to use them in model \\n• One of the 2 teams from IIT Kharagpur and among 25 national teams who qualified for 2nd round of the competition \\nZS Young Data Scientist Challenge (July'17) \\nEvents Sequence Prediction \\n Rank 17/500+ \\n \\n• Analyzed the medical history of 3,000 patients who have visited for around 6,000 events (Diagnosis, Procedure or Treatment) in 3 years \\n • Predicted next 10 events reported by patients in order of occurrence based on their 3 years of medical history \\n• Achieved private leaderboard NDCG (Normalized discounted cumulative gain) @K (K = 10) score 0.10 (highest 0.11) \\n    \\nCERTIFICATIONS \\n  \\n• R Programming \\n     • Getting and Cleaning Data \\n      • Practical Machine Learning \\n• The Analytics Edge \\n     • Data Analysis and Statistical Inference \\n      • Introduction to Probability and Data \\n  \\nSKILLS AND EXPERTISE \\n  \\n• R \\n                               • SQL & Hive                                                  • Python                                                     • Tableau                       \\n\",\n",
       " 'Zishan Kamal  \\nData Scientist \\n \\n# 14244, Prestige Lakeside Habitat, Gunjur Village, Varthur Hobli, Bangalore (India) – 560087 \\n \\n\\uf029 +91– 9886726680 / +91– 9632700442  \\n\\uf02a zishan.kamal@gmail.com \\n zishan-kamal  \\nAV rank \\n \\nA data scientist with 6 years of experience with a keen interest to apply machine leanrning and AI techniques \\nto solve complex business problems. Overall, possess a rich experience of 16 years in software development \\nand IT ranging from application development, data analytics to project management. \\nKey strength includes collaborating with stakeholders, subject matter experts, client teams and other functional \\nteams globally to understand business problems, design and execute solutions using various data sources and \\nadvanced analytics approach. \\nProfessional Summary \\n\\uf0a7 \\nExperience as a Data Scientist in building Data Science solutions using Machine Learning, Statistical \\nModeling, Data Mining, Natural Language Processing (NLP), Deep Learning and Data visualization. \\n\\uf0a7 \\nExperience working with large data; interpret and communicate insights and finding from analysis and \\nexperiment toboth technical and non-technical audience in products, service and business. \\n\\uf0a7 \\nImplemented several supervised and unsupervised learning algorithms such as Ensemble methods, \\nLinear / Logistic regression, Regularization techniques, Naïve Bayes, SVM, Clustering techniques, \\nDeep Learning, Natural Language Processing, Transfer learning and Time series forecasting methods. \\n\\uf0a7 \\nPlayed key role in requirement gathering, articulating business needs, formulating the problem \\nstatement, solution design, presenting viable solutions to client and providing technical (AI & ML) \\nguidance to the team to implement these solutions.  \\n\\uf0a7 \\nDemonstrated ability to adapt to multiple domains, dynamic work environments and techno functional \\nroles \\n\\uf0a7 \\nRich experience in all phases of software development life cycle utilizing multiple development \\nmethodologies including Design Patterns, OOD, Agile methodology and Structured Programming. \\n\\uf0a7 \\nAn effective Communicator & Coordinator with Excellent Networking & Interpersonal skills. \\n\\uf0a7 \\nStrong skills in building client relationship, conflict resolution and Strategic planning. \\n\\uf0a7 \\nExcellent communication, interpersonal and leadership skills. \\n\\uf0a7 \\nWide exposure to solving Analytical Problems \\n\\uf0a7 \\nActive participation in data science hackathons and initiaves within and outside the organization. \\nExperience Summary \\n \\n \\n\\uf0a7 \\nWorking as Data Scientist in Wipro Technologies Limited since January 2017. \\n\\uf0a7 \\nWorked as Project Manager in Cognizant Technology Solutions from January 2011 to January 2017. \\n\\uf0a7 \\nWorked as Technology Lead in Infosys Technologies Limited from October 2004 to January 2011. \\n \\n\\\\ \\n\\\\Education \\n \\n \\n\\uf0a7 \\nBusiness Analytics and Intelligence – 2018-2019 (1 Year Classroom Programme) \\nIndian Institute of Management, Bangalore \\n \\n\\uf0a7 \\nBachelor of Engineering (Information Science) – 2000-2004 \\nVisveswaraiah Technological University, Belgaum \\n \\n \\n \\n \\nKey Skills \\n \\n\\uf0a7  Data visualization and Interpretation\\uf020\\n\\uf0a7  Supervised Learning\\uf020\\n\\uf0a7  Data pre-processing and Imputation\\uf020\\no    Linear and Logistic Regression \\n\\uf0a7  Hypothesis Testing\\uf020\\no    K-Nearest Neighbour \\n\\uf0a7  Unsupervised Learning\\uf020\\no    Support Vector Machine \\n   o    Principal Component Analysis \\no    Classification and Regression trees \\n   o    Clustering \\no    Random Forest  \\n\\uf0a7  Time series forecasting \\uf020\\no    Gradient Boosting Machines  \\n\\uf0a7  Deep Learning – CNN, RNN, LSTM,     \\nAutoencoder\\uf020\\n\\uf0a7  Natural Language Processing \\uf020\\n \\nKey Project highlights  \\n \\nExtraction of attributes from various contract documents  \\n \\nThe project is aimed at legal departments who spend substantial manual effort (~5 days) in reading heavy \\nlegal documents of sizes ranging from 50 to 500 pages and then reviewing key attributes present in the \\ndocument. AI based tool aims to reduce this manual effort by extracting large number (50 to 500) of key \\nattributes and providing a displayof the document where portions are mapped to each of the attribute being \\ndiscussed. The solution uses Sequence tagger with bi-direction LSTM + CRF with GloVe word embeddings. \\n \\nForecast warranty claims of one of the largest Two-Wheeler manufacturers \\n \\nProblem faced by client was that Forecasting of warrantly claims was based on qualitative technique and was \\nnot data driven, thereby resulting in long service times for customer due to stock out of parts coming under \\nwarrantly claims. \\nThe project aimed to develop a data driven forecasting model based on historical timeseries data that is \\navailable and enable the organization to have an accurate view of warranty claims. The organization aimed to \\nimprove its customer service and reduce its inventory carrying costs. Several traditional timeseries forecasting \\ntechniques like ARMA, ARIMA, SARIMA and Gradient boosting trees with date related derived features \\nwere tried but LSTM model gave the best result. \\n \\nDetection of anomaly and fraudulent transactions \\n \\nFraud risk monitoring (FRM) division used to perform anomaly and fraud detection based on a set of defined \\nrules. Rules needed to be tweaked frequently and entire detection was done manually. Solution was designed \\nto automate the detection with the help of ML techniques. This resulted in huge effort saving and getting better \\nresults. Solution was implemented using Autoencoders with 3 layers of standard feed forward encoder and \\ndecoder. \\n \\nClassification of emails received by marketing team \\n \\nMarketing team in a retail sector recieved several emails related to query, complaints, promotions, some \\nunrelated etc. They had to go through all the emails, identify the type of email and redirect it to relevant \\nemailing group to take necessary steps and respond to the email. A system was put in place to classify the \\nemails. Final solution was a BERT model to perform multiclass classification. \\n \\nCapturing details from Invoices \\n \\nBack office team receives several travel airline invoices and needs to capture various details for accounting \\nand validation purposes. Capturing of details from invoices were manual in nature and hence resulted in lots \\nof efforts. Invoices were in PDF format with mix of text and image (scanned copy) format. A solution was \\nimplemented to automate the capture of key attributes along with the model’s confidence level so that manual \\nreview can only be done where model’s confidence was low. This project drastically reduced the human effort \\nand extra burden from BO team especially during their month end and quarter end processing. \\n \\nCross selling initiative of a bank \\n \\nBusiness wanted a mechanism to identify list of customers who were most likely to be the target of this \\nCross-Sell initiative. Customer spending patterns, their demographical information and transaction history \\nwere used to identify potential customers.  \\n \\nCustomer churn analytics for a bank \\n \\nBank wanted to identify customers likely to churn balances in the next quarter by atleast 50% vis-à-vis current \\nquarter. Customers information such as age, gender, demographics along with their assets, liabilities and \\ntransactional were analysed to predict the prop ensity to churn for each customer. Final solution was a model \\ntrained with SMOTE oversampling with Gradient Boosting trees. \\n',\n",
       " 'Rooban Sappani\\n18PD30\\nFather’s name\\nGender\\nDate of Birth\\nLanguages known\\nEmail\\nMobile\\nSappani Muthiah\\nMale\\n21st December 2000\\nEnglish, Tamil\\n2000rooban@gmail.com\\n+91-97904-56377\\nPermanent Address\\nA 1/6, Baskara Apts,\\nNo. 1 Tolgate,\\nTiruchirappalli,\\nTamil Nadu,\\n621216.\\nOBJECTIVE\\nTo obtain a position as a Student Intern for a period of six months from May\\n2021 to November 2021.\\nACADEMIC QUALIFICATION\\nCurrently pursuing 3rd year of 5-year Integrated M.Sc. Data Science at the\\nDepartment of Applied Mathematics and Computational Sciences at PSG College of Technology.\\nSKILL SET\\nLanguages\\nC++, Python, C, R\\nPlatform\\nWindows, Linux\\nTools\\nTensorflow, OpenCV, GitHub, Oracle\\nAREAS OF INTEREST\\n●\\nSupervised and Unsupervised Learning\\n●\\nData Structures and Algorithms\\n●\\nDeep Learning\\n●\\nComputer Vision\\nACADEMIC RECORD\\nCourse\\nInstitution\\nBoard/University\\nCompletion By\\nMarks\\nM.Sc\\nPSG College of Technology,\\nCoimbatore\\nAnna University\\n2023\\n7.86\\nX\\nIndian School of Bahrain\\nCBSE\\n2016\\n94.00%\\nXII\\nThe Velammal International\\nSchool\\nCBSE\\n2018\\n92.80%\\nINDUSTRY BASED PROJECT EXPERIENCE\\nNIT Trichy - Machine Learning Research Intern\\nResearch and development of an Automated Crime News Classification model. Worked under\\nDr. A. Santhanavijayan as a deep learning intern to work on the development of the research\\npaper, \"A Knowledge Centric Hybridized Approach for Crime Classification incorporating Deep\\nBiLSTM Neural Network\", which has been submitted(Under Review) to Multimedia Tools and\\nApplications, Springer.\\nNON-ACADEMIC PROJECTS\\n●\\nReal Time object detection, using the YOLO v3 kernel data on kaggle, an automated\\nobject detecting model was built on keras. Darknet 53 was used for feature extraction and\\nthe YOLO v3 algorithm was used for object detection. With the help of OpenCV, a real\\ntime system was built on python to detect upto 257 different objects through videos.\\n●\\nAll about Faces, A real time system that works on a picture of a person’s face to detect the\\nlocation of the face(using Haar Cascade), presence of a mask, the gender and the\\nethnicity of the person. Dataset: UTK-face, Tools used: Kaggle, CNN (Transfer learning,\\nVGG16), keras and opencv.\\n●\\nGoogle Stocks Predict, an application built using Google Data, to predict stock prices of\\nGoogle, using a LSTM Neural Network. The data being structured, was pre-processed\\nusing Min-Max scaling.\\nACADEMIC PROJECTS\\n●\\nGive me a Movie, Content based recommendation system implemented in C++ with the\\nIMDB dataset. Data structures: Bloom filters for IR, BK trees for spell checking, linked list.\\nAlgorithms: Dynamic levenshtein algorithm for search suggestion and recommendation,\\nFNV and Murmurhash3 to construct the bloom filters.\\n●\\nFake News Detector, an application built on Python to automatically classify news articles\\ninto\\nfalse\\nor\\ngenuine.\\nTF-IDF vectorizer was used for word embeddings and\\nPassive-Aggressive classifier was the algorithm used.\\n●\\nForest fire Analysis, Implemented in python for analysing the past 5 years data from\\nMontesinho park, Portugal and using various Machine Learning and visualizations, have\\nefficiently predicted future locations and degree of fire in the Park.\\nEXTRA-CURRICULAR ACTIVITIES AND ACHIEVEMENTS\\n●\\nParticipated in the smart internz IBM Hackathon 2020(creating a chatbot for automotive\\nresume selection) and came to the final round.\\n●\\nActive Machine Learning, Deep Learning and Python freelancer on freelancer.in.\\n●\\nCompleted the Deep Learning Specialization consisting of 5 courses organized by Andrew\\nng on Coursera.\\n●\\nMember of the Coursera Professional Community(Through IBM Professional Data Science\\nCourse).\\n●\\nPiano Practicals and Theory, ABRSM levels 1,2,3,4,5 passed with distinction.\\nDECLARATION\\nI, Rooban Sappani, do hereby confirm that the information given above is true\\nto the best of my knowledge.\\nPlace: Coimbatore\\nDate : 03/05/2021\\n(Rooban Sappani)\\n',\n",
       " 'Wayal Rushikesh\\n|LinkedIn|:Wayal Rushikesh\\nwayalrushi5@gmail.com | 8597165877\\nEDUCATION\\nIIT KHARAGPUR\\nDUAL DEGREE\\nELECTRONICS AND\\nELECTRICAL COMMUNICATIONS\\nENGINEERING(AIR-690)\\n2017-2021\\nCGPA: 8.76/10\\nAKLANK PUBLIC SCHOOL\\nCLASS XII\\n2016-2017\\nPercentage: 84.6\\nPODAR INT. SCHOOL\\nCLASS X\\nNTSE Qualified\\n2014-2015\\nCGPA : 10/10\\nSKILLS\\nPROGRAMMING\\n• Python\\n• C and C++\\nLIBRARIES\\n• Pytorch\\n• Keras\\n• Scikit learn\\n• Numpy\\n• Pandas\\nUTILITIES\\n• Tableau\\n• Solid Works\\nCOURSEWORK\\n•Machine Learning\\n•Neural Networks\\nand its Application\\n•Probability and Stochastic\\n•Mathematical Methods\\n•Algorithms(Theory and Lab)\\n•Engineering Mathematics\\n•Programming and Data Structures\\n•Matrix Algebra\\nEXTRA-CURRICULAR\\nNational Sports Organisation\\n•Taught the students of primary school\\n•Part of the NSO - Yoga\\nInter Hall Events\\n•Data Analytics\\n•Open-Soft\\nINTERNSHIPS\\nHSBC\\n| JUNIOR ANALYST\\n•Study of order-book resilience.\\n•Developed an algorithm to classify the aggressiveness of orders.\\n•Quantiﬁed the changes in behaviours of liquidity measures around\\naggressive orders.\\n•Quantiﬁed the differences in market impact left by various orders and in\\nvarious markets.\\n•Extended the created mechanism to ﬁnd optimal combination of aggressive\\norders to give low cost and minimal market impact.\\nCOMPETITIONS\\nINSTANT GRATIFICATION | KAGGLE\\n•Binary prediction challenge having a puzzling data-set.\\n•The activation of certain columns was dependent on certain factors and\\nhugely affected the prediction\\n•Found out the best model and hence data distribution type. Rank:244\\nML/AI CHALLENGE\\n| FLIPKART GRID – TE[A]CH THE MACHINES\\n•Trained A transfer learning algorithm to predict the bounding box of objects\\nNETAPP DATA CHALLENGE | KSHITIJ 2019, IIT KHARAGPUR\\n•Used Tf-idf and word2vec to build a news classiﬁer based on headlines and\\nshort description\\n•Stood 2nd among more than 150 teams, held at Data Analytics Event.\\nINTEL SCENE CLASSIFICATION CHALLENGE | ANALYTICS VIDHYA\\n•Used Fastai libraby (resnet152) to build image classiﬁer\\n•Final rank 82 with accuracy 0.94\\nMOOCS\\nMACHINE LEARNING A-Z™\\n•Study of regression, CART models, Text parsing Regression trees,\\nClustering, Data Visualization and its implementation.\\nDEEP LEARNING SPECIALIZATION\\n•Study of CNN, RCNN for image processing and LSTM for text processing\\nMACHINE LEARNING(CS239) | OFFERED BY STANFORD UNIVERSITY\\n•Study of CNN,RCNN and ResNet\\nGROUPS\\nKHARAGPUR DATA ANALYTICS GROUP | IIT KHARAGPUR\\n• Conducting of data antics camps.\\nPOSITION OF RESPONSIBILITY\\nCAPTAIN : DATA ANALYTICS | MEGHNAD SAHA HALL OF RESIDENCE\\n• Headed the gold wining team in Open IIT Data Analytics competition.\\n• Trained new batch in Data Science\\n• Responsibility of conducting meetings on regular intervals for preparation\\nof Inter hall event competitions.\\n1\\n',\n",
       " 'SAI SUCHITH MAHAJAN\\nIndian Institute of Technology Palakkad\\n� mahajan.saisuchith@gmail.com\\n� +91-8329867394\\n� Palakkad, India\\n� www.linkedin.com/in/sai-suchith-mahajan/\\n� https://github.com/Saisuchith\\nwww.kaggle.com/suchith0312\\nEDUCATION\\nBachelor of Technology (Electrical\\nEngineering)\\nIndian Institute of Technology Palakkad\\n� 2020\\n� Palakkad\\n• CGPA: 8.16\\nCOURSES\\n• Machine learning, Deep learning\\n• Reinforcement learning, Numerical Analysis\\n• Linear Algebra, Convex Optimization\\n• Probabilistic Systems Analysis\\nTECHNICAL SKILLS\\n• C, C++, Python\\n• PyTorch, Keras, Scikit learn\\n• Matlab, MongoDB\\nACHIEVEMENTS\\n• Kaggle Competition Master Rank : 1151\\n• Secured a rank of 9(Gold Medal) out of 7198\\nparticipants in Kaggle Home Credit Default\\nRisk competition\\n• Secured a rank of 4 in online and 5 in ofﬂine\\nBrainwaves 2019\\n• Recipient of KVPY Fellowship Award 2015\\n• Placed National wise Top 1% in NSEC\\n(2015-2016)\\nEXPERIENCE\\nAssociate Data Scientist\\nSociete Generale Global Solution Centre\\n� September 2020-Present\\n� Bengaluru, India\\n• Developed a ML, heuristic based pipeline for automating client\\non-boarding for a payment app.\\n• Developed a Deep Learning pipeline to automatically extract\\nthe information of UBO’s from RBE documents.\\nSubject Matter Expert\\nHackerEarth\\n� April 2019 – May 2020\\n� Bengaluru(Remote)\\n• Creating new question content related to ML/Data Science for\\nHackerEarth’s question Library\\nData Science Intern\\nSigTuple Technologies Pvt. Ltd.\\n� May 2019 – July 2019\\n� Bengaluru, India\\n• Developed a generic automated pipeline for evaluation of 4\\ndiagnostic products and generated intelligent insights\\nEDUCATION\\nBachelor of Technology (Electrical\\nEngineering)\\nIndian Institute of Technology Palakkad\\n� 2020\\n� Palakkad\\n• CGPA: 8.16\\nCOURSES\\n• Machine learning, Deep learning\\n• Reinforcement learning, Numerical Analysis\\n• Linear Algebra, Convex Optimization\\n• Probabilistic Systems Analysis\\nTECHNICAL SKILLS\\n• C, C++, Python\\n• PyTorch, Keras, Scikit learn\\n• Matlab, MongoDB\\nACHIEVEMENTS\\n• Kaggle Competition Master Rank : 1151\\n• Secured a rank of 9(Gold Medal) out of 7198\\nparticipants in Kaggle Home Credit Default\\nRisk competition\\n• Secured a rank of 4 in online and 5 in ofﬂine\\nBrainwaves 2019\\n• Recipient of KVPY Fellowship Award 2015\\n• Placed National wise Top 1% in NSEC\\n(2015-2016)\\nPROJECTS\\nInferring Wireless Channel Gains\\nPredicting channel gains at one frequency given at\\nanother frequency\\n• Deep neural networks are used to approximate the mapping\\nbetween uplink channel gains and downlink channel gains. This\\nmapping will help eliminating the downlink training and\\nfeedback overhead in co-located/distributed FDD massive\\nMIMO systems.\\nTweet Sentiment Extraction\\nExtract phrases from the tweet\\n• Challenge is to look at the sentiment and tweet and extract the\\nwords or phrase that best supports the sentiment. Trained\\nBERT, RoBERTa model for extracting the phrases.\\nJigsaw Unintended Bias in Toxicity Classiﬁcation\\nDetect Toxicity in comments\\n• Trained LSTM and BERT models to detect toxic comments and\\nminimizes unintended bias with respect to mentions of\\nidentities. Was in Top 5 % of competition in ﬁnal results.\\n',\n",
       " 'Saurav Mishra  \\n \\n \\n \\nDepartment of Biological Sciences and Bioengineering                                                                         Phone: (+91)-9435838761  \\nMinor: Machine Learning & Applications\\u200b                                                                             Email: \\u200bsauravmishra554@gmail.com \\n \\nE\\u200bDUCATIONAL \\u200bQ\\u200bUALIFICATIONS \\nP\\u200bROJECTS \\nLTFS Data Science FinHack 3\\u200b \\u200b(Analytics Vidhya)                       \\u200b                                                                                                                     (Jan’21-Feb’21) \\nBasic Needs Basic Rights Kenya- Tech4 Mental Health\\u200b \\u200b(Zindi NLP Competition)\\u200b                                                                               (Aug’20-Sep’20) \\nPUBG Final Score Prediction (Data Mining) \\u200b(Mentor: Prof. Arnab Bhattacharya)                                                                                      (Jul’18-Nov’18) \\nS\\u200bCHOLASTIC \\u200bA\\u200bCHIEVEMENTS \\n●\\nSecured \\u200b3rd\\u200b position among \\u200b750+\\u200b participants in House Price Prediction hackathon organized by Machine Hack                     \\u200b(2020) \\n●\\nObtained \\u200b12th \\u200brank among \\u200b600+\\u200b participants in Cross Sell Prediction hackathon organized by Analytics VIdhya                        \\u200b(2020) \\n●\\nAll India Rank- 4856\\u200b in \\u200bJEE Advanced 2016\\u200b out of 1.5 lakh successful candidates of JEE Mains 2016\\n                                   \\u200b(2016) \\n●\\nAchieved \\u200b98.10 percentile \\u200bin \\u200bJEE Mains Examination 2016 \\u200bamongst 12 lakh students                                                        \\u200b              \\u200b(2016) \\nT\\u200bECHNICAL \\u200bS\\u200bKILLS \\n●\\nLibraries                                                                        \\u200bNumpy, Pandas, Scikit Learn, Matplotlib, Seaborn, Nltk \\n●\\nProgramming Languages and Software               \\u200bC, C++, Python, SQL , MS Office, GitHub \\nR\\u200bELEVANT \\u200bC\\u200bOURSES \\n  \\nData Mining                                             Introduction to Machine Learning                                                        Data Structures and Algorithm \\n \\nEconometrics                                          Advanced Statistical Methods for Business Analytics                        Probability and Statistics  \\nP\\u200bOSITIONS \\u200bO\\u200bF \\u200bR\\u200bESPONSIBILITY \\nManager, Events, Udghosh’18 \\u200b(Annual Sports Festival IIT Kanpur)\\u200b                                                                           \\u200b                             \\u200b(Jun’18- Oct’18) \\n●\\nRecruited and led a \\u200b2-tier\\u200b team comprising of about \\u200b18 \\u200bmembers under various positions and divisions \\n●\\nHandled a team of \\u200b100 Campus Ambassadors \\u200bfor publicizing the festival in various colleges across the nation \\n●\\nResponsible for smooth conduction of \\u200bChess, Carrom, and Mr.Udghosh events \\u200bwith around \\u200b150+\\u200b participants \\nManagement Executive, Josh’18 \\u200b(Annual Sports Festival  IIT Kanpur) \\u200b                                                                                              \\u200b(Dec’17- Jan’18) \\n●\\nSpearheaded a team of \\u200b5 members \\u200bto conduct \\u200b17 major events \\u200bof all sports including Football, Cricket, Hockey, Athletics, etc. \\n●\\nTended to the needs of \\u200b1500+\\u200b participants for smooth conduction of events, double the previous year’s count of \\u200b800\\u200b students \\nE\\u200bXTRA-\\u200bC\\u200bURRICULAR \\u200bA\\u200bCTIVITIES \\n●\\nBagged \\u200b5 gold\\u200b and \\u200b1 silver\\u200b medal in 4*100m relay, 4*400m relay athletics event in various national level sports competitions \\n●\\nParticipated in \\u200b400m, 4*400m Relay\\u200b Athletics Event in \\u200bInter-IIT Sports Meet’18 \\u200borganized by IIT Guwahati \\n●\\nDramatics:\\u200b Performed \\u200b‘Sab Chalta hai’ \\u200bnukkad natak\\u200b \\u200b(street play) in \\u200bDramatics Evening 1 \\u200bin front of a crowd of\\u200b 300+ \\u200bpeople \\n●\\nBuilt a \\u200bSemi-Autonomous Robot\\u200b which was capable of moving objects in an arena, in a team of \\u200b5 members\\u200b in \\u200bTakneek’16 \\nYear \\nDegree/Certificate \\nInstitute/School \\nCGPA/Percentage \\n2016-2020 \\nB.Tech \\nIndian Institute of Technology, Kanpur \\n6.9/10 \\n2015 \\nXII-CBSE \\nHindustani Kendriya Vidyalaya, Guwahati \\n84.8% \\n2013 \\nX-SEBA \\nY.W.C.A. English School, Guwahati \\n83% \\nObjective \\n●\\nTo classify the time period in which a loan applicant would require a top-up on the existing loan \\n Approach \\n●\\nAnalyzed both Bureau data, which contained information about all loans of the applicants and, Application \\ndata which contained LTFS loan specific data to identify significant trends \\n●\\nGenerated some innovative features by rolling up the bureau data to application level and merged these \\nfeatures to the Application data for further processing and model building \\n●\\nPerformed \\u200badversarial validation\\u200b and removed some features to get more reliable predictions  \\n●\\nApplied \\u200bStratified K-Fold\\u200b cross validation and tuned the model hyper-parameters using \\u200bGrid Search  \\nResult \\n●\\nClassified the loan applications using \\u200b5-fold LGBM\\u200b and secured \\u200b34th\\u200b rank among \\u200b500+\\u200b teams \\nObjective \\n●\\nTo classify text from university students in Kenya into four different mental health categories \\n Approach \\n●\\nPerformed text pre-processing by removing punctuations and stopwords  \\n●\\nNormalized the words by \\u200bstemming \\u200band converted text into numerical features like \\u200bbag-of-word\\u200b and \\u200btf-idf  \\n●\\nApplied various ML models like \\u200bLogistic Regression\\u200b, \\u200bNaive Bayes\\u200b, and \\u200bLGBM \\u200bto predict class probabilities \\n●\\nUsed pre-trained word embeddings to train \\u200bRNN\\u200b based models like \\u200bLSTM \\nResult \\n●\\nPredicted the class probability using LGBM on tf-idf features as it was the best performing model. \\nObjective \\n●\\nTo predict final win place percentile from in-game statistics and initial player ratings \\n●\\nTo discover some of the best ways to survive and increase the chances of winning \\n Approach \\n●\\nCollected 5500 games worth of anonymized player data consisting of over 30,000 observations and 25 \\nattributes and performed an extensive \\u200bExploratory Data Analysis \\u200bon it \\n●\\nPre-processed the data by removing \\u200boutliers\\u200b, \\u200bimputing\\u200b the missing values, and removing redundant \\nfeatures found through \\u200bcorrelation matrix\\u200b and feature importance \\n●\\nApplied various ML models like \\u200bLinear\\u200b \\u200bRegression, Decision Tree, XGBoost\\u200b to predict win place percentile \\nResult \\n●\\nXGBoost was the most effective algorithm on this dataset, giving a Mean Absolute Error of 0.07 \\n',\n",
       " \"Shyam Sundar\\nMachine Learning Engineer\\nshyamnambiraja01@gmail.com\\n9789954424\\nhttps://www.linkedin.com/in/shyam-sundar4801/\\nhttps://github.com/Shyam4801\\nhttps://shyamnambiraja01.medium.com/\\nProfile\\nMachine Learning engineer with a demonstrated experience in Computer Vision research for diverse set of business use \\ncases including optimizing and deploying models on Edge platforms\\nProfessional Experience\\nMachine Learning Engineer, e-con systems\\n01/2020 – present | Chennai, India\\n- \\nDeveloped a Biometric application based on Face recognition \\nincluding model evaluation , performance benchmarking\\n- \\nPerformance evaluation of State of the art Anomaly detection \\ntechniques focusing on manufacturing use cases like Surface defects , \\nThreaded rod \\n- \\nMissing Component detection of manually annotated PCB board \\nimages using object detection\\n- \\nDeveloped an SDK for Wire rope anomaly detection based on a \\nclassification model\\n- \\nDeveloped a Smart cart application using a classification model \\nfeaturing on device fine tuning\\n- Evaluated different Edge platforms and their frameworks like Intel's \\nOpenVino, Nvidia , Xilinx VitisAI\\n- \\nPlatform-aware Neural Network adaptation\\n- \\nContributed technical blogs to promote our existing products and \\nsolutions\\nProjects\\nPrice optimization\\nChoosing the right pricing strategy using a reliable price response \\nfunction as well as choosing an optimum price under uncertainty \\nthereby maximizing profit\\nProduct recommendation\\nProvide personalized recommendations to customers analyzing their \\nbehavior and thereby reducing customer retention rate \\nSupply chain optimization\\nAnalyzing the complexity involved in balancing the operating costs \\namong various players in the supply chain , thereby forecasting the \\ndemand so as to maximize the profit\\nExposure / Skills\\n• Deep Learning • Tensorflow • Pytorch  \\n• Python  • MySQL • Tableau  • \\nStatistical Testing •  Machine Learning \\n(Supervised and Unsupervised) •  \\nEnsemble Techniques •  Time Series \\nforecasting (Univariate)\\nEducation\\nPGP DATA SCIENCE AND \\nENGINEERING, Great learning, \\nGreat Lakes Institute of \\nManagement\\n03/2019 – 10/2019 | Chennai, India\\nBachelor's Degree in ECE, College \\nof Engineering Guindy, Anna \\nuniversity\\nChennai, India\\nHigh school, \\nGRT Mahalakshmi Vidyalaya\\nCertificates\\nReinforcement Learning Specialization\\nIntroduction to TensorFlow for \\nArtificial Intelligence, Machine \\nLearning, and Deep Learning by \\ndeeplearning.ai on Coursera.\\nInterests\\nTech enthusiast , Pencil Sketching , \\nFitness , Badminton\\n\",\n",
       " 'Sri Chandra Duddu\\nManager, Axis Bank AICoE\\nData Scientist with ~2 years of work experience leveraging Predictive Data Modeling and Artiﬁcial\\nIntelligence products to deliver insights and implement action-oriented solutions to complex business\\nproblems. Vivid expertise in variety of Machine Learning problems specially in Agriculture and Banking\\ndomain. Computer Vision and Natural Language Understanding (NLU) are special areas of interest.\\nduddusrichandra@gmail.com\\n+91 9933999147\\nBangalore, India\\nlinkedin.com/in/sri-chandra-duddu\\nWORK EXPERIENCE\\nData Scientist\\nAxis Bank Business Intelligence Unit (BIU)\\n07/2019 - Present, \\nBangalore, India\\nProﬁcient with Python. Hands on Experience in creating, tuning and\\nmanaging the AI product roadmap\\nDeveloped a RASA powered Search for MIS capable\\nChatbot which could exploit the use of Knowledge Graphs\\nfor retrieving MIS directly for business stakeholders and the\\nuse of Buttons to achieve controlled, assisted conversation\\nfor employees to resolve incentive related queries\\nEmployed Positive-Unlabeled (PU) Learning based\\napproach for Hidden Aﬄuent Identiﬁcation using\\nthoughtfully designed Hidden variables like Merchant\\nTransactions, Bureau trade-lines and Customer Investments\\nFacilitated the pilot launch of a third-party delivered\\nRobotic Call Centre project for the organization from end\\nto end design, development and implementation\\nDesigned a Savings Account Feature Revamp Proposition\\nto complement minimum balance requirement criteria with\\ncustomer behavior metrics using Regression analysis\\nAccomplished SARIMAX based time series forecasting\\napproach to Savings Accounts Fee Budgeting capable of\\nEvent Detection through Intervention Analysis\\nExploited the inherent working of Bi-directional Encoder\\nRepresentations from Transformers (BERT) Deep Learning\\narchitecture for extracting contextual word embeddings for\\nText Classiﬁcation and Clustering\\nEDUCATION\\nAgricultural Systems and Management -\\nM.Tech (Dual Degree)\\nIndian Institute of Technology, Kharagpur\\n2018 - 2019, \\n8.50/10\\nAgricultural Systems\\nModeling\\nAgri-Project Cash Flow\\nAnalysis & Finance\\nDigital Soil Mapping\\nDeep Learning\\nFoundations &\\nApplications\\nAgricultural and Food Engineering - B.Tech\\nIndian Institute of Technology, Kharagpur\\n2014 - 2018, \\n7.50/10\\nLogistics Systems\\nProduct Development\\nManagement &\\nProductivity\\nMachine Learning\\nSKILLS & EXPERTISE\\nPython\\nSAS\\nSQL\\nR\\nAWS\\nFlask\\nDocker\\nGoogle Earth Engine\\nINTERNSHIPS & PROJECTS\\nDevelopment of FAQ Bot to service Axis Bank Products\\nSuccessfully built a Retrieval based Chatbot framework in Flask\\nfrom scratch in Python assessed against Axis Aha! and was awarded\\nthe Pre - placement Oﬀer\\nAchieved 96% Reliability by extracting 3000 Latent Semantic\\nAnalysis components and classifying Query Intents using Multi-\\nlayer Perceptron\\nReal-time Paddy crop health monitoring and\\nmanagement using Drone for diseases Detection\\nAchieved a 94.1% Mean Average Precision (MAP) in predicting the\\npaddy plant diseases for an Intersection over Union (IoU) > 0.5\\ncriteria\\nEmployed Faster-RCNN meta-architecture with a ResNet 101\\nfeature extractor in predicting Detection Results that aﬀected\\nPaddy plant diseases\\nSuccessfully designed an Alert System which communicates with\\nthe farmer of the health condition of the Paddy crop once\\nSurveilled by the Drone\\nEstimation of Soil Arsenic Content using Partial Least\\nSquares (PLS) Regression\\nSpectral Reﬂectance data in 350-2000 nm from Diﬀuse\\nReﬂectance Spectroscopy were considered for estimating Soil\\nArsenic content using Partial Least Squares (PLS) Regression\\nDeveloped PLS model achieved adjusted R-square value of 0.94\\nProduction Estimation using Regression Splines\\nDiameter at Breast Height (DBH) and Normalized Diﬀerence\\nVegetative Index (NDVI) were considered for estimating Biomass\\nusing B-Splines approach\\nAchieved adjusted R2 value of 0.98 against the Lowess Curve ﬁt\\nmodel with adjusted R2 value of 0.82\\nHACKATHONS & AWARDS\\nMcKinsey Young Optimization Crackerjack Contest\\n(Rank 5)\\nDemand Forecasting and Optimization was the crux of the Hackathon.\\nARIMA based demand forecasting with constraint optimization helped\\nme achieve Rank 5 on the Leaderboard\\nHackerEarth Deep Learning Challenge (Rank 17)\\nResnet50 Feature Extractor and K-means based Image Clustering to\\nclassify toys from baby products and Tesseract based OCR to extract\\nBrand names got me Rank 17 on the leaderboard\\nINTERESTS\\nML Hackathons\\nCricket\\nBadminton\\nMusic\\nAchievements/Tasks\\nRelevant Courses\\nRelevant Courses\\n',\n",
       " 'KUMAR SUBHAM\\nBachelor of Technology\\nNational Institute of Technology, Raipur\\n\\uffff krsubham48@gmail.com\\n\\uffff +91 7766 932528\\n\\uffff linkedin.com/in/krsubham48\\n\\uffff github.com/krsubham48\\n\\uffff Gurugram, India\\nPUBLICATIONS\\n• Paper published at IEEE ICCCIS 2019 titled \"SegNet – based\\nCorpus Callosum segmentation for brain Magnetic Resonance\\nImages (MRI)\"\\n• Paper published at IEEE ICECCT 2019 titled \"Corpus\\nCallosum Segmentation from Brain MRI and its possible\\napplication in detection of diseases\"\\nPROJECTS\\nMachine Translation\\n• Experiments with Encoder-Decoder and Attention Network\\narchitecture to translate French to English\\nCorpus Callosum Segmentation\\n• U-Net and SegNet based architectures for segmentation,\\nimage processing techniques for augmentation and\\npreprocessing\\n• Test set dice coe�cient of 0.967, best in domain\\n• Extensive parametric and semantic analysis on results\\nHuman Safety using Deep Learning\\n• CNN models for classi�cation, detection, segmentation and\\ninstance segmentation of humans in an image\\n• Experimentation on building a crowd-count system based on\\nCSRNet Model architecture\\nQuestion-Answer Mapping\\n• Experiments with 1-D CNN, BiLSTM and use of\\nTransformer Network to implement an Attention Model\\nto classify most relevant answer\\nText Prediction and Classi�cation\\n• LSTM based sequence models for next-word prediction and\\nexperiments with Count Vectorizer, Bag of Words, TF-IDF\\nand similar approaches for classi�cation using SVM\\nExploratory Data Analysis\\n• Classi�cation, Unsupervised Clustering and Continuous\\nvalue Prediction on common datasets\\n• Implementation of Machine Learning algorithms and\\ntechniques from scratch to understand underlying\\nmathematics on dummy data\\nEDUCATION\\nB.Tech, Electronics and Telecomm, 8.01/10.0\\nNational Institute of Technology, Raipur\\n\\uffff July 2015 – May 2019\\nAISSCE, 91.00 %\\nLoyola High School, Patna\\n\\uffff April 2012 – May 2014\\nEXPERIENCE\\nSoftware Engineer I\\nSnapdeal Private Limited\\n\\uffff Jan 2020 - present\\n\\uffff Gurugram, India\\n• Use of Deep NLP on user chat-messages to perform Intent\\nClassi�cation using LSTM based models for automatic\\nnavigation in chatbot, supported languages include English\\nand Hinglish\\n• Use of Sequence Models to predict user’s click behaviour\\nusing historical click-stream data, used for next-click and\\nsimilar product prediction\\n• Experimentation on Attention Network based\\nRecommendation System using Transformer Model variants\\n• Developed a Background Removal System based on U-Net\\nmodel architecture for product Image Segmentation from\\nnoisy images\\n• Development of Visual Search feature model by performing\\nmulti-class classi�cation on product catalog tree using CNN\\nbased architecture\\n• Extensive exploration and experimentation on development\\nof Product Graph - Knowledge Graph focused on Product\\nCatalog\\n• Core-development of Chat-Engine and Support-Panel project\\nProject Engineer\\nWipro Limited\\n\\uffff Aug 2019 - Dec 2019\\n\\uffff Bengaluru, India\\n• Working as a Python Developer in Data Analytics and AI\\ndomain\\nData Science Intern\\nFractal Analytics\\n\\uffff May 2018 - July 2018\\n\\uffff Mumbai, India\\n• Responsible for development of �rst Data-to-Text System\\nfor cuddle.ai, a subsidiary of Fractal Analytics\\n• Use of Statistics and Machine Learning algorithms for Time-\\nSeries data analysis and classical NLG to translate captured\\ninformation into human-readable summary\\nACHIEVEMENTS\\n• Award for Leading ML E�orts at Snapdeal\\n• Selected among Top 10% entrants in Microsoft AI Challenge\\nIndia 2018 and quali�ed for Final Round\\n• Finalist at Smart India Hackathon 2018, organised by MHRD\\n• First Prize in Vigyaan 2017, a National Level Project Exhibition\\nat NIT Raipur\\nSKILLS\\n• C, C++, Java, Python, Data Structures, Algorithms\\n• TensorFlow, Keras, PyTorch, Scikit-Learn\\n• Computer Vision, NLP, Attention Networks\\n• Data Science, Data Analysis\\n• Probability, Statistics, Linear Algebra\\n',\n",
       " \"SUNIL HULE\\n \\nHow to reach me:\\nCell: \\nEmail:\\n+917709308309\\nsunilhule4444@gmail.com\\nSkills\\nCoding Language:\\n Python\\nSKN Sinhgad Institute of Technology\\nTools:\\nPursuing B.E. in Computer Science\\nPersonal Profile\\nI am a final year computer science engineering\\nstudent. Extremely motivated to constantly\\ndevelop my skills.\\nI am a data science aspirant. I have experience of\\n15+ hackathons and secured podium in 4 of them.\\nAchievements\\nPlaying Chess\\nSpeed Cubing\\nBadminton\\nNumPy, Pandas, Matplotlib,\\nseaborn, scikit-learn.\\nData Analysis\\nMachine Learning\\nPredictive Modelling\\nGithub:\\nhttps://github.com/Sunil-Hule\\nLinkedIn:\\nhttps://www.linkedin.com/in/su\\nnil-hule-838418184/\\nProjects\\nAnomaly Detection  in Oil Pipeline:\\nhttps://github.com/Sunil-Hule/Anomaly-Detection-In-Oil-Pipeline\\nCross Sale Prediction (Rank 68 on Analytics Vidhya):\\nhttps://github.com/Sunil-Hule/Big-Mart-Sales-Prediction-Analytics-Vidhya\\nTinder Millennial Match (Rank 1 on DPhi):\\nhttps://github.com/Sunil-Hule/DPhi-Tinder-Millennial-Match\\nMerchandise-Popularity-Prediction-Challenge (Rank 19 on MachineHack)\\nhttps://github.com/Sunil-Hule/Merchandise-Popularity-Prediction-Challenge\\nHouse Price Prediction Challenge ( Rank 2 on MachineHack): \\nhttps://github.com/Sunil-Hule/House-Price-Prediction-Challenge---Machine-Hack\\nhttps://github.com/Sunil-Hule/Cross-Sell-Prediction\\nBig Mart Sales Prediction: \\nPredict-The-Cost-of-Sculpture (Rank 1 on HackerEarth):\\nhttps://github.com/Sunil-Hule/HackerEarth-Predict-The-Cost-of-Sculpture\\nHobbies\\nEducational Training\\n1st in Predict-The-Cost-of-Sculpture on HackerEarth.\\n1st in DPhi Tinder Millennial Match Prediction.\\n2nd in House Price Prediction Challenge on MachineHack.\\n2nd in Analytics Vidhya's Guided community Hackathon.\\nFeatured in AIM: https://analyticsindiamag.com/house-price-prediction-challenge-\\nmeet-the-winners-know-their-approach/\\n\",\n",
       " ' \\n \\n \\n \\nSUNIL SHARMA \\nPHONE: 425-647-0975 \\nE-MAIL ADDRESS: SUNILKSH@OUTLOOK.COM  \\n                                 \\n \\nSUMMARY \\n \\n\\uf0b7 \\nSeasoned Telecommunication/Data Communication professional with experience in companies \\nlike AT&T, TELUS, Acision/LogicaCMG, Ericsson-HP, Alstom, True Position, and HCL Perot \\nsystems. \\n\\uf0b7 \\nExperience in products design, development and deployment life cycle, right from identifying \\nbusiness requirements, design and implementation, testing and designing customized solutions \\nand handling complex integration projects. \\n\\uf0b7 \\nExperience in solution architecture with developing technical architecture and defining strategy \\nfor integrating products & deliver overall solution according to the customer requirement and \\nspecifications. \\n\\uf0b7 \\nExperience of customer interaction including requirement gathering and scoping, value \\npropositioning and negotiations. \\n\\uf0b7 \\nAbility to create, present, and implement competitive solutions by partnering with customers to \\nanalyze needs, gather specifications and situational requirements, and work with business \\ndevelopment personnel to create effective solutions. \\n\\uf0b7 \\nExcellent interpersonal and customer relationship management skills. \\n\\uf0b7 \\nProduced several research/white papers, case studies and other collaterals to generate sales leads \\n(Pre-sales) and responded to several RFIs/RFPs and prepared and gave product demonstrations. \\n\\uf0b7 \\nStrong team player with ability to handle multiple tasks and also works independently. \\n\\uf0b7 \\nPossess good analytical skills and problem solving capabilities along with good communication \\nskills. \\n \\nTECHNICAL SKILLS \\n \\nPROGRAMMING \\nLANGUAGES \\nPython, Java, C, C++ \\nDATABASE \\nSQL, INFORMIX, MySQL, Cassandra, MariaDB \\nDATA SCIENCE TOOLS \\nJupyter Notebook, Panda, Scikit-Learn, Numpy, SciPy, Matplotlib,  \\nElasticsearch, Logstash, Kibana, Grafana \\nOTHERS \\nHibernate, Ant, JUnit, CVS, SVN, Eclipse, XML, Visual Studio, XML, SOAP, \\nEclipse, Git \\nTELECOM/DATACOM \\nTECHNOLOGIES \\n \\nM2M/IOT,IP Messaging, LTE, HSPA/UMTS, GSM, E911, Pre-paid \\nTelecom billing, IN, OSS, ISDN, VOIP, IMS, Wi-Fi, BLUETOOTH, Network \\nManagement, International Roaming, OTA, Device Management \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nPROFESSIONAL EXPERIENCE  \\n \\n \\nMasters of Science in Machine Learning and AI \\n (Online)      \\n       \\n(Nov’ 2019– Present) \\n \\nPursuing MSc in Machine Learning and AI from Liverpool John Moores University \\n \\nProjects & Case Studies undertaken so far as part of the course: \\n- \\nTelecom Churn case Study - Predicting Customer churn for a telecom operator \\n- \\nLending Club case study - Decipher the types of customer default on loan \\n- \\nCar Pricing in US market - Understand the factors affecting the pricing of cars in US market, \\nhelping a Chinese company enter the US market \\n \\nMavenir ’USA - Senior Solutions Architect  \\n \\n \\n       \\n \\n(May 2019– Oct 2019) \\n \\nWorked as solutions architect for message store (mStore) product in IP messaging domain. \\n \\nSENTACA ’USA - Senior Solutions Architect \\n \\n \\n \\n       \\n(Jun 2015– May 2019) \\n \\nPROJECT- AT&T - Messaging Operational Certification –SAG, FDA, MWI, OTA, CPM and \\nAutomation/Data Analytics \\n \\nTechnologies/Protocols/Interfaces: Mavenir SMSC, SMS Aggregation Gateway & OTA     (FDA, \\nMWI, SAG & OTA), Charging gateway, Antispam server, ENUM server. SMPP, Sigtran, MAP, \\nDiameter, Simulators, Elastic search, Logstash, Kibana, MariaDB, YAML, Pandas, Numpy, Matplotlib. \\n \\nDESCRIPTION: As part of Messaging group operational certification, the scope of the project is to \\ncertify Messaging platforms in Pre-Production for operational readiness in production environment. \\n \\nROLE \\n\\uf0b7 \\nReview and finalize Customer requirement specification with Vendor and Client. \\n\\uf0b7 \\nTest cases planning, development and execution support, conduct Triages & troubleshoot issues.  \\n\\uf0b7 \\nProvide design support for the solution to be deployed in Pre-Production and Production \\nenvironment. \\n\\uf0b7 \\nInterface with end point nodes primes for the integration requirements for the implementation. \\n\\uf0b7 \\nProvide progress reports to both internal and external stakeholders. \\n\\uf0b7 \\nProject planning platforms pre-production validation/verification/implementation activities. \\n\\uf0b7 \\nDevelopment of tools and simulators for platform deliveries verification. \\n\\uf0b7 \\nTest case automation implementation for CPM platform using in-house Prism platform. \\n\\uf0b7 \\nInteract with vendor to coordinate the deployment and testing efforts and other onsite activities. \\n\\uf0b7 \\nData Analytics to gain insights into the network issues based on network Logs, TRL, and CDRs \\netc. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nEDUCATION \\n \\n \\nB.E (Electronics & Telecommunication) – GJ University’ India \\nPGD (ML & AI) – IIIT Bangalore ‘India \\nM.sc Machine Learning and AI (Pursuing) - Liverpool John Moores University’ UK \\nMBA -Global (Pursuing) – Deakin University’ Australia \\n \\nCERTIFICATIONS/COURSES: \\n \\nPMP Certification - PMI, PA, USA \\nEnterprise Management Certification - IIT Delhi \\nCorporate Finance Essentials - IESE Business School ‘ Coursera \\nCertified Openstack Administrator - COA \\n \\nDATA Analytics: \\n \\nIntroduction to Data Science in Python – University of Michigan (Coursera) \\nApplied Machine Learning in Python – University of Michigan (Coursera) \\nApplied Text Mining in Python – University Of Michigan (Coursera) \\nMachine Learning Foundations: A case study approach - University of Washington (Coursera) \\nMachine Learning: Regression - University of Washington (Coursera) \\nEssential Math for Machine Learning: Python Edition – EDX \\nMathematics for Machine Learning: Linear Algebra – Imperial College London (Coursera) \\nMathematics for Machine Learning: Multivariate Calculus– Imperial College London (Coursera) \\n \\nDS101: Introduction to Apache Cassandra - Datastax \\nDS201: Enterprise 6 Foundations of Apache Cassandra - DataStax \\nDS210: Enterprise 6 Operations with Apache Cassandra - DataStax \\n \\nTELECOM TRAININGS: \\n \\nMessage Store, SAG, FDA, MWI, RMS - Mavenir \\nSIMOTA DPA - Giesecke & Devrient, Canada \\nIMS & LTE-EPS Network Signaling Training Course - AWARD SOLUTIONS TX, USA \\nProduct Training/Exposure on INP, PSA, SMSC, MAG - ACISION/LOGICACMG \\nProduct Training on VMCC, DCG - ROAMWARE SYSTEMS CA, USA \\nProduct Training on Global Roamer Platform - KEYNOTE SIGOS GMBH, GERMANY \\nProduct Training workshop on RIA platform, GeoProbe - TEKTRONIX CA, USA \\nProduct training on AXE switch, TMOS, Mediation Platform - ERICSSON HP \\n \\nOTHERS \\n \\n \\nAwarded PATENT in Messaging \\nLORE Award for Best performance in Acision/LOGICACMG \\nBest Performance Award in Ericsson HP. \\nIEEE Member \\n \\n',\n",
       " 'V A I B H A V   M A T H U R                                          mathur06.vaibhav@gmail.com| 7217309517 \\n \\nEDUCATIONAL QUALIFICATION \\n Bachelor of Technology (CSE) \\n                 SRM UNIVERSITY (81.98%) \\n   2015 – 2019 \\n AISSCE (CBSE) \\n CENTRAL ACADEMY, JODHPUR (85.2%) \\n   2013 – 2014 \\n AISSE (CBSE) \\n CENTRAL ACADEMY, JODHPUR (9.4/10) \\n   2011 – 2012 \\n  \\n             PROFESSIONAL EXPERIENCE \\n \\n \\n  \\n \\n Data Scientist | Impact Analytics, Bengaluru                             \\n     Oct’20 - Present  \\n \\n• Developed in-house model using Pytorch to predict quantity sold; Calculated trend, seasonality, promo, price contributions  \\n• Used Fourier series to calculate seasonality; Added changepoints to calculate trend and include restrictions for coefficients   \\n \\n• Added upper-level seasonality for new articles; Improved overall WAPE by 15% compare to previous in-house models \\n \\nData Scientist | Impact Analytics, Bengaluru                                                                                                           Sept’19 – Oct’20 \\n• Developed a promotion optimization tool for a large warehouse club retailer that resulted in incremental revenue of 8%  \\n \\n• Predicted discount depths for articles in every cycle of year; LightGBM model used & Bayesian Opti for parameter tuning  \\n• Optimization of Margin done by Gurobi based on client’s conditions on like Spend, Revenue values compared to last year   \\n \\nData Science Intern | Impact Analytics, Bengaluru                                                                                                 Jun’19 – Sept’19 \\n• Implemented ML model to find similar product from client’s competitor website to compare their products prices with client  \\n \\n• Worked on product description to find similarity; Used NLP techniques to deal with text data cleaning and lemmatization   \\n \\n• Used TF-IDF to convert text to numerical format and created vectors for product descriptions and find cosine similarity \\n• Used prices for both type of products to get best matched products after applying cosine similarity between vectors of text \\n \\nData Science Intern | CapitalVia Global Research Limited, Indore                                                                            May’18 – Jul’18 \\n• Improved strategies of the sales department by forecasting employee’s sales using Time Series algorithm ARIMA model \\n \\n• Increased the lead by 21% using predicted lead score by Random Forest & XGBoost; Bayesian Opt for parameter tuning \\n \\n• Improved the quality of HR management by Predicted the employee attrition using LightGBM and got roc-auc of 0.88 \\n \\nData Science Intern | Veritas Techsoft Private Ltd, Noida                                                                                          Dec’17 – Jan’18 \\n• Increased the sales of a local shopping center by 20% through predicting the future sales of the different types of items \\n• Decision Tree, LightGBM algorithm; Final model contain ensemble of both best resulting models with rmse score 0.15/1  \\n \\n• Advanced the quality of content in a local magazine by predicting popularity of twitter celebrity using XGB; roc-auc metric  \\n \\n \\n \\n \\n \\n \\n \\n       PROJECTS \\n  \\nPredicting click on ad/promotion by user                                                                                                                SRM’19 \\n• Build a machine learning classifier to predict whether user will click on ad/promotion or not present on the company website \\n \\n• Used user demographics, user actions, product details as features; LightGBM algorithm used and got roc-auc score 0.92/1 \\n \\nAlzheimer Disease Detection by Machine Learning                                                                                                SRM’18 \\n• Implemented machine learning classification algorithm to build a classifier XGBoost and Random Forest algorithms used \\n \\n• Roc-Auc score 0.85/1; Medical features present such as mmse, year of education, nwbv ; used SMOTE to balance classes  \\n \\nTwitter Sentiment Analysis                                                                                                                                         SRM’17 \\n• The task was to classify racist or sexist tweets from others; cleaned tweets by tokenization, stemming, lemmatization; python  \\n• Bag-of-Words, TF-IDF to generate text features with the help of; model build using SVM with Grid Search to tune params \\n                                                                    ACHEIVMENTS AND AWARDS  \\n • Landed at Top 2% out of 6456 in WNS Analytics Wizard hackathon; organized by Analytics Vidhya    \\n • Ranked 19 out of 798 participants in Buyer’s Time Prediction Challenge; organized by Machine Hack \\n      2019 \\n      2020  \\n \\n • Achieved Rank 2 out of 6662 in HDFC Machine Learning Hiring Challenge; organized Hackerearth    \\n      2018  \\n \\n • Landed at Top 2% out of 3749 in India ML Hiring hackathon; organized by Analytics Vidhya                                  2019  \\n • Landed at Rank 34 out of 5000 in Affine Analytics ML Hackathon; organized by Hackerearth                                  2018                                                                            \\nSKILLS \\n  \\nTechnical – Python, Machine Learning, NLP, Time Series, Pytorch, Deep Learning \\nSoft Skills – Leadership, Conflict Resolution, Teamwork, Professional communication, Adaptability \\nEXTRA CURRICULAR ACTIVITIES \\nSRM \\nOrganized workshop on Machine Learning/Data Science for juniors to introduce them with these new \\ntechnologies and how they can learn and enhance their skills in machine learning; Got 2nd/6 in VolleyBall \\nCA \\nWon 2 times Bronze medal in Maths Olympiad held across India; Winner in science quiz out of 10 teams \\n \\n',\n",
       " ' \\n \\nVINCY SAGAR                         \\npaulsagar13vincy@gmail.com |+91 8800858216 | LinkedIn: https://www.linkedin.com/in/vincy-sagar-326099138/  \\nGitHub: www.github.com/Vincy13 | Blog: https://easyplacementguide.wordpress.com/ \\nPROFILE \\nOrganized and ambitious budding professional with experience in data science and business analysis with a thirst for learning \\ntop-notch skills, collaborating with great teams, sharing new ideas and leveraging my skills for the growth of the company. \\nSkills Summary \\nData Science                       Machine Learning R&D                      Statistics                       Python                         SAS                         Linux                                              \\nEXPERIENCE \\nDec’19 -\\nPresent \\nRisk Analyst II, American Express, Gurgaon \\n-Prevent monetary losses by capturing the risky transactions, high fraud rate merchants and risky customers using \\nGBM model and neural networks \\n-Increased the fraud coverage of the neural networks model from 34% to 45% that correspond to additional 100,000 \\nUSD fraud amount that can be saved by the company  \\n-Captured a fraud attack of 50000 USD on Amex cards, by generating and analyzing a weekly fraud network report \\n-Analyzed the fraud cases to evaluate patterns, suspicious activities, fraud applications and credit burst and come \\nup with new variable ideas that capture more information \\n-Personal research and development and collaborative brainstorming to improve the fraud coverage of existing \\nmachine learning models \\n-Collaborated with multiple teams in the same time period to deliver the projects in time; Held mentorship sessions \\n \\nSep’19 -\\nDec’19 \\nData Scientist, Impact Analytics, Bengaluru \\n-Predictive modelling using algorithm like linear regression and XGBoost to predict sales of the various categories of \\nproducts of a jewelry brand \\n-Improve upon existing promotions and recommend best promotions for maximum profit for a jewelry brand \\n-Worked in a team centered environment providing suggestions and assistance to fellow teammates to improve the \\nproject delivery and performance \\n \\nJun’19 -\\nSep’19 \\nData Science Intern, Impact Analytics, Bengaluru \\n- Promo recommendation and demand forecasting for a jewelry brand through EDA; Language used – Python \\n- Forecasting the weekly inventory for the products of a jewelry brand using ARIMA model; Language used - R \\n \\nEDUCATION \\nSRMIST (SRM University)                                   2015-2019                          Bachelor of Computer Science and Engineering - 7.98/10 \\nSophia Girls’ School, Meerut                             2013-2014                                                                   Higher Secondary (ICSE) - 73.40% \\nSophia Girls’ School, Meerut                             2011-2012                                                                              High School (ICSE) - 80.86% \\nPROJECTS \\nAnalysis of The Customer’s Banking Behavioral Scorecard using Machine Learning                                  SRM University 2019 \\nPredictive modelling using algorithms like logistic regression, random forest and XGBoost to predict the bad loan applications \\nPerformed data visualization for analysis and feature engineering to generate new features \\nKEY SKILLS AND CHARACTERISTICS \\nCornerstone, Hive, SAS, Linux, Jupyter, Predictive Modelling, Neural Networks, Python, SQL, Statistical Analysis, MS Excel \\nTeamwork, Timely delivery of projects, Adaptability, Effective Communication, Critical thinking, Problem-solving \\nACHIEVEMENTS \\nExpert guide on AntWak and Expertrons, providing career guidance and interview tips to the aspirants                                  2020 \\nScored 99.30% in Novartis Data Scientist Hiring Hackathon conducted on Hackerearth                                                                2020 \\n \\n',\n",
       " 'MAJELLA YUKTHA B \\n18PD17 \\n \\nOBJECTIVE \\n \\nTo obtain a position as a Student Intern for a period of six months from May\\n \\n  \\n \\n  \\n \\n \\n  \\n \\n \\n \\n \\n \\n \\n2021 to November 2021. \\n \\nACADEMIC QUALIFICATION\\n \\n \\n \\nCurrently pursuing 3\\u200brd year of 5 year Integrated M.Sc. Data Science at\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nDepartment of Applied Mathematics and Computational Sciences at PSG College of\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nTechnology. \\n \\nSKILL SET \\n \\n \\nAREAS OF INTEREST \\n \\nACADEMIC RECORD \\n \\n \\n \\n \\n \\n \\nFather’s name \\nGender \\n \\nDate of Birth \\nLanguages known \\nEmail \\nMobile \\n \\nFidelis Biju M \\nFemale \\n10\\u200bth\\u200b May 2001 \\nEnglish, Tamil \\nyukthabijuu@gmail.com \\n+91-81909-88815 \\nPermanent Address \\n3-273/5A, Green Street, \\nAnanthan Nagar, \\nGurukulam Road,  \\nNagercoil, Kanyakumari, \\nTamil Nadu – 629201. \\n \\nLanguages \\nPython, R, C, C++, Java \\nBack-End \\nOracle \\nPlatform \\nWindows, Linux \\nTools \\nMySQL \\n●\\nStatistics \\n●\\nDBMS \\n●\\nSupervised and Unsupervised Learning \\n \\nCourse \\nInstitution \\nBoard \\nCompletion By Marks (%) \\n \\nM.Sc. \\n \\nPSG College of Technology, \\nCoimbatore \\nAnna University \\n \\n2023 \\n8.22 \\n \\nXII \\nShree Sarasswathi Vidhyaah \\nMandheer, \\nMettupalayam \\nCBSE \\n2018 \\n90.6 \\n \\nX \\nAdarsh Vidya Kendra, \\nNagercoil \\nCBSE \\n \\n2016 \\n95 \\n \\nACADEMIC PROJECTS\\n \\n \\n \\n●\\nE-HumanAct, developed using \\u200bPython\\u200b, for the study of various Machine Learning\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nalgorithms to solve the problem of Human Activity Recognition and analyze the efficiency\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nof the various approaches used such as \\u200bDecision Trees, SVM, KNN and\\u200b \\u200bADA-Boost\\u200b. \\n●\\nDrug Analylab, \\u200bdeveloped using \\u200bPython\\u200b, for the analyses of various characteristics of\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\ndrug-drug interaction in the aspect of chemical similarity, target similarity and other\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nfeatures using ML algorithms\\u200b. \\n●\\nCondoQuest, \\u200bdeveloped using \\u200bHTML, CSS, JS and PHP\\u200b, is a Real Estate client-server web\\n \\n \\n \\n \\n \\n \\n \\n   \\n \\n \\n \\n \\napplication. With formulated constraints and considering human ideology, a web\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\napplication was developed that could be useful for both clients searching for homes and\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nselling homes. \\n●\\nUniSpotifer, developed using \\u200bPython\\u200b, for the purpose of analyzing and visualizing various\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nuniversities based on the quality of education, faculty and various other factors. The\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nproject uses libraries such as \\u200bTKinter,\\u200b \\u200bmpl_toolkits\\u200b and \\u200bplotly\\u200b.  \\n●\\nFuzzy-Matchy, developed using \\u200bC++\\u200b, to implement fuzzy string matching using the data\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nstructure, \\u200bBK Trees\\u200b. Given a file containing words, the contents are extracted and inserted\\n \\n \\n \\n  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nto the BK-Tree to enable searching, spelling corrections and similar word prediction.  \\n \\nEXTRA-CURRICULAR ACTIVITIES AND ACHIEVEMENTS \\n \\n●\\nCertified by ABRSM for completing fourth grade in Acoustic Guitar. \\n●\\nMember of Entrepreneur Club, PSG College of Technology. \\n●\\nMember of Pragma Club. \\n \\nDECLARATION  \\n \\nI, Majella Yuktha B, do hereby confirm that the information given above is true\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n  \\n \\nto the best of my knowledge. \\n \\nPlace: Coimbatore  \\nDate : 23/01/2021 \\n   (Majella Yuktha B)  \\n \\n',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '']"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "5e7ae581",
   "metadata": {},
   "outputs": [],
   "source": [
    "while(\"\" in a):\n",
    "    a.remove(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "3a432fe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "1df3bf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(a)):\n",
    "    a[i] = a[i].replace(\"\\n\",\"\").replace(\"\\uf0b7\",\"\").replace(r\"●\",\"\").replace(\"-\",\" \").replace(\"_\",\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "14a37c67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CHARU BANSAL                    5th Year Undergraduate Mb: +91 7080024445                                          Department of Mathematics & Scientific Computing, with Minor in Machine Learning and Applications Email: charub@iitk.ac.in                       Indian Institute of Technology Kanpur Technical Skills  Programming languages  C/C++, HTML/CSS, R, Python, MATLAB  Operating System  Windows, Linux Educational Qualifications  Year Degree/Board Institute/School CGPA/% 2018 2019 M.S. (MTH) IIT Kanpur 8.0* 2014 2018 B.S. (MTH) IIT Kanpur 6.9 2014 AISSCE (CBSE XIIth Board) Jai Academy, Jhansi 92.8 2012 AISSE (CBSE Xth Board) Jai Academy, Jhansi 10.0 *CGPA at the end of 8th Semester Experience Summer Intern @Accenture Digital (Project on Fraud Detection in Procurement Analysis)                   [May’18 Jul’18]  Performed Outlier Detection on Procurement Data using unsupervised k NN density approach, Local Outlier Factor, k means Clustering and one class SVM in R.  Built a User Interface in R Shiny, to integrate all the processes for the user. The platform provides the user with the option of selecting and filtering data, choosing the model and its hyperparameters, and view interactive plots of the required variables.  Tested the program on six specific use cases of fraud, for comparing models and analyzing their effectiveness. Probabilistic Machine Learning Project on ‘Zero Shot Learning’                  [Aug’17 Nov’17] Project under Prof. Piyush Rai, Computer Science Department, IIT Kanpur  Predicted the distribution parameters of unseen classes using regression on distribution parameters of seen classes.  Employed and compared Multivariate Regression Tree and Gaussian Process Regression for prediction.  Attempted Generalised Zero Shot Learning by sampling data from the predicted distribution of unseen classes. Natural Language Processing Project on ‘Sentiment Analysis on Tweets’                     [Jan’18 Apr’18] Project under Prof. Harish Karnick, Computer Science Department, IIT Kanpur  Analyzed and classified sentiments of tweets into positive and negative.  Used the BBoW, tf, tfidf, Word2Vec, & GLoVE representations for feature extraction, after their tokenization and pre processing.  Classified the tweets using Naïve Bayes, Logistic Regression, SVM, Feed Forward Neural Net (Multi Layer Perceptron) and LSTM. Time Series Analysis Project on ‘GDP Forecasting’                 [Aug’17 Nov’17] Project under Prof. Amit Mitra, Mathematics and Statistics Department, IIT Kanpur  Analyzed for trend, seasonality, stationarity, and predicted GDP based on its time series, in R  Implemented the ARIMA model, Holt Winters Smoothing, Augmented Dickey Fuller, KPSS, & Ljung Box tests for further analysis. Regression Analysis Project on ‘Statistical Modelling of Housing Prices’                      [Jan’17 Apr’17] Project under Prof. Sharmishtha Mitra, Mathematics and Statistics Department, IIT Kanpur  Designed a consistent Linear Multiple Regression Model to predict Housing Prices; employing a series of steps; fitting the usual Ordinary Least Squares model, residual analysis, checking multicollinearity and variable selection; for the process. Statistical Simulation and Data Analysis Project on ‘Identifying Authenticity of Currency Notes’                               [Jan’18 Apr’18] Project under Prof. Debasis Kundu, Mathematics and Statistics Department, IIT Kanpur  Modelled the Swiss bank data set on real and fake currencies with a two component Gaussian Mixture Model using Expectation Maximization algorithm, employed AIC/BIC scores to decide the shape and correlation structure of the clusters.  Checked and dismissed the requirement of k means parameter initialization and soft clustering classification. Machine Learning Project on ‘E mail Spam Filtering’                [Aug’16 Nov’16] Project under Prof. Piyush Rai, Computer Science Department, IIT Kanpur  Explored different classifiers for spam filtering and compared the results obtained to get a fair and comparative idea about the accuracy of various learning algorithms.  Scholastic Achievements  Secured AIR 1496 in JEE Advanced 2014 out of the top 150,000 applicants selected in JEE Mains 2014.  Secured AIR  838 in GATE 2018 in Mathematics.  Selected among the top 1500 students qualified for the final interview round of NTSE 2010, out of the 300,000 applicants.  Relevant Courses   Data Structure and Algorithm  Machine Learning Techniques  Probabilistic Machine Learning    Regression Analysis  Applied Stochastic Processes  Applied Game Theory   Probability and Statistics     Inference  Time Series Analysis   Natural Language Processing  Statistical Simulation & Data Analysis    Data Mining*              *On going Courses Positions of Responsibility  President, BloodConnect, Kanpur   Coordinator, Fine Arts Club, IITK   Student coordinator, Raktarpan, NSS  Extra Curricular Activities  Mentored 4 underprivileged students of class IX under the program NSS.  Was part of team of 12 members, who organized and went for a week long trek to the Chandrashila peak (Uttarakhand) and back.  Arranged donation for underprivileged kids as social initiative by Inter IIT Sports Meet’16.  Performed and mentored Sand Art performance during various institute functions like Freshers’ night and Suicide Prevention Day. '"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bec6e5",
   "metadata": {},
   "source": [
    "## Using regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "85623bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "db3c9c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r\"\\s{2,}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "17511718",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(a)):\n",
    "    a[i] = re.sub(pattern,\" \",a[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "b167217d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Anant Mundra Pre final year student B.Tech (information technology (8.23 cgpa) Indian Institute of Information Technology, UNA (IIIT Una) IndiaEDUCATION SENIOR SECONDARY SCHOOL – [2016 2018] Maheshwari Public School, Kota (79.8%, CBSE) SECONDARY SCHOOL – [2014 2016] The Aditya Birla Public School, Chittorgarh (9.6 cgpa , CBSE) Research Intern at ALPHABETA INC A deep financial technology firm using Visualization, AI, DLT and Edge Computing. June 2020 Aug 2020 (Certificate) • Worked on a set of problems faced by migrant labour in India during CoVID 19 lockdowns. My role was to build data driven models for most effective ways to: • Disburse govt help to migrant labours, and • Bring them back to their home states, safely. • Worked on to build a data driven model for govt incentives in commercial lending to MSME starting with stressed sectors facing CoVID 19 induced problems. • Designing an appropriate model on raising capital for OLA keeping in mind the CoVID 19 impact on business. Machine Learning – Teaching Assistant at ALPHABETA INC on Cocalc . Sept 2020 Nov 2020 • Tasked to build evaluation model including assignments, interactive quizzes for FinTech courses, to be delivered to Engineering and Management students at a leading university. • Updated the CoCalc environment with relevant processes for evaluation LEARNING & ACHIEVEMENT: • Part of technical team at ALPHABETA, I worked on building multiple machine learning models, doing various data analysis.LANGUAGESENGLISHHINDICOURSES COMPLETED • Joy Of Computing Using Python (NPTEL) Achieved Silver Medal with 87.5% • Complete Data Science Boot Camp (UDEMY) Complete Data Science Training: Mathematics, Statistics, Python, Advanced Statistics in Python, Machine and Deep Learning. • Introduction to Investing and Portfolio Management (ALPHABETA) Basics of stock market, looking at risk return relationship, exploring asset classes, passive and active investing using ETFs, and Index Funds.ACTIVITIES & AWARDS• PARTICIPATED IN THE IIIT JABALPUR (GUSTO’20) SPORTS FIESTA TABLE TENNIS TOURNAMENT. • PARTICIPATED IN THE CBSE CLUSTER WEST ZONE HANDBALL TOURNAMENT. • IN MY FREE TIME I TAKE OUT TIME FOR SPORTS (SWIMMING) AND ENJOY WATCHING THRILLER/CRIME SERIALS.SKILLS SUMMARYCOMPETITIVE PROGRAMMERPYTHON PROGRAMMING LANGUAGEDATA SCIENCE PYTHON | TABLEAU HTML/CSSSUPERVISED MLUNSUPERVISED MLDEEP LEARNINGC/C++ LANGUAGEanantmundra11@gmail.com+91 63781.33616CONTACTWORK EXPERIENCE Research Intern at IIT MANDI Jan 2021 – ongoing (6 months) ROLE: •Part of a team, working on the growth of plants. Building data driven models to analyse different factors affecting the growth. •Building various regression models for weather forecasting. •Uni variate & Multi variate time series analysis. (Prophet, ARIMA, LSTM) •Correlating local weather data to the global dataset, with the actuals.'"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "e81e734b",
   "metadata": {},
   "outputs": [],
   "source": [
    "email = []\n",
    "pattern = \"[?=A-Za-z]\\w+\\.*\\w+@+\\w+\\.[a-zA-Z]+\"\n",
    "for i in range(len(a)):\n",
    "    x = re.search(pattern,a[i])\n",
    "    try:\n",
    "        email.append(x.group())      \n",
    "    except AttributeError:\n",
    "        email.append(None)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "5bdc2f6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['charub@iitk.ac',\n",
       " 'Kamalajayganti3@gmail.com',\n",
       " 'ajinkya.takawale97@gmail.com',\n",
       " 'LANGUAGEanantmundra11@gmail.com',\n",
       " 'solutions.anuragiitr9ag@gmail.com',\n",
       " 'life.aravindpai23@gmail.com',\n",
       " 'ashishkr.30@gmail.com',\n",
       " 'saiharshithballa99@gmail.com',\n",
       " 'ayushpaliwal2015@gmail.com',\n",
       " None,\n",
       " 'abhiroopkumar.iitkgp@gmail.com',\n",
       " 'statsguysalim@gmail.com',\n",
       " 'dhruvbishnoi0010@gmail.com',\n",
       " 'kanishka24sept@gmail.com',\n",
       " 'gobillamothy85@gmail.com',\n",
       " 'lohawala.che14@iitbhu.ac',\n",
       " 'jassican300@gmail.com',\n",
       " 'erjyoti@gmail.com',\n",
       " 'kkchaitu27@gmail.com',\n",
       " 'krishnapriyakejriwal@gmail.com',\n",
       " 'manasisingh11@gmail.com',\n",
       " 'nandinib1811@gmail.comKEY',\n",
       " 'Telugunithilaau28@gmail.com',\n",
       " 'omkarpathak27@gmail.com',\n",
       " 'prashantarora998@gmail.com',\n",
       " 'rajat.ranjan24@gmail.com',\n",
       " 'ravinirala123@gmail.com',\n",
       " 'sidharth19.sachdeva@gmail.com',\n",
       " 'anjali9singh@gmail.com',\n",
       " 'sunilkumarn@protonmail.comlinkedin',\n",
       " 'niteshkumardmk1@gmail.com',\n",
       " 'zishan.kamal@gmail.com',\n",
       " 'Tamil2000rooban@gmail.com',\n",
       " 'Rushikeshwayalrushi5@gmail.com',\n",
       " 'mahajan.saisuchith@gmail.com',\n",
       " 'sauravmishra554@gmail.com',\n",
       " 'Engineershyamnambiraja01@gmail.com',\n",
       " 'interest.duddusrichandra@gmail.com',\n",
       " 'krsubham48@gmail.com',\n",
       " 'sunilhule4444@gmail.comSkillsCoding',\n",
       " 'SUNILKSH@OUTLOOK.COM',\n",
       " 'mathur06.vaibhav@gmail.com',\n",
       " 'paulsagar13vincy@gmail.com',\n",
       " 'yukthabijuu@gmail.com']"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "2ca05863",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = []\n",
    "pattern = \"\\w+ \\w+\"\n",
    "for i in range(len(a)):\n",
    "    x = re.search(pattern,a[i])\n",
    "    names.append(x.group())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "1bd5f85b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CHARU BANSAL',\n",
       " 'Ganti Sri',\n",
       " 'Ajinkya Takawale1',\n",
       " 'Anant Mundra',\n",
       " 'ANURAG GUPTAData',\n",
       " 'Aravind PaiData',\n",
       " 'SENIOR DATA',\n",
       " 'BALLA SAI',\n",
       " 'EDUCATION INTERNSHIP',\n",
       " 'SUDARSHAN KUMAR',\n",
       " 'May July',\n",
       " 'Salim Shaikh',\n",
       " 'DHRUV DEEP',\n",
       " 'Kanishka Kayathwal',\n",
       " 'Army Institute',\n",
       " 'Huzefa LohawalaData',\n",
       " 'Jaswinder SinghLinkedin',\n",
       " '1 JYOTI',\n",
       " 'K Krishna',\n",
       " 'KRISHNA PRIYA',\n",
       " 'MANASI SINGH',\n",
       " 'nandini b',\n",
       " 'NITHILAA U18PD22Father',\n",
       " 'Omkar PathakSOFTWARE',\n",
       " 'Prashant AroraMachine',\n",
       " 'RAJAT RANJAN',\n",
       " 'RAVI NIRALA',\n",
       " 'SIDHARTH SACHDEVA',\n",
       " 'ANJALI SINGH',\n",
       " 'SUNIL KUMAR',\n",
       " 'NITESH KUMAR',\n",
       " 'Zishan Kamal',\n",
       " 'Rooban Sappani18PD30Father',\n",
       " 'Wayal Rushikesh',\n",
       " 'SAI SUCHITH',\n",
       " 'Saurav Mishra',\n",
       " 'Shyam SundarMachine',\n",
       " 'Sri Chandra',\n",
       " 'KUMAR SUBHAMBachelor',\n",
       " 'SUNIL HULE',\n",
       " 'SUNIL SHARMA',\n",
       " 'V A',\n",
       " 'VINCY SAGAR',\n",
       " 'MAJELLA YUKTHA']"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "d29329b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = \"\\+?\\d*[ \\-]*\\d{3,}[ \\-]*\\d{3,}[ \\-]*\\d{3,}\"\n",
    "pattern1 = \"\\+?\\d*[ \\-]*\\d{5,} *\\d{5,}\"\n",
    "phone = []\n",
    "for i in range(len(a)):\n",
    "    try:\n",
    "        x = re.search(pattern,a[i])\n",
    "        phone.append(x.group())\n",
    "    except AttributeError:\n",
    "        try:\n",
    "            x = re.search(pattern1,a[i])\n",
    "            phone.append(x.group())\n",
    "        except AttributeError:\n",
    "            phone.append(None)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "4afd9d60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['+91 7080024445',\n",
       " '+91 958 111 3007',\n",
       " None,\n",
       " None,\n",
       " '8979546574',\n",
       " '+91 8074101068',\n",
       " '+91 9619794540',\n",
       " '+91 9493105147',\n",
       " '9800315999',\n",
       " None,\n",
       " '+91 9932549291',\n",
       " ' 8286588859',\n",
       " '+91 9728427702',\n",
       " '+91 9833570896',\n",
       " ' 7780272224',\n",
       " '+91 8602242352',\n",
       " ' 7054122875',\n",
       " ' 425 829 8544',\n",
       " '+91 8105442793',\n",
       " '+917856048599',\n",
       " ' 8130921709',\n",
       " None,\n",
       " '+91 89408 02277',\n",
       " ' 8087996634',\n",
       " '08630831390',\n",
       " '+919886043959',\n",
       " '+919661276718',\n",
       " ' 09650302606',\n",
       " '+91 8884544016',\n",
       " '+91 7397419727',\n",
       " '+91 9641575820',\n",
       " ' 9886726680',\n",
       " '+91 97904 56377',\n",
       " ' 8597165877',\n",
       " '+91 8329867394',\n",
       " ' 9435838761',\n",
       " '9789954424',\n",
       " '+91 9933999147',\n",
       " '+91 7766 932528',\n",
       " '+917709308309',\n",
       " ' 425 647 0975',\n",
       " ' 7217309517',\n",
       " '+91 8800858216',\n",
       " '+91 81909 88815']"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "49088af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "edb9a599",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_doc = []\n",
    "tokens = []\n",
    "for i in a:\n",
    "    spacy_doc.append(nlp(i))\n",
    "    tokens.append(list(nlp(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "6ea70403",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_', '__bytes__', '__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__ne__', '__new__', '__pyx_vtable__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__unicode__', 'ancestors', 'check_flag', 'children', 'cluster', 'conjuncts', 'dep', 'dep_', 'doc', 'ent_id', 'ent_id_', 'ent_iob', 'ent_iob_', 'ent_kb_id', 'ent_kb_id_', 'ent_type', 'ent_type_', 'get_extension', 'has_dep', 'has_extension', 'has_head', 'has_morph', 'has_vector', 'head', 'i', 'idx', 'iob_strings', 'is_alpha', 'is_ancestor', 'is_ascii', 'is_bracket', 'is_currency', 'is_digit', 'is_left_punct', 'is_lower', 'is_oov', 'is_punct', 'is_quote', 'is_right_punct', 'is_sent_end', 'is_sent_start', 'is_space', 'is_stop', 'is_title', 'is_upper', 'lang', 'lang_', 'left_edge', 'lefts', 'lemma', 'lemma_', 'lex', 'lex_id', 'like_email', 'like_num', 'like_url', 'lower', 'lower_', 'morph', 'n_lefts', 'n_rights', 'nbor', 'norm', 'norm_', 'orth', 'orth_', 'pos', 'pos_', 'prefix', 'prefix_', 'prob', 'rank', 'remove_extension', 'right_edge', 'rights', 'sent', 'sent_start', 'sentiment', 'set_extension', 'set_morph', 'shape', 'shape_', 'similarity', 'subtree', 'suffix', 'suffix_', 'tag', 'tag_', 'tensor', 'text', 'text_with_ws', 'vector', 'vector_norm', 'vocab', 'whitespace_']\n"
     ]
    }
   ],
   "source": [
    "print (dir(first_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "b5151ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "charub@iitk.ac.in\n",
      "Kamalajayganti3@gmail.com\n",
      "ajinkya.takawale97@gmail.com\n",
      "LANGUAGEanantmundra11@gmail.com+91\n",
      "solutions.anuragiitr9ag@gmail.com8979546574Gurgaon\n",
      "life.aravindpai23@gmail.com+91\n",
      "ashishkr.30@gmail.com\n",
      "ashishkr.30@gmail.com\n",
      "saiharshithballa99@gmail.com\n",
      "ayushpaliwal2015@gmail.com|9800315999\n",
      "abhiroopkumar.iitkgp@gmail.com\n",
      "statsguysalim@gmail.com\n",
      "dhruvbishnoi0010@gmail.com\n",
      "kanishka24sept@gmail.com\n",
      "kanishka24sept@gmail.com\n",
      "gobillamothy85@gmail.com\n",
      "hodcomp@aitpune.edu.in\n",
      "huzefa.lohawala.che14@iitbhu.ac.in\n",
      "jassican300@gmail.com\n",
      "erjyoti@gmail.com\n",
      "kkchaitu27@gmail.com\n",
      "krishnapriyakejriwal@gmail.com\n",
      "nandinib1811@gmail.comKEY\n",
      "Telugunithilaau28@gmail.com+91\n",
      "omkarpathak27@gmail.com|\n",
      ".prashantarora998@gmail.com08630831390Amroha\n",
      "rajat.ranjan24@gmail.com\n",
      "ravinirala123@gmail.com\n",
      "sidharth19.sachdeva@gmail.com\n",
      "anjali9singh@gmail.com\n",
      "7397419727sunilkumarn@protonmail.comlinkedin.com/in/sunil\n",
      "niteshkumardmk1@gmail.com\n",
      "zishan.kamal@gmail.com\n",
      "Tamil2000rooban@gmail.com+91\n",
      "Rushikeshwayalrushi5@gmail.com\n",
      "mahajan.saisuchith@gmail.com\n",
      "Engineershyamnambiraja01@gmail.com9789954424https://www.linkedin.com\n",
      "interest.duddusrichandra@gmail.com+91\n",
      "krsubham48@gmail.com￿\n",
      "SUNILKSH@OUTLOOK.COM\n",
      "mathur06.vaibhav@gmail.com|\n",
      "paulsagar13vincy@gmail.com\n",
      "yukthabijuu@gmail.com\n"
     ]
    }
   ],
   "source": [
    "for j in tokens:\n",
    "    for i in j:\n",
    "        if i.like_email:\n",
    "            print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "fc541625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHARU CHARU compound NNP PROPN \n",
      "BANSAL bansal nmod NN NOUN \n",
      "5th 5th amod JJ ADJ DATE\n",
      "Year Year compound NNP PROPN DATE\n",
      "Undergraduate undergraduate compound NN NOUN \n",
      "Mb mb nmod NN NOUN \n",
      ": : punct : PUNCT \n",
      "+91 +91 nmod NNP PROPN \n",
      "7080024445 7080024445 nummod CD NUM \n",
      "Department Department appos NNP PROPN ORG\n",
      "of of prep IN ADP ORG\n",
      "Mathematics Mathematics pobj NNP PROPN ORG\n",
      "& & cc CC CCONJ ORG\n",
      "Scientific Scientific compound NNP PROPN ORG\n",
      "Computing Computing conj NNP PROPN ORG\n",
      ", , punct , PUNCT \n",
      "with with prep IN ADP \n",
      "Minor Minor pobj NNP PROPN \n",
      "in in prep IN ADP \n",
      "Machine Machine compound NNP PROPN \n",
      "Learning Learning pobj NNP PROPN \n",
      "and and cc CC CCONJ \n",
      "Applications Applications compound NNPS PROPN \n",
      "Email Email conj NNP PROPN \n",
      ": : punct : PUNCT \n",
      "charub@iitk.ac.in charub@iitk.ac.in compound NNP PROPN \n",
      "Indian Indian nmod NNP PROPN ORG\n",
      "Institute Institute nmod NNP PROPN ORG\n",
      "of of prep IN ADP ORG\n",
      "Technology Technology pobj NNP PROPN ORG\n",
      "Kanpur Kanpur compound NNP PROPN ORG\n",
      "Technical Technical compound NNP PROPN ORG\n",
      "Skills Skills compound NNPS PROPN ORG\n",
      "Programming Programming appos NNP PROPN ORG\n",
      "languages language ROOT NNS NOUN \n",
      "C C nmod NNP PROPN \n",
      "/ / punct SYM SYM \n",
      "C++ C++ npadvmod NNP PROPN \n",
      ", , punct , PUNCT \n",
      "HTML HTML nmod NNP PROPN \n",
      "/ / punct SYM SYM \n",
      "CSS CSS conj NNP PROPN \n",
      ", , punct , PUNCT \n",
      "R r conj NN NOUN \n",
      ", , punct , PUNCT \n",
      "Python Python conj NNP PROPN \n",
      ", , punct , PUNCT \n",
      "MATLAB MATLAB compound NNP PROPN ORG\n",
      "Operating Operating compound NNP PROPN ORG\n",
      "System System compound NNP PROPN ORG\n",
      "Windows Windows conj NNP PROPN ORG\n",
      ", , punct , PUNCT \n",
      "Linux Linux compound NNP PROPN \n",
      "Educational Educational compound NNP PROPN \n",
      "Qualifications Qualifications compound NNPS PROPN \n",
      "Year Year compound NNP PROPN \n",
      "Degree Degree nmod NNP PROPN ORG\n",
      "/ / punct SYM SYM ORG\n",
      "Board Board nmod NNP PROPN ORG\n",
      "Institute Institute nmod NNP PROPN ORG\n",
      "/ / punct SYM SYM ORG\n",
      "School School compound NNP PROPN ORG\n",
      "CGPA/% CGPA/% conj NNP PROPN ORG\n",
      "2018 2018 nummod CD NUM \n",
      "2019 2019 nummod CD NUM \n",
      "M.S. M.S. nmod NNP PROPN ORG\n",
      "( ( punct -LRB- PUNCT \n",
      "MTH MTH nmod NNP PROPN \n",
      ") ) punct -RRB- PUNCT \n",
      "IIT IIT compound NNP PROPN ORG\n",
      "Kanpur Kanpur npadvmod NNP PROPN \n",
      "8.0 8.0 nummod CD NUM \n",
      "* * punct CD NUM \n",
      "2014 2014 nummod CD NUM DATE\n",
      "2018 2018 nummod CD NUM DATE\n",
      "B.S. B.S. appos NNP PROPN \n",
      "( ( punct -LRB- PUNCT \n",
      "MTH MTH nmod NNP PROPN \n",
      ") ) punct -RRB- PUNCT \n",
      "IIT IIT compound NNP PROPN ORG\n",
      "Kanpur Kanpur appos NNP PROPN \n",
      "6.9 6.9 nummod CD NUM DATE\n",
      "2014 2014 nummod CD NUM DATE\n",
      "AISSCE AISSCE conj NNP PROPN \n",
      "( ( punct -LRB- PUNCT \n",
      "CBSE CBSE compound NNP PROPN ORG\n",
      "XIIth XIIth compound NNP PROPN ORG\n",
      "Board Board appos NNP PROPN ORG\n",
      ") ) punct -RRB- PUNCT \n",
      "Jai Jai compound NNP PROPN ORG\n",
      "Academy Academy ROOT NNP PROPN ORG\n",
      ", , punct , PUNCT \n",
      "Jhansi Jhansi appos NNP PROPN GPE\n",
      "92.8 92.8 nummod CD NUM \n",
      "2012 2012 nummod CD NUM \n",
      "AISSE AISSE appos NNP PROPN ORG\n",
      "( ( punct -LRB- PUNCT \n",
      "CBSE CBSE compound NNP PROPN ORG\n",
      "Xth Xth compound NNP PROPN ORG\n",
      "Board Board appos NNP PROPN ORG\n",
      ") ) punct -RRB- PUNCT \n",
      "Jai Jai compound NNP PROPN ORG\n",
      "Academy Academy ROOT NNP PROPN ORG\n",
      ", , punct , PUNCT \n",
      "Jhansi Jhansi appos NNP PROPN \n",
      "10.0 10.0 nummod CD NUM \n",
      "* * punct NFP PUNCT \n",
      "CGPA CGPA appos NNP PROPN ORG\n",
      "at at prep IN ADP \n",
      "the the det DT DET DATE\n",
      "end end pobj NN NOUN DATE\n",
      "of of prep IN ADP DATE\n",
      "8th 8th amod JJ ADJ DATE\n",
      "Semester Semester compound NNP PROPN \n",
      "Experience Experience compound NNP PROPN \n",
      "Summer Summer compound NNP PROPN \n",
      "Intern Intern compound NNP PROPN \n",
      "@Accenture @Accenture compound NNP PROPN \n",
      "Digital Digital pobj NNP PROPN \n",
      "( ( punct -LRB- PUNCT \n",
      "Project Project appos NNP PROPN ORG\n",
      "on on prep IN ADP ORG\n",
      "Fraud Fraud compound NNP PROPN ORG\n",
      "Detection Detection pobj NNP PROPN ORG\n",
      "in in prep IN ADP ORG\n",
      "Procurement Procurement compound NNP PROPN ORG\n",
      "Analysis Analysis pobj NNP PROPN ORG\n",
      ") ) punct -RRB- PUNCT \n",
      "[ [ dep XX X \n",
      "May’18 may’18 compound NN NOUN \n",
      "Jul’18 jul’18 dep NN NOUN \n",
      "] ] punct -RRB- PUNCT \n",
      "Performed perform amod VBN VERB \n",
      "Outlier Outlier compound NNP PROPN \n",
      "Detection Detection appos NNP PROPN \n",
      "on on prep IN ADP \n",
      "Procurement Procurement compound NNP PROPN \n",
      "Data Data pobj NNP PROPN \n",
      "using use acl VBG VERB \n",
      "unsupervised unsupervised det JJ ADJ \n",
      "k k compound NNP PROPN \n",
      "NN NN compound NNP PROPN \n",
      "density density compound NN NOUN \n",
      "approach approach dobj NN NOUN \n",
      ", , punct , PUNCT \n",
      "Local local compound JJ ADJ \n",
      "Outlier Outlier compound NNP PROPN \n",
      "Factor Factor appos NNP PROPN \n",
      ", , punct , PUNCT \n",
      "k k nsubj NNP PROPN \n",
      "means mean ROOT VBZ VERB \n",
      "Clustering clustering nsubj NN NOUN \n",
      "and and cc CC CCONJ \n",
      "one one nummod CD NUM CARDINAL\n",
      "class class compound NN NOUN \n",
      "SVM svm conj NN NOUN ORG\n",
      "in in prep IN ADP \n",
      "R. R. pobj NNP PROPN PRODUCT\n",
      "Built build ccomp VBN VERB PRODUCT\n",
      "a a det DT DET \n",
      "User User compound NNP PROPN \n",
      "Interface Interface dobj NNP PROPN \n",
      "in in prep IN ADP \n",
      "R R compound NNP PROPN \n",
      "Shiny Shiny pobj NNP PROPN \n",
      ", , punct , PUNCT \n",
      "to to aux TO PART \n",
      "integrate integrate advcl VB VERB \n",
      "all all predet PDT DET \n",
      "the the det DT DET \n",
      "processes process dobj NNS NOUN \n",
      "for for prep IN ADP \n",
      "the the det DT DET \n",
      "user user pobj NN NOUN \n",
      ". . punct . PUNCT \n",
      "The the det DT DET \n",
      "platform platform nsubj NN NOUN \n",
      "provides provide ROOT VBZ VERB \n",
      "the the det DT DET \n",
      "user user dobj NN NOUN \n",
      "with with prep IN ADP \n",
      "the the det DT DET \n",
      "option option pobj NN NOUN \n",
      "of of prep IN ADP \n",
      "selecting select pcomp VBG VERB \n",
      "and and cc CC CCONJ \n",
      "filtering filter conj VBG VERB \n",
      "data datum dobj NNS NOUN \n",
      ", , punct , PUNCT \n",
      "choosing choose advcl VBG VERB \n",
      "the the det DT DET \n",
      "model model dobj NN NOUN \n",
      "and and cc CC CCONJ \n",
      "its its poss PRP$ PRON \n",
      "hyperparameters hyperparameter conj NNS NOUN \n",
      ", , punct , PUNCT \n",
      "and and cc CC CCONJ \n",
      "view view conj VB VERB \n",
      "interactive interactive amod JJ ADJ \n",
      "plots plot dobj NNS NOUN \n",
      "of of prep IN ADP \n",
      "the the det DT DET \n",
      "required require amod VBN VERB \n",
      "variables variable pobj NNS NOUN \n",
      ". . punct . PUNCT \n",
      "Tested test ROOT VBD VERB \n",
      "the the det DT DET \n",
      "program program dobj NN NOUN \n",
      "on on prep IN ADP \n",
      "six six nummod CD NUM CARDINAL\n",
      "specific specific amod JJ ADJ \n",
      "use use compound NN NOUN \n",
      "cases case pobj NNS NOUN \n",
      "of of prep IN ADP \n",
      "fraud fraud pobj NN NOUN \n",
      ", , punct , PUNCT \n",
      "for for prep IN ADP \n",
      "comparing compare pcomp VBG VERB \n",
      "models model dobj NNS NOUN \n",
      "and and cc CC CCONJ \n",
      "analyzing analyze conj VBG VERB \n",
      "their their poss PRP$ PRON \n",
      "effectiveness effectiveness dobj NN NOUN \n",
      ". . punct . PUNCT \n",
      "Probabilistic Probabilistic compound NNP PROPN ORG\n",
      "Machine Machine compound NNP PROPN ORG\n",
      "Learning Learning compound NNP PROPN ORG\n",
      "Project Project ROOT NNP PROPN ORG\n",
      "on on prep IN ADP \n",
      "‘ ' punct `` PUNCT \n",
      "Zero Zero compound NNP PROPN \n",
      "Shot Shot compound NNP PROPN \n",
      "Learning Learning pobj NNP PROPN \n",
      "’ ' punct '' PUNCT \n",
      "[ [ nmod -LRB- PUNCT \n",
      "Aug’17 Aug’17 compound NNP PROPN \n",
      "Nov’17 Nov’17 nmod NNP PROPN \n",
      "] ] punct -RRB- PUNCT \n",
      "Project Project nsubj NNP PROPN \n",
      "under under prep IN ADP \n",
      "Prof. Prof. compound NNP PROPN \n",
      "Piyush Piyush compound NNP PROPN PERSON\n",
      "Rai Rai pobj NNP PROPN PERSON\n",
      ", , punct , PUNCT \n",
      "Computer Computer compound NNP PROPN ORG\n",
      "Science Science compound NNP PROPN ORG\n",
      "Department Department appos NNP PROPN ORG\n",
      ", , punct , PUNCT \n",
      "IIT IIT compound NNP PROPN ORG\n",
      "Kanpur Kanpur nsubj NNP PROPN ORG\n",
      "Predicted predict ROOT VBD VERB \n",
      "the the det DT DET \n",
      "distribution distribution compound NN NOUN \n",
      "parameters parameter dobj NNS NOUN \n",
      "of of prep IN ADP \n",
      "unseen unseen amod JJ ADJ \n",
      "classes class pobj NNS NOUN \n",
      "using use acl VBG VERB \n",
      "regression regression dobj NN NOUN \n",
      "on on prep IN ADP \n",
      "distribution distribution compound NN NOUN \n",
      "parameters parameter pobj NNS NOUN \n",
      "of of prep IN ADP \n",
      "seen see amod VBN VERB \n",
      "classes class pobj NNS NOUN \n",
      ". . punct . PUNCT \n",
      "Employed employ ROOT VBN VERB \n",
      "and and cc CC CCONJ \n",
      "compared compare conj VBD VERB \n",
      "Multivariate Multivariate compound NNP PROPN ORG\n",
      "Regression Regression compound NNP PROPN ORG\n",
      "Tree Tree pobj NNP PROPN ORG\n",
      "and and cc CC CCONJ ORG\n",
      "Gaussian Gaussian compound NNP PROPN ORG\n",
      "Process Process compound NNP PROPN ORG\n",
      "Regression Regression conj NNP PROPN ORG\n",
      "for for prep IN ADP \n",
      "prediction prediction pobj NN NOUN \n",
      ". . punct . PUNCT \n",
      "Attempted attempt ROOT VBN VERB \n",
      "Generalised Generalised compound NNP PROPN \n",
      "Zero Zero compound NNP PROPN ORG\n",
      "Shot Shot compound NNP PROPN ORG\n",
      "Learning Learning dobj NNP PROPN ORG\n",
      "by by prep IN ADP \n",
      "sampling sample pcomp VBG VERB \n",
      "data datum dobj NNS NOUN \n",
      "from from prep IN ADP \n",
      "the the det DT DET \n",
      "predicted predict amod VBN VERB \n",
      "distribution distribution pobj NN NOUN \n",
      "of of prep IN ADP \n",
      "unseen unseen amod JJ ADJ \n",
      "classes class pobj NNS NOUN \n",
      ". . punct . PUNCT \n",
      "Natural Natural compound NNP PROPN ORG\n",
      "Language Language compound NNP PROPN ORG\n",
      "Processing Processing compound NNP PROPN ORG\n",
      "Project Project nsubj NNP PROPN ORG\n",
      "on on prep IN ADP ORG\n",
      "‘ ' punct `` PUNCT ORG\n",
      "Sentiment Sentiment compound NNP PROPN ORG\n",
      "Analysis Analysis pobj NNP PROPN ORG\n",
      "on on prep IN ADP \n",
      "Tweets tweet pobj NNS NOUN \n",
      "’ ' punct '' PUNCT \n",
      "[ [ punct XX X \n",
      "Jan’18 jan’18 advmod RB ADV \n",
      "Apr’18 apr’18 nmod JJ ADJ \n",
      "] ] punct -RRB- PUNCT \n",
      "Project Project appos NNP PROPN \n",
      "under under prep IN ADP \n",
      "Prof. Prof. compound NNP PROPN \n",
      "Harish Harish compound NNP PROPN PERSON\n",
      "Karnick Karnick pobj NNP PROPN PERSON\n",
      ", , punct , PUNCT \n",
      "Computer Computer compound NNP PROPN ORG\n",
      "Science Science compound NNP PROPN ORG\n",
      "Department Department appos NNP PROPN ORG\n",
      ", , punct , PUNCT \n",
      "IIT IIT compound NNP PROPN ORG\n",
      "Kanpur Kanpur nsubj NNP PROPN \n",
      "Analyzed analyze ROOT VBD VERB \n",
      "and and cc CC CCONJ \n",
      "classified classified amod JJ ADJ \n",
      "sentiments sentiment conj NNS NOUN \n",
      "of of prep IN ADP \n",
      "tweets tweet pobj NNS NOUN \n",
      "into into prep IN ADP \n",
      "positive positive amod JJ ADJ \n",
      "and and cc CC CCONJ \n",
      "negative negative conj JJ ADJ \n",
      ". . punct . PUNCT \n",
      "Used use ROOT VBD VERB \n",
      "the the det DT DET \n",
      "BBoW BBoW dobj NNP PROPN GPE\n",
      ", , punct , PUNCT \n",
      "tf tf appos NNP PROPN GPE\n",
      ", , punct , PUNCT \n",
      "tfidf tfidf conj NNP PROPN \n",
      ", , punct , PUNCT \n",
      "Word2Vec Word2Vec conj NNP PROPN \n",
      ", , punct , PUNCT \n",
      "& & cc CC CCONJ \n",
      "GLoVE GLoVE compound NNP PROPN \n",
      "representations representation conj NNS NOUN \n",
      "for for prep IN ADP \n",
      "feature feature compound NN NOUN \n",
      "extraction extraction pobj NN NOUN \n",
      ", , punct , PUNCT \n",
      "after after prep IN ADP \n",
      "their their poss PRP$ PRON \n",
      "tokenization tokenization pobj NN NOUN \n",
      "and and cc CC CCONJ \n",
      "pre pre compound NN NOUN \n",
      "processing processing conj NN NOUN \n",
      ". . punct . PUNCT \n",
      "Classified classify ROOT VBN VERB \n",
      "the the det DT DET \n",
      "tweets tweet dobj NNS NOUN \n",
      "using use acl VBG VERB \n",
      "Naïve Naïve compound NNP PROPN ORG\n",
      "Bayes Bayes dobj NNP PROPN ORG\n",
      ", , punct , PUNCT \n",
      "Logistic Logistic compound NNP PROPN ORG\n",
      "Regression Regression conj NNP PROPN ORG\n",
      ", , punct , PUNCT \n",
      "SVM SVM conj NNP PROPN ORG\n",
      ", , punct , PUNCT \n",
      "Feed Feed compound NNP PROPN \n",
      "Forward Forward compound NNP PROPN \n",
      "Neural Neural compound NNP PROPN \n",
      "Net Net conj NNP PROPN \n",
      "( ( punct -LRB- PUNCT \n",
      "Multi Multi compound NNP PROPN \n",
      "Layer Layer compound NNP PROPN \n",
      "Perceptron Perceptron appos NNP PROPN \n",
      ") ) punct -RRB- PUNCT \n",
      "and and cc CC CCONJ \n",
      "LSTM LSTM conj NNP PROPN ORG\n",
      ". . punct . PUNCT \n",
      "Time Time compound NNP PROPN \n",
      "Series Series compound NNP PROPN \n",
      "Analysis Analysis compound NNP PROPN \n",
      "Project Project ROOT NNP PROPN \n",
      "on on prep IN ADP \n",
      "‘ ' punct `` PUNCT \n",
      "GDP GDP compound NNP PROPN \n",
      "Forecasting Forecasting pobj NNP PROPN \n",
      "’ ' punct '' PUNCT \n",
      "[ [ nmod -LRB- PUNCT \n",
      "Aug’17 Aug’17 compound NNP PROPN \n",
      "Nov’17 Nov’17 nmod NNP PROPN \n",
      "] ] punct -RRB- PUNCT \n",
      "Project Project nsubj NNP PROPN \n",
      "under under prep IN ADP \n",
      "Prof. Prof. compound NNP PROPN \n",
      "Amit Amit compound NNP PROPN PERSON\n",
      "Mitra Mitra pobj NNP PROPN PERSON\n",
      ", , punct , PUNCT \n",
      "Mathematics Mathematics conj NNP PROPN ORG\n",
      "and and cc CC CCONJ ORG\n",
      "Statistics Statistics compound NNPS PROPN ORG\n",
      "Department Department conj NNP PROPN ORG\n",
      ", , punct , PUNCT \n",
      "IIT IIT compound NNP PROPN ORG\n",
      "Kanpur Kanpur nsubj NNP PROPN \n",
      "Analyzed analyze ROOT VBD VERB \n",
      "for for prep IN ADP \n",
      "trend trend pobj NN NOUN \n",
      ", , punct , PUNCT \n",
      "seasonality seasonality conj NN NOUN \n",
      ", , punct , PUNCT \n",
      "stationarity stationarity conj NN NOUN \n",
      ", , punct , PUNCT \n",
      "and and cc CC CCONJ \n",
      "predicted predict conj VBD VERB \n",
      "GDP gdp dobj NN NOUN \n",
      "based base acl VBN VERB \n",
      "on on prep IN ADP \n",
      "its its poss PRP$ PRON \n",
      "time time compound NN NOUN \n",
      "series series pobj NN NOUN \n",
      ", , punct , PUNCT \n",
      "in in mark IN SCONJ \n",
      "R r nsubj NN NOUN \n",
      "Implemented implement ccomp VBD VERB \n",
      "the the det DT DET \n",
      "ARIMA ARIMA compound NNP PROPN ORG\n",
      "model model dobj NN NOUN \n",
      ", , punct , PUNCT \n",
      "Holt Holt compound NNP PROPN PERSON\n",
      "Winters Winters compound NNP PROPN PERSON\n",
      "Smoothing Smoothing appos NNP PROPN PERSON\n",
      ", , punct , PUNCT \n",
      "Augmented Augmented compound NNP PROPN ORG\n",
      "Dickey Dickey compound NNP PROPN PERSON\n",
      "Fuller Fuller conj NNP PROPN PERSON\n",
      ", , punct , PUNCT \n",
      "KPSS KPSS conj NNP PROPN ORG\n",
      ", , punct , PUNCT ORG\n",
      "& & cc CC CCONJ ORG\n",
      "Ljung Ljung compound NNP PROPN ORG\n",
      "Box Box conj NNP PROPN \n",
      "tests test conj VBZ VERB \n",
      "for for prep IN ADP \n",
      "further further amod JJ ADJ \n",
      "analysis analysis pobj NN NOUN \n",
      ". . punct . PUNCT \n",
      "Regression regression compound NN NOUN \n",
      "Analysis Analysis compound NNP PROPN \n",
      "Project Project ROOT NNP PROPN \n",
      "on on prep IN ADP \n",
      "‘ ' punct `` PUNCT \n",
      "Statistical Statistical compound NNP PROPN \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelling Modelling pobj NNP PROPN \n",
      "of of prep IN ADP \n",
      "Housing Housing compound NNP PROPN \n",
      "Prices Prices pobj NNPS PROPN \n",
      "’ ' punct '' PUNCT \n",
      "[ [ nmod XX X \n",
      "Jan’17 Jan’17 compound NNP PROPN \n",
      "Apr’17 Apr’17 nmod NNP PROPN \n",
      "] ] punct -RRB- PUNCT \n",
      "Project Project ROOT NNP PROPN \n",
      "under under prep IN ADP \n",
      "Prof. Prof. punct NNP PROPN \n",
      "Sharmishtha Sharmishtha compound NNP PROPN PERSON\n",
      "Mitra Mitra nsubj NNP PROPN PERSON\n",
      ", , punct , PUNCT \n",
      "Mathematics Mathematics npadvmod NNP PROPN ORG\n",
      "and and cc CC CCONJ ORG\n",
      "Statistics Statistics compound NNPS PROPN ORG\n",
      "Department Department conj NNP PROPN ORG\n",
      ", , punct , PUNCT \n",
      "IIT IIT compound NNP PROPN ORG\n",
      "Kanpur Kanpur conj NNP PROPN ORG\n",
      "Designed design ROOT VBD VERB ORG\n",
      "a a det DT DET \n",
      "consistent consistent amod JJ ADJ \n",
      "Linear Linear compound NNP PROPN ORG\n",
      "Multiple Multiple compound NNP PROPN ORG\n",
      "Regression Regression compound NNP PROPN ORG\n",
      "Model Model dobj NNP PROPN ORG\n",
      "to to aux TO PART \n",
      "predict predict xcomp VB VERB \n",
      "Housing Housing compound NNP PROPN \n",
      "Prices Prices dobj NNPS PROPN \n",
      "; ; punct : PUNCT \n",
      "employing employ advcl VBG VERB \n",
      "a a det DT DET \n",
      "series series dobj NN NOUN \n",
      "of of prep IN ADP \n",
      "steps step pobj NNS NOUN \n",
      "; ; punct : PUNCT \n",
      "fitting fit conj VBG VERB \n",
      "the the det DT DET \n",
      "usual usual amod JJ ADJ \n",
      "Ordinary Ordinary npadvmod NNP PROPN LAW\n",
      "Least Least compound NNP PROPN LAW\n",
      "Squares Squares compound NNP PROPN LAW\n",
      "model model dobj NN NOUN \n",
      ", , punct , PUNCT \n",
      "residual residual amod JJ ADJ \n",
      "analysis analysis appos NN NOUN \n",
      ", , punct , PUNCT \n",
      "checking check advcl VBG VERB \n",
      "multicollinearity multicollinearity dobj NN NOUN \n",
      "and and cc CC CCONJ \n",
      "variable variable amod JJ ADJ \n",
      "selection selection conj NN NOUN \n",
      "; ; punct : PUNCT \n",
      "for for prep IN ADP \n",
      "the the det DT DET \n",
      "process process pobj NN NOUN \n",
      ". . punct . PUNCT \n",
      "Statistical Statistical compound NNP PROPN \n",
      "Simulation Simulation nmod NNP PROPN \n",
      "and and cc CC CCONJ \n",
      "Data Data compound NNP PROPN \n",
      "Analysis Analysis conj NNP PROPN \n",
      "Project Project ROOT NNP PROPN \n",
      "on on prep IN ADP \n",
      "‘ ' punct `` PUNCT \n",
      "Identifying identify pcomp VBG VERB \n",
      "Authenticity Authenticity dobj NNP PROPN \n",
      "of of prep IN ADP \n",
      "Currency Currency compound NNP PROPN \n",
      "Notes Notes pobj NNP PROPN \n",
      "’ ' punct '' PUNCT \n",
      "[ [ dep XX X \n",
      "Jan’18 jan’18 advmod RB ADV \n",
      "Apr’18 apr’18 amod JJ ADJ \n",
      "] ] punct -RRB- PUNCT \n",
      "Project Project advcl NNP PROPN \n",
      "under under prep IN ADP \n",
      "Prof. Prof. compound NNP PROPN \n",
      "Debasis Debasis compound NNP PROPN PERSON\n",
      "Kundu Kundu pobj NNP PROPN PERSON\n",
      ", , punct , PUNCT \n",
      "Mathematics Mathematics conj NNP PROPN \n",
      "and and cc CC CCONJ \n",
      "Statistics Statistics compound NNPS PROPN \n",
      "Department Department conj NNP PROPN \n",
      ", , punct , PUNCT \n",
      "IIT IIT compound NNP PROPN ORG\n",
      "Kanpur Kanpur nsubj NNP PROPN \n",
      "Modelled model ROOT VBD VERB \n",
      "the the det DT DET \n",
      "Swiss swiss amod JJ ADJ NORP\n",
      "bank bank compound NN NOUN \n",
      "data datum dobj NNS NOUN \n",
      "set set acl VBN VERB \n",
      "on on prep IN ADP \n",
      "real real amod JJ ADJ \n",
      "and and cc CC CCONJ \n",
      "fake fake conj JJ ADJ \n",
      "currencies currency pobj NNS NOUN \n",
      "with with prep IN ADP \n",
      "a a det DT DET \n",
      "two two nummod CD NUM CARDINAL\n",
      "component component compound NN NOUN \n",
      "Gaussian Gaussian compound NNP PROPN WORK_OF_ART\n",
      "Mixture Mixture compound NNP PROPN WORK_OF_ART\n",
      "Model Model pobj NNP PROPN WORK_OF_ART\n",
      "using use acl VBG VERB \n",
      "Expectation Expectation compound NNP PROPN \n",
      "Maximization Maximization dobj NNP PROPN \n",
      "algorithm algorithm dobj NN NOUN \n",
      ", , punct , PUNCT \n",
      "employed employ acl VBN VERB \n",
      "AIC AIC nmod NNP PROPN ORG\n",
      "/ / punct SYM SYM ORG\n",
      "BIC BIC compound NNP PROPN ORG\n",
      "scores score dobj NNS NOUN \n",
      "to to aux TO PART \n",
      "decide decide advcl VB VERB \n",
      "the the det DT DET \n",
      "shape shape nmod NN NOUN \n",
      "and and cc CC CCONJ \n",
      "correlation correlation conj NN NOUN \n",
      "structure structure dobj NN NOUN \n",
      "of of prep IN ADP \n",
      "the the det DT DET \n",
      "clusters cluster pobj NNS NOUN \n",
      ". . punct . PUNCT \n",
      "Checked check ROOT VBN VERB \n",
      "and and cc CC CCONJ \n",
      "dismissed dismiss conj VBD VERB \n",
      "the the det DT DET \n",
      "requirement requirement dobj NN NOUN \n",
      "of of prep IN ADP \n",
      "k k pobj NNP PROPN \n",
      "means mean conj VBZ VERB \n",
      "parameter parameter compound NN NOUN \n",
      "initialization initialization dobj NN NOUN \n",
      "and and cc CC CCONJ \n",
      "soft soft amod JJ ADJ \n",
      "clustering clustering compound NN NOUN \n",
      "classification classification conj NN NOUN \n",
      ". . punct . PUNCT \n",
      "Machine Machine compound NNP PROPN ORG\n",
      "Learning Learning compound NNP PROPN ORG\n",
      "Project Project nsubj NNP PROPN ORG\n",
      "on on prep IN ADP \n",
      "‘ ' punct `` PUNCT \n",
      "E e compound NN NOUN \n",
      "mail mail nmod NN NOUN \n",
      "Spam spam compound NN NOUN \n",
      "Filtering filtering pobj NN NOUN \n",
      "’ ' punct '' PUNCT \n",
      "[ [ nmod XX X \n",
      "Aug’16 aug’16 amod JJ ADJ \n",
      "Nov’16 nov’16 nmod NN NOUN \n",
      "] ] punct -RRB- PUNCT \n",
      "Project Project appos NNP PROPN \n",
      "under under prep IN ADP \n",
      "Prof. Prof. compound NNP PROPN \n",
      "Piyush Piyush compound NNP PROPN PERSON\n",
      "Rai Rai pobj NNP PROPN PERSON\n",
      ", , punct , PUNCT \n",
      "Computer Computer compound NNP PROPN ORG\n",
      "Science Science compound NNP PROPN ORG\n",
      "Department Department appos NNP PROPN ORG\n",
      ", , punct , PUNCT \n",
      "IIT IIT compound NNP PROPN ORG\n",
      "Kanpur Kanpur nsubj NNP PROPN ORG\n",
      "Explored explore ROOT VBD VERB ORG\n",
      "different different amod JJ ADJ \n",
      "classifiers classifier dobj NNS NOUN \n",
      "for for prep IN ADP \n",
      "spam spam compound NN NOUN \n",
      "filtering filtering pobj NN NOUN \n",
      "and and cc CC CCONJ \n",
      "compared compare conj VBD VERB \n",
      "the the det DT DET \n",
      "results result pobj NNS NOUN \n",
      "obtained obtain acl VBN VERB \n",
      "to to aux TO PART \n",
      "get get advcl VB VERB \n",
      "a a det DT DET \n",
      "fair fair amod JJ ADJ \n",
      "and and cc CC CCONJ \n",
      "comparative comparative conj JJ ADJ \n",
      "idea idea dobj NN NOUN \n",
      "about about prep IN ADP \n",
      "the the det DT DET \n",
      "accuracy accuracy pobj NN NOUN \n",
      "of of prep IN ADP \n",
      "various various amod JJ ADJ \n",
      "learning learning compound NN NOUN \n",
      "algorithms algorithm pobj NNS NOUN \n",
      ". . punct . PUNCT \n",
      "Scholastic Scholastic compound NNP PROPN \n",
      "Achievements Achievements compound NNPS PROPN \n",
      "Secured Secured ROOT NNP PROPN \n",
      "AIR AIR dobj NNP PROPN \n",
      "1496 1496 nummod CD NUM \n",
      "in in prep IN ADP \n",
      "JEE JEE compound NNP PROPN ORG\n",
      "Advanced Advanced pobj NNP PROPN ORG\n",
      "2014 2014 npadvmod CD NUM DATE\n",
      "out out prep IN ADP \n",
      "of of prep IN ADP \n",
      "the the det DT DET \n",
      "top top amod JJ ADJ \n",
      "150,000 150,000 nummod CD NUM CARDINAL\n",
      "applicants applicant pobj NNS NOUN \n",
      "selected select acl VBN VERB \n",
      "in in prep IN ADP \n",
      "JEE JEE compound NNP PROPN ORG\n",
      "Mains Mains pobj NNP PROPN ORG\n",
      "2014 2014 nummod CD NUM ORG\n",
      ". . punct . PUNCT \n",
      "Secured secure amod VBN VERB \n",
      "AIR AIR ROOT NNP PROPN \n",
      "838 838 nummod CD NUM \n",
      "in in prep IN ADP \n",
      "GATE GATE pobj NNP PROPN \n",
      "2018 2018 nummod CD NUM DATE\n",
      "in in prep IN ADP \n",
      "Mathematics Mathematics pobj NNP PROPN NORP\n",
      ". . punct . PUNCT \n",
      "Selected select ROOT VBN VERB \n",
      "among among prep IN ADP \n",
      "the the det DT DET \n",
      "top top amod JJ ADJ \n",
      "1500 1500 nummod CD NUM CARDINAL\n",
      "students student pobj NNS NOUN \n",
      "qualified qualified amod JJ ADJ \n",
      "for for prep IN ADP \n",
      "the the det DT DET \n",
      "final final amod JJ ADJ \n",
      "interview interview compound NN NOUN \n",
      "round round pobj NN NOUN \n",
      "of of prep IN ADP \n",
      "NTSE NTSE pobj NNP PROPN \n",
      "2010 2010 nummod CD NUM DATE\n",
      ", , punct , PUNCT \n",
      "out out prep IN ADP \n",
      "of of prep IN ADP \n",
      "the the det DT DET \n",
      "300,000 300,000 nummod CD NUM CARDINAL\n",
      "applicants applicant pobj NNS NOUN \n",
      ". . punct . PUNCT \n",
      "Relevant Relevant compound NNP PROPN \n",
      "Courses Courses compound NNP PROPN \n",
      "Data Data compound NNP PROPN \n",
      "Structure Structure ROOT NNP PROPN \n",
      "and and cc CC CCONJ \n",
      "Algorithm Algorithm compound NNP PROPN \n",
      "Machine Machine compound NNP PROPN \n",
      "Learning Learning compound NNP PROPN \n",
      "Techniques Techniques compound NNPS PROPN \n",
      "Probabilistic Probabilistic compound NNP PROPN \n",
      "Machine Machine compound NNP PROPN \n",
      "Learning Learning compound NNP PROPN \n",
      "Regression Regression compound NNP PROPN \n",
      "Analysis Analysis compound NNP PROPN \n",
      "Applied apply compound VBN VERB \n",
      "Stochastic Stochastic compound NNP PROPN \n",
      "Processes Processes compound NNP PROPN \n",
      "Applied Applied compound NNP PROPN \n",
      "Game Game compound NNP PROPN \n",
      "Theory Theory nmod NNP PROPN \n",
      "Probability Probability conj NNP PROPN \n",
      "and and cc CC CCONJ \n",
      "Statistics Statistics conj NNPS PROPN \n",
      "Inference Inference compound NNP PROPN \n",
      "Time Time compound NNP PROPN \n",
      "Series Series compound NNP PROPN \n",
      "Analysis Analysis compound NNP PROPN \n",
      "Natural Natural compound NNP PROPN \n",
      "Language Language compound NNP PROPN \n",
      "Processing Processing compound NNP PROPN \n",
      "Statistical Statistical compound NNP PROPN \n",
      "Simulation Simulation conj NNP PROPN \n",
      "& & cc CC CCONJ \n",
      "Data Data compound NNP PROPN \n",
      "Analysis Analysis compound NNP PROPN \n",
      "Data Data compound NNP PROPN \n",
      "Mining Mining conj NNP PROPN \n",
      "* * punct NFP PUNCT \n",
      "* * punct NFP PUNCT \n",
      "On on prep IN ADP \n",
      "going go pcomp VBG VERB \n",
      "Courses Courses compound NNPS PROPN ORG\n",
      "Positions Positions dobj NNPS PROPN ORG\n",
      "of of prep IN ADP ORG\n",
      "Responsibility Responsibility compound NNP PROPN ORG\n",
      "President President pobj NNP PROPN \n",
      ", , punct , PUNCT \n",
      "BloodConnect BloodConnect conj NNP PROPN PERSON\n",
      ", , punct , PUNCT \n",
      "Kanpur Kanpur compound NNP PROPN ORG\n",
      "Coordinator Coordinator conj NNP PROPN ORG\n",
      ", , punct , PUNCT \n",
      "Fine Fine compound NNP PROPN ORG\n",
      "Arts Arts compound NNP PROPN ORG\n",
      "Club Club conj NNP PROPN ORG\n",
      ", , punct , PUNCT \n",
      "IITK IITK compound NNP PROPN \n",
      "Student Student compound NNP PROPN \n",
      "coordinator coordinator conj NN NOUN \n",
      ", , punct , PUNCT \n",
      "Raktarpan Raktarpan conj NNP PROPN PERSON\n",
      ", , punct , PUNCT \n",
      "NSS NSS compound NNP PROPN \n",
      "Extra Extra compound NNP PROPN \n",
      "Curricular Curricular compound NNP PROPN \n",
      "Activities Activities compound NNPS PROPN \n",
      "Mentored Mentored ROOT NNP PROPN \n",
      "4 4 nummod CD NUM \n",
      "underprivileged underprivileged amod JJ ADJ \n",
      "students student appos NNS NOUN \n",
      "of of prep IN ADP \n",
      "class class compound NN NOUN \n",
      "IX IX pobj NNP PROPN \n",
      "under under prep IN ADP \n",
      "the the det DT DET \n",
      "program program pobj NN NOUN \n",
      "NSS NSS appos NNP PROPN ORG\n",
      ". . punct . PUNCT \n",
      "Was be ROOT VBD AUX \n",
      "part part attr NN NOUN \n",
      "of of prep IN ADP \n",
      "team team pobj NN NOUN \n",
      "of of prep IN ADP \n",
      "12 12 nummod CD NUM CARDINAL\n",
      "members member pobj NNS NOUN \n",
      ", , punct , PUNCT \n",
      "who who nsubj WP PRON \n",
      "organized organize relcl VBD VERB \n",
      "and and cc CC CCONJ \n",
      "went go conj VBD VERB \n",
      "for for prep IN ADP \n",
      "a a det DT DET \n",
      "week week nmod NN NOUN \n",
      "long long amod JJ ADJ \n",
      "trek trek pobj NN NOUN \n",
      "to to prep IN ADP \n",
      "the the det DT DET \n",
      "Chandrashila Chandrashila compound NNP PROPN PERSON\n",
      "peak peak pobj NN NOUN \n",
      "( ( punct -LRB- PUNCT \n",
      "Uttarakhand Uttarakhand appos NNP PROPN ORG\n",
      ") ) punct -RRB- PUNCT \n",
      "and and cc CC CCONJ \n",
      "back back conj RB ADV \n",
      ". . punct . PUNCT \n",
      "Arranged arrange amod VBN VERB \n",
      "donation donation ROOT NN NOUN \n",
      "for for prep IN ADP \n",
      "underprivileged underprivileged amod JJ ADJ \n",
      "kids kid pobj NNS NOUN \n",
      "as as prep IN ADP \n",
      "social social amod JJ ADJ \n",
      "initiative initiative pobj NN NOUN \n",
      "by by prep IN ADP \n",
      "Inter Inter compound NNP PROPN ORG\n",
      "IIT IIT compound NNP PROPN ORG\n",
      "Sports Sports compound NNPS PROPN ORG\n",
      "Meet’16 Meet’16 pobj NNP PROPN \n",
      ". . punct . PUNCT \n",
      "Performed perform ROOT VBN VERB \n",
      "and and cc CC CCONJ \n",
      "mentored mentor conj VBD VERB \n",
      "Sand Sand compound NNP PROPN \n",
      "Art Art compound NNP PROPN \n",
      "performance performance dobj NN NOUN \n",
      "during during prep IN ADP \n",
      "various various amod JJ ADJ \n",
      "institute institute compound NN NOUN \n",
      "functions function pobj NNS NOUN \n",
      "like like prep IN ADP \n",
      "Freshers Freshers poss NNP PROPN ORG\n",
      "’ ’ case POS PART \n",
      "night night nmod NN NOUN \n",
      "and and cc CC CCONJ \n",
      "Suicide Suicide conj NNP PROPN \n",
      "Prevention Prevention compound NNP PROPN \n",
      "Day Day pobj NNP PROPN \n",
      ". . punct . PUNCT \n"
     ]
    }
   ],
   "source": [
    "for i in tokens[0]:\n",
    "    print(i,i.lemma_,i.dep_,i.tag_,i.pos_,i.ent_type_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "d3cc5ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 CHARU BANSAL 5th Year Undergraduate Mb: +91 7080024445 Department of Mathematics & Scientific Computing, with Minor in Machine Learning and Applications Email: charub@iitk.ac.in Indian Institute of Technology Kanpur Technical Skills Programming languages C/C++, HTML/CSS, R, Python, MATLAB Operating System Windows, Linux Educational Qualifications Year Degree/Board Institute/School CGPA/% 2018 2019 M.S. (MTH) IIT Kanpur 8.0* 2014 2018 B.S. (MTH) IIT Kanpur 6.9 2014 AISSCE (CBSE XIIth Board)\n",
      "1 Jai Academy, Jhansi 92.8 2012 AISSE (CBSE Xth Board)\n",
      "2 Jai Academy, Jhansi 10.0 *CGPA at the end of 8th Semester Experience Summer Intern @Accenture Digital (Project on Fraud Detection in Procurement Analysis)\n",
      "3 [May’18 Jul’18] Performed Outlier Detection on Procurement Data using unsupervised k NN density approach, Local Outlier Factor, k means Clustering and one class SVM in R. Built a User Interface in R Shiny, to integrate all the processes for the user.\n",
      "4 The platform provides the user with the option of selecting and filtering data, choosing the model and its hyperparameters, and view interactive plots of the required variables.\n",
      "5 Tested the program on six specific use cases of fraud, for comparing models and analyzing their effectiveness.\n",
      "6 Probabilistic Machine Learning Project on ‘Zero Shot Learning’\n",
      "7 [Aug’17 Nov’17] Project under Prof. Piyush Rai, Computer Science Department, IIT Kanpur Predicted the distribution parameters of unseen classes using regression on distribution parameters of seen classes.\n",
      "8 Employed and compared Multivariate Regression Tree and Gaussian Process Regression for prediction.\n",
      "9 Attempted Generalised Zero Shot Learning by sampling data from the predicted distribution of unseen classes.\n",
      "10 Natural Language Processing Project on ‘Sentiment Analysis on Tweets’ [Jan’18 Apr’18] Project under Prof. Harish Karnick, Computer Science Department, IIT Kanpur Analyzed and classified sentiments of tweets into positive and negative.\n",
      "11 Used the BBoW, tf, tfidf, Word2Vec, & GLoVE representations for feature extraction, after their tokenization and pre processing.\n",
      "12 Classified the tweets using Naïve Bayes, Logistic Regression, SVM, Feed Forward Neural Net (Multi Layer Perceptron) and LSTM.\n",
      "13 Time Series Analysis Project on ‘GDP Forecasting’\n",
      "14 [Aug’17 Nov’17] Project under Prof. Amit Mitra, Mathematics and Statistics Department, IIT Kanpur Analyzed for trend, seasonality, stationarity, and predicted GDP based on its time series, in R Implemented the ARIMA model, Holt Winters Smoothing, Augmented Dickey Fuller, KPSS, & Ljung Box tests for further analysis.\n",
      "15 Regression Analysis Project on ‘Statistical Modelling of Housing Prices’\n",
      "16 [Jan’17 Apr’17] Project under Prof.\n",
      "17 Sharmishtha Mitra, Mathematics and Statistics Department, IIT Kanpur Designed a consistent Linear Multiple Regression Model to predict Housing Prices; employing a series of steps; fitting the usual Ordinary Least Squares model, residual analysis, checking multicollinearity and variable selection; for the process.\n",
      "18 Statistical Simulation and Data Analysis Project on ‘Identifying Authenticity of Currency Notes’\n",
      "19 [Jan’18 Apr’18] Project under Prof. Debasis Kundu, Mathematics and Statistics Department, IIT Kanpur Modelled the Swiss bank data set on real and fake currencies with a two component Gaussian Mixture Model using Expectation Maximization algorithm, employed AIC/BIC scores to decide the shape and correlation structure of the clusters.\n",
      "20 Checked and dismissed the requirement of k means parameter initialization and soft clustering classification.\n",
      "21 Machine Learning Project on ‘E mail Spam Filtering’ [Aug’16 Nov’16] Project under Prof. Piyush Rai, Computer Science Department, IIT Kanpur Explored different classifiers for spam filtering and compared the results obtained to get a fair and comparative idea about the accuracy of various learning algorithms.\n",
      "22 Scholastic Achievements Secured AIR 1496 in JEE Advanced 2014 out of the top 150,000 applicants selected in JEE Mains 2014.\n",
      "23 Secured AIR 838 in GATE 2018 in Mathematics.\n",
      "24 Selected among the top 1500 students qualified for the final interview round of NTSE 2010, out of the 300,000 applicants.\n",
      "25 Relevant Courses Data Structure and Algorithm Machine Learning Techniques Probabilistic Machine Learning Regression Analysis Applied Stochastic Processes Applied Game Theory Probability and Statistics Inference Time Series Analysis Natural Language Processing Statistical Simulation & Data Analysis Data Mining* *\n",
      "26 On going Courses Positions of Responsibility President, BloodConnect, Kanpur Coordinator, Fine Arts Club, IITK Student coordinator, Raktarpan, NSS Extra Curricular Activities Mentored 4 underprivileged students of class IX under the program NSS.\n",
      "27 Was part of team of 12 members, who organized and went for a week long trek to the Chandrashila peak (Uttarakhand) and back.\n",
      "28 Arranged donation for underprivileged kids as social initiative by Inter IIT Sports Meet’16.\n",
      "29 Performed and mentored Sand Art performance during various institute functions like Freshers’ night and Suicide Prevention Day.\n"
     ]
    }
   ],
   "source": [
    "for index, sentence in enumerate(spacy_doc[0].sents):\n",
    "    print (index, sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "17678c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern1 = [{'IS_ALPHA': True, 'OP':'*'},{'IS_PUNCT': True, 'OP':'*'},{'LOWER': 'skill'},{'IS_ALPHA': True, 'OP':'*'}]\n",
    "pattern2 = [{'IS_ALPHA': True, 'OP':'*'},{'IS_PUNCT': True, 'OP':'*'},{'LOWER': 'skills'},{'IS_ALPHA': True, 'OP':'*'}]\n",
    "matcher.add('skills', [pattern1, pattern2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "f47668d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "found_matches = []\n",
    "for i in range(len(spacy_doc)):\n",
    "    found_matches.append(matcher(spacy_doc[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "68082c9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(12917368283846924199, 26, 33),\n",
       "  (12917368283846924199, 27, 33),\n",
       "  (12917368283846924199, 28, 33),\n",
       "  (12917368283846924199, 29, 33),\n",
       "  (12917368283846924199, 30, 33),\n",
       "  (12917368283846924199, 31, 33),\n",
       "  (12917368283846924199, 32, 33),\n",
       "  (12917368283846924199, 26, 34),\n",
       "  (12917368283846924199, 27, 34),\n",
       "  (12917368283846924199, 28, 34),\n",
       "  (12917368283846924199, 29, 34),\n",
       "  (12917368283846924199, 30, 34),\n",
       "  (12917368283846924199, 31, 34),\n",
       "  (12917368283846924199, 32, 34),\n",
       "  (12917368283846924199, 26, 35),\n",
       "  (12917368283846924199, 27, 35),\n",
       "  (12917368283846924199, 28, 35),\n",
       "  (12917368283846924199, 29, 35),\n",
       "  (12917368283846924199, 30, 35),\n",
       "  (12917368283846924199, 31, 35),\n",
       "  (12917368283846924199, 32, 35),\n",
       "  (12917368283846924199, 26, 36),\n",
       "  (12917368283846924199, 27, 36),\n",
       "  (12917368283846924199, 28, 36),\n",
       "  (12917368283846924199, 29, 36),\n",
       "  (12917368283846924199, 30, 36),\n",
       "  (12917368283846924199, 31, 36),\n",
       "  (12917368283846924199, 32, 36)],\n",
       " [],\n",
       " [(12917368283846924199, 37, 49),\n",
       "  (12917368283846924199, 38, 49),\n",
       "  (12917368283846924199, 39, 49),\n",
       "  (12917368283846924199, 40, 49),\n",
       "  (12917368283846924199, 41, 49),\n",
       "  (12917368283846924199, 42, 49),\n",
       "  (12917368283846924199, 43, 49),\n",
       "  (12917368283846924199, 44, 49),\n",
       "  (12917368283846924199, 45, 49),\n",
       "  (12917368283846924199, 46, 49),\n",
       "  (12917368283846924199, 47, 49),\n",
       "  (12917368283846924199, 48, 49)],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [(12917368283846924199, 150, 152),\n",
       "  (12917368283846924199, 151, 152),\n",
       "  (12917368283846924199, 150, 153),\n",
       "  (12917368283846924199, 151, 153)],\n",
       " [(12917368283846924199, 35, 53),\n",
       "  (12917368283846924199, 36, 53),\n",
       "  (12917368283846924199, 37, 53),\n",
       "  (12917368283846924199, 38, 53),\n",
       "  (12917368283846924199, 39, 53),\n",
       "  (12917368283846924199, 40, 53),\n",
       "  (12917368283846924199, 41, 53),\n",
       "  (12917368283846924199, 42, 53),\n",
       "  (12917368283846924199, 43, 53),\n",
       "  (12917368283846924199, 44, 53),\n",
       "  (12917368283846924199, 45, 53),\n",
       "  (12917368283846924199, 46, 53),\n",
       "  (12917368283846924199, 47, 53),\n",
       "  (12917368283846924199, 48, 53),\n",
       "  (12917368283846924199, 49, 53),\n",
       "  (12917368283846924199, 50, 53),\n",
       "  (12917368283846924199, 51, 53),\n",
       "  (12917368283846924199, 52, 53),\n",
       "  (12917368283846924199, 35, 54),\n",
       "  (12917368283846924199, 36, 54),\n",
       "  (12917368283846924199, 37, 54),\n",
       "  (12917368283846924199, 38, 54),\n",
       "  (12917368283846924199, 39, 54),\n",
       "  (12917368283846924199, 40, 54),\n",
       "  (12917368283846924199, 41, 54),\n",
       "  (12917368283846924199, 42, 54),\n",
       "  (12917368283846924199, 43, 54),\n",
       "  (12917368283846924199, 44, 54),\n",
       "  (12917368283846924199, 45, 54),\n",
       "  (12917368283846924199, 46, 54),\n",
       "  (12917368283846924199, 47, 54),\n",
       "  (12917368283846924199, 48, 54),\n",
       "  (12917368283846924199, 49, 54),\n",
       "  (12917368283846924199, 50, 54),\n",
       "  (12917368283846924199, 51, 54),\n",
       "  (12917368283846924199, 52, 54),\n",
       "  (12917368283846924199, 35, 55),\n",
       "  (12917368283846924199, 36, 55),\n",
       "  (12917368283846924199, 37, 55),\n",
       "  (12917368283846924199, 38, 55),\n",
       "  (12917368283846924199, 39, 55),\n",
       "  (12917368283846924199, 40, 55),\n",
       "  (12917368283846924199, 41, 55),\n",
       "  (12917368283846924199, 42, 55),\n",
       "  (12917368283846924199, 43, 55),\n",
       "  (12917368283846924199, 44, 55),\n",
       "  (12917368283846924199, 45, 55),\n",
       "  (12917368283846924199, 46, 55),\n",
       "  (12917368283846924199, 47, 55),\n",
       "  (12917368283846924199, 48, 55),\n",
       "  (12917368283846924199, 49, 55),\n",
       "  (12917368283846924199, 50, 55),\n",
       "  (12917368283846924199, 51, 55),\n",
       "  (12917368283846924199, 52, 55),\n",
       "  (12917368283846924199, 35, 56),\n",
       "  (12917368283846924199, 36, 56),\n",
       "  (12917368283846924199, 37, 56),\n",
       "  (12917368283846924199, 38, 56),\n",
       "  (12917368283846924199, 39, 56),\n",
       "  (12917368283846924199, 40, 56),\n",
       "  (12917368283846924199, 41, 56),\n",
       "  (12917368283846924199, 42, 56),\n",
       "  (12917368283846924199, 43, 56),\n",
       "  (12917368283846924199, 44, 56),\n",
       "  (12917368283846924199, 45, 56),\n",
       "  (12917368283846924199, 46, 56),\n",
       "  (12917368283846924199, 47, 56),\n",
       "  (12917368283846924199, 48, 56),\n",
       "  (12917368283846924199, 49, 56),\n",
       "  (12917368283846924199, 50, 56),\n",
       "  (12917368283846924199, 51, 56),\n",
       "  (12917368283846924199, 52, 56),\n",
       "  (12917368283846924199, 35, 57),\n",
       "  (12917368283846924199, 36, 57),\n",
       "  (12917368283846924199, 37, 57),\n",
       "  (12917368283846924199, 38, 57),\n",
       "  (12917368283846924199, 39, 57),\n",
       "  (12917368283846924199, 40, 57),\n",
       "  (12917368283846924199, 41, 57),\n",
       "  (12917368283846924199, 42, 57),\n",
       "  (12917368283846924199, 43, 57),\n",
       "  (12917368283846924199, 44, 57),\n",
       "  (12917368283846924199, 45, 57),\n",
       "  (12917368283846924199, 46, 57),\n",
       "  (12917368283846924199, 47, 57),\n",
       "  (12917368283846924199, 48, 57),\n",
       "  (12917368283846924199, 49, 57),\n",
       "  (12917368283846924199, 50, 57),\n",
       "  (12917368283846924199, 51, 57),\n",
       "  (12917368283846924199, 52, 57),\n",
       "  (12917368283846924199, 35, 58),\n",
       "  (12917368283846924199, 36, 58),\n",
       "  (12917368283846924199, 37, 58),\n",
       "  (12917368283846924199, 38, 58),\n",
       "  (12917368283846924199, 39, 58),\n",
       "  (12917368283846924199, 40, 58),\n",
       "  (12917368283846924199, 41, 58),\n",
       "  (12917368283846924199, 42, 58),\n",
       "  (12917368283846924199, 43, 58),\n",
       "  (12917368283846924199, 44, 58),\n",
       "  (12917368283846924199, 45, 58),\n",
       "  (12917368283846924199, 46, 58),\n",
       "  (12917368283846924199, 47, 58),\n",
       "  (12917368283846924199, 48, 58),\n",
       "  (12917368283846924199, 49, 58),\n",
       "  (12917368283846924199, 50, 58),\n",
       "  (12917368283846924199, 51, 58),\n",
       "  (12917368283846924199, 52, 58),\n",
       "  (12917368283846924199, 35, 59),\n",
       "  (12917368283846924199, 36, 59),\n",
       "  (12917368283846924199, 37, 59),\n",
       "  (12917368283846924199, 38, 59),\n",
       "  (12917368283846924199, 39, 59),\n",
       "  (12917368283846924199, 40, 59),\n",
       "  (12917368283846924199, 41, 59),\n",
       "  (12917368283846924199, 42, 59),\n",
       "  (12917368283846924199, 43, 59),\n",
       "  (12917368283846924199, 44, 59),\n",
       "  (12917368283846924199, 45, 59),\n",
       "  (12917368283846924199, 46, 59),\n",
       "  (12917368283846924199, 47, 59),\n",
       "  (12917368283846924199, 48, 59),\n",
       "  (12917368283846924199, 49, 59),\n",
       "  (12917368283846924199, 50, 59),\n",
       "  (12917368283846924199, 51, 59),\n",
       "  (12917368283846924199, 52, 59),\n",
       "  (12917368283846924199, 35, 60),\n",
       "  (12917368283846924199, 36, 60),\n",
       "  (12917368283846924199, 37, 60),\n",
       "  (12917368283846924199, 38, 60),\n",
       "  (12917368283846924199, 39, 60),\n",
       "  (12917368283846924199, 40, 60),\n",
       "  (12917368283846924199, 41, 60),\n",
       "  (12917368283846924199, 42, 60),\n",
       "  (12917368283846924199, 43, 60),\n",
       "  (12917368283846924199, 44, 60),\n",
       "  (12917368283846924199, 45, 60),\n",
       "  (12917368283846924199, 46, 60),\n",
       "  (12917368283846924199, 47, 60),\n",
       "  (12917368283846924199, 48, 60),\n",
       "  (12917368283846924199, 49, 60),\n",
       "  (12917368283846924199, 50, 60),\n",
       "  (12917368283846924199, 51, 60),\n",
       "  (12917368283846924199, 52, 60),\n",
       "  (12917368283846924199, 35, 61),\n",
       "  (12917368283846924199, 36, 61),\n",
       "  (12917368283846924199, 37, 61),\n",
       "  (12917368283846924199, 38, 61),\n",
       "  (12917368283846924199, 39, 61),\n",
       "  (12917368283846924199, 40, 61),\n",
       "  (12917368283846924199, 41, 61),\n",
       "  (12917368283846924199, 42, 61),\n",
       "  (12917368283846924199, 43, 61),\n",
       "  (12917368283846924199, 44, 61),\n",
       "  (12917368283846924199, 45, 61),\n",
       "  (12917368283846924199, 46, 61),\n",
       "  (12917368283846924199, 47, 61),\n",
       "  (12917368283846924199, 48, 61),\n",
       "  (12917368283846924199, 49, 61),\n",
       "  (12917368283846924199, 50, 61),\n",
       "  (12917368283846924199, 51, 61),\n",
       "  (12917368283846924199, 52, 61),\n",
       "  (12917368283846924199, 35, 62),\n",
       "  (12917368283846924199, 36, 62),\n",
       "  (12917368283846924199, 37, 62),\n",
       "  (12917368283846924199, 38, 62),\n",
       "  (12917368283846924199, 39, 62),\n",
       "  (12917368283846924199, 40, 62),\n",
       "  (12917368283846924199, 41, 62),\n",
       "  (12917368283846924199, 42, 62),\n",
       "  (12917368283846924199, 43, 62),\n",
       "  (12917368283846924199, 44, 62),\n",
       "  (12917368283846924199, 45, 62),\n",
       "  (12917368283846924199, 46, 62),\n",
       "  (12917368283846924199, 47, 62),\n",
       "  (12917368283846924199, 48, 62),\n",
       "  (12917368283846924199, 49, 62),\n",
       "  (12917368283846924199, 50, 62),\n",
       "  (12917368283846924199, 51, 62),\n",
       "  (12917368283846924199, 52, 62),\n",
       "  (12917368283846924199, 144, 148),\n",
       "  (12917368283846924199, 145, 148),\n",
       "  (12917368283846924199, 146, 148),\n",
       "  (12917368283846924199, 147, 148)],\n",
       " [(12917368283846924199, 1, 7),\n",
       "  (12917368283846924199, 2, 7),\n",
       "  (12917368283846924199, 3, 7),\n",
       "  (12917368283846924199, 4, 7),\n",
       "  (12917368283846924199, 5, 7),\n",
       "  (12917368283846924199, 6, 7),\n",
       "  (12917368283846924199, 1, 8),\n",
       "  (12917368283846924199, 2, 8),\n",
       "  (12917368283846924199, 3, 8),\n",
       "  (12917368283846924199, 4, 8),\n",
       "  (12917368283846924199, 5, 8),\n",
       "  (12917368283846924199, 6, 8),\n",
       "  (12917368283846924199, 1, 9),\n",
       "  (12917368283846924199, 2, 9),\n",
       "  (12917368283846924199, 3, 9),\n",
       "  (12917368283846924199, 4, 9),\n",
       "  (12917368283846924199, 5, 9),\n",
       "  (12917368283846924199, 6, 9),\n",
       "  (12917368283846924199, 1, 10),\n",
       "  (12917368283846924199, 2, 10),\n",
       "  (12917368283846924199, 3, 10),\n",
       "  (12917368283846924199, 4, 10),\n",
       "  (12917368283846924199, 5, 10),\n",
       "  (12917368283846924199, 6, 10),\n",
       "  (12917368283846924199, 1, 11),\n",
       "  (12917368283846924199, 2, 11),\n",
       "  (12917368283846924199, 3, 11),\n",
       "  (12917368283846924199, 4, 11),\n",
       "  (12917368283846924199, 5, 11),\n",
       "  (12917368283846924199, 6, 11),\n",
       "  (12917368283846924199, 1, 12),\n",
       "  (12917368283846924199, 2, 12),\n",
       "  (12917368283846924199, 3, 12),\n",
       "  (12917368283846924199, 4, 12),\n",
       "  (12917368283846924199, 5, 12),\n",
       "  (12917368283846924199, 6, 12),\n",
       "  (12917368283846924199, 1, 13),\n",
       "  (12917368283846924199, 2, 13),\n",
       "  (12917368283846924199, 3, 13),\n",
       "  (12917368283846924199, 4, 13),\n",
       "  (12917368283846924199, 5, 13),\n",
       "  (12917368283846924199, 6, 13),\n",
       "  (12917368283846924199, 1, 14),\n",
       "  (12917368283846924199, 2, 14),\n",
       "  (12917368283846924199, 3, 14),\n",
       "  (12917368283846924199, 4, 14),\n",
       "  (12917368283846924199, 5, 14),\n",
       "  (12917368283846924199, 6, 14),\n",
       "  (12917368283846924199, 1, 15),\n",
       "  (12917368283846924199, 2, 15),\n",
       "  (12917368283846924199, 3, 15),\n",
       "  (12917368283846924199, 4, 15),\n",
       "  (12917368283846924199, 5, 15),\n",
       "  (12917368283846924199, 6, 15),\n",
       "  (12917368283846924199, 1, 16),\n",
       "  (12917368283846924199, 2, 16),\n",
       "  (12917368283846924199, 3, 16),\n",
       "  (12917368283846924199, 4, 16),\n",
       "  (12917368283846924199, 5, 16),\n",
       "  (12917368283846924199, 6, 16),\n",
       "  (12917368283846924199, 1, 17),\n",
       "  (12917368283846924199, 2, 17),\n",
       "  (12917368283846924199, 3, 17),\n",
       "  (12917368283846924199, 4, 17),\n",
       "  (12917368283846924199, 5, 17),\n",
       "  (12917368283846924199, 6, 17),\n",
       "  (12917368283846924199, 1, 18),\n",
       "  (12917368283846924199, 2, 18),\n",
       "  (12917368283846924199, 3, 18),\n",
       "  (12917368283846924199, 4, 18),\n",
       "  (12917368283846924199, 5, 18),\n",
       "  (12917368283846924199, 6, 18),\n",
       "  (12917368283846924199, 1, 19),\n",
       "  (12917368283846924199, 2, 19),\n",
       "  (12917368283846924199, 3, 19),\n",
       "  (12917368283846924199, 4, 19),\n",
       "  (12917368283846924199, 5, 19),\n",
       "  (12917368283846924199, 6, 19)],\n",
       " [(12917368283846924199, 666, 678),\n",
       "  (12917368283846924199, 667, 678),\n",
       "  (12917368283846924199, 668, 678),\n",
       "  (12917368283846924199, 669, 678),\n",
       "  (12917368283846924199, 670, 678),\n",
       "  (12917368283846924199, 671, 678),\n",
       "  (12917368283846924199, 672, 678),\n",
       "  (12917368283846924199, 673, 678),\n",
       "  (12917368283846924199, 674, 678),\n",
       "  (12917368283846924199, 675, 678),\n",
       "  (12917368283846924199, 676, 678),\n",
       "  (12917368283846924199, 677, 678),\n",
       "  (12917368283846924199, 666, 679),\n",
       "  (12917368283846924199, 667, 679),\n",
       "  (12917368283846924199, 668, 679),\n",
       "  (12917368283846924199, 669, 679),\n",
       "  (12917368283846924199, 670, 679),\n",
       "  (12917368283846924199, 671, 679),\n",
       "  (12917368283846924199, 672, 679),\n",
       "  (12917368283846924199, 673, 679),\n",
       "  (12917368283846924199, 674, 679),\n",
       "  (12917368283846924199, 675, 679),\n",
       "  (12917368283846924199, 676, 679),\n",
       "  (12917368283846924199, 677, 679),\n",
       "  (12917368283846924199, 666, 680),\n",
       "  (12917368283846924199, 667, 680),\n",
       "  (12917368283846924199, 668, 680),\n",
       "  (12917368283846924199, 669, 680),\n",
       "  (12917368283846924199, 670, 680),\n",
       "  (12917368283846924199, 671, 680),\n",
       "  (12917368283846924199, 672, 680),\n",
       "  (12917368283846924199, 673, 680),\n",
       "  (12917368283846924199, 674, 680),\n",
       "  (12917368283846924199, 675, 680),\n",
       "  (12917368283846924199, 676, 680),\n",
       "  (12917368283846924199, 677, 680),\n",
       "  (12917368283846924199, 666, 681),\n",
       "  (12917368283846924199, 667, 681),\n",
       "  (12917368283846924199, 668, 681),\n",
       "  (12917368283846924199, 669, 681),\n",
       "  (12917368283846924199, 670, 681),\n",
       "  (12917368283846924199, 671, 681),\n",
       "  (12917368283846924199, 672, 681),\n",
       "  (12917368283846924199, 673, 681),\n",
       "  (12917368283846924199, 674, 681),\n",
       "  (12917368283846924199, 675, 681),\n",
       "  (12917368283846924199, 676, 681),\n",
       "  (12917368283846924199, 677, 681),\n",
       "  (12917368283846924199, 666, 682),\n",
       "  (12917368283846924199, 667, 682),\n",
       "  (12917368283846924199, 668, 682),\n",
       "  (12917368283846924199, 669, 682),\n",
       "  (12917368283846924199, 670, 682),\n",
       "  (12917368283846924199, 671, 682),\n",
       "  (12917368283846924199, 672, 682),\n",
       "  (12917368283846924199, 673, 682),\n",
       "  (12917368283846924199, 674, 682),\n",
       "  (12917368283846924199, 675, 682),\n",
       "  (12917368283846924199, 676, 682),\n",
       "  (12917368283846924199, 677, 682)],\n",
       " [(12917368283846924199, 57, 58),\n",
       "  (12917368283846924199, 57, 59),\n",
       "  (12917368283846924199, 57, 60),\n",
       "  (12917368283846924199, 57, 61),\n",
       "  (12917368283846924199, 57, 62)],\n",
       " [(12917368283846924199, 34, 35),\n",
       "  (12917368283846924199, 34, 36),\n",
       "  (12917368283846924199, 34, 37)],\n",
       " [(12917368283846924199, 12, 14),\n",
       "  (12917368283846924199, 13, 14),\n",
       "  (12917368283846924199, 12, 15),\n",
       "  (12917368283846924199, 13, 15),\n",
       "  (12917368283846924199, 12, 16),\n",
       "  (12917368283846924199, 13, 16),\n",
       "  (12917368283846924199, 12, 17),\n",
       "  (12917368283846924199, 13, 17),\n",
       "  (12917368283846924199, 12, 18),\n",
       "  (12917368283846924199, 13, 18)],\n",
       " [],\n",
       " [(12917368283846924199, 27, 50),\n",
       "  (12917368283846924199, 28, 50),\n",
       "  (12917368283846924199, 29, 50),\n",
       "  (12917368283846924199, 30, 50),\n",
       "  (12917368283846924199, 31, 50),\n",
       "  (12917368283846924199, 32, 50),\n",
       "  (12917368283846924199, 33, 50),\n",
       "  (12917368283846924199, 34, 50),\n",
       "  (12917368283846924199, 35, 50),\n",
       "  (12917368283846924199, 36, 50),\n",
       "  (12917368283846924199, 37, 50),\n",
       "  (12917368283846924199, 38, 50),\n",
       "  (12917368283846924199, 39, 50),\n",
       "  (12917368283846924199, 40, 50),\n",
       "  (12917368283846924199, 41, 50),\n",
       "  (12917368283846924199, 42, 50),\n",
       "  (12917368283846924199, 43, 50),\n",
       "  (12917368283846924199, 44, 50),\n",
       "  (12917368283846924199, 45, 50),\n",
       "  (12917368283846924199, 46, 50),\n",
       "  (12917368283846924199, 47, 50),\n",
       "  (12917368283846924199, 48, 50),\n",
       "  (12917368283846924199, 49, 50),\n",
       "  (12917368283846924199, 27, 51),\n",
       "  (12917368283846924199, 28, 51),\n",
       "  (12917368283846924199, 29, 51),\n",
       "  (12917368283846924199, 30, 51),\n",
       "  (12917368283846924199, 31, 51),\n",
       "  (12917368283846924199, 32, 51),\n",
       "  (12917368283846924199, 33, 51),\n",
       "  (12917368283846924199, 34, 51),\n",
       "  (12917368283846924199, 35, 51),\n",
       "  (12917368283846924199, 36, 51),\n",
       "  (12917368283846924199, 37, 51),\n",
       "  (12917368283846924199, 38, 51),\n",
       "  (12917368283846924199, 39, 51),\n",
       "  (12917368283846924199, 40, 51),\n",
       "  (12917368283846924199, 41, 51),\n",
       "  (12917368283846924199, 42, 51),\n",
       "  (12917368283846924199, 43, 51),\n",
       "  (12917368283846924199, 44, 51),\n",
       "  (12917368283846924199, 45, 51),\n",
       "  (12917368283846924199, 46, 51),\n",
       "  (12917368283846924199, 47, 51),\n",
       "  (12917368283846924199, 48, 51),\n",
       "  (12917368283846924199, 49, 51),\n",
       "  (12917368283846924199, 27, 52),\n",
       "  (12917368283846924199, 28, 52),\n",
       "  (12917368283846924199, 29, 52),\n",
       "  (12917368283846924199, 30, 52),\n",
       "  (12917368283846924199, 31, 52),\n",
       "  (12917368283846924199, 32, 52),\n",
       "  (12917368283846924199, 33, 52),\n",
       "  (12917368283846924199, 34, 52),\n",
       "  (12917368283846924199, 35, 52),\n",
       "  (12917368283846924199, 36, 52),\n",
       "  (12917368283846924199, 37, 52),\n",
       "  (12917368283846924199, 38, 52),\n",
       "  (12917368283846924199, 39, 52),\n",
       "  (12917368283846924199, 40, 52),\n",
       "  (12917368283846924199, 41, 52),\n",
       "  (12917368283846924199, 42, 52),\n",
       "  (12917368283846924199, 43, 52),\n",
       "  (12917368283846924199, 44, 52),\n",
       "  (12917368283846924199, 45, 52),\n",
       "  (12917368283846924199, 46, 52),\n",
       "  (12917368283846924199, 47, 52),\n",
       "  (12917368283846924199, 48, 52),\n",
       "  (12917368283846924199, 49, 52),\n",
       "  (12917368283846924199, 27, 53),\n",
       "  (12917368283846924199, 28, 53),\n",
       "  (12917368283846924199, 29, 53),\n",
       "  (12917368283846924199, 30, 53),\n",
       "  (12917368283846924199, 31, 53),\n",
       "  (12917368283846924199, 32, 53),\n",
       "  (12917368283846924199, 33, 53),\n",
       "  (12917368283846924199, 34, 53),\n",
       "  (12917368283846924199, 35, 53),\n",
       "  (12917368283846924199, 36, 53),\n",
       "  (12917368283846924199, 37, 53),\n",
       "  (12917368283846924199, 38, 53),\n",
       "  (12917368283846924199, 39, 53),\n",
       "  (12917368283846924199, 40, 53),\n",
       "  (12917368283846924199, 41, 53),\n",
       "  (12917368283846924199, 42, 53),\n",
       "  (12917368283846924199, 43, 53),\n",
       "  (12917368283846924199, 44, 53),\n",
       "  (12917368283846924199, 45, 53),\n",
       "  (12917368283846924199, 46, 53),\n",
       "  (12917368283846924199, 47, 53),\n",
       "  (12917368283846924199, 48, 53),\n",
       "  (12917368283846924199, 49, 53),\n",
       "  (12917368283846924199, 27, 54),\n",
       "  (12917368283846924199, 28, 54),\n",
       "  (12917368283846924199, 29, 54),\n",
       "  (12917368283846924199, 30, 54),\n",
       "  (12917368283846924199, 31, 54),\n",
       "  (12917368283846924199, 32, 54),\n",
       "  (12917368283846924199, 33, 54),\n",
       "  (12917368283846924199, 34, 54),\n",
       "  (12917368283846924199, 35, 54),\n",
       "  (12917368283846924199, 36, 54),\n",
       "  (12917368283846924199, 37, 54),\n",
       "  (12917368283846924199, 38, 54),\n",
       "  (12917368283846924199, 39, 54),\n",
       "  (12917368283846924199, 40, 54),\n",
       "  (12917368283846924199, 41, 54),\n",
       "  (12917368283846924199, 42, 54),\n",
       "  (12917368283846924199, 43, 54),\n",
       "  (12917368283846924199, 44, 54),\n",
       "  (12917368283846924199, 45, 54),\n",
       "  (12917368283846924199, 46, 54),\n",
       "  (12917368283846924199, 47, 54),\n",
       "  (12917368283846924199, 48, 54),\n",
       "  (12917368283846924199, 49, 54),\n",
       "  (12917368283846924199, 27, 55),\n",
       "  (12917368283846924199, 28, 55),\n",
       "  (12917368283846924199, 29, 55),\n",
       "  (12917368283846924199, 30, 55),\n",
       "  (12917368283846924199, 31, 55),\n",
       "  (12917368283846924199, 32, 55),\n",
       "  (12917368283846924199, 33, 55),\n",
       "  (12917368283846924199, 34, 55),\n",
       "  (12917368283846924199, 35, 55),\n",
       "  (12917368283846924199, 36, 55),\n",
       "  (12917368283846924199, 37, 55),\n",
       "  (12917368283846924199, 38, 55),\n",
       "  (12917368283846924199, 39, 55),\n",
       "  (12917368283846924199, 40, 55),\n",
       "  (12917368283846924199, 41, 55),\n",
       "  (12917368283846924199, 42, 55),\n",
       "  (12917368283846924199, 43, 55),\n",
       "  (12917368283846924199, 44, 55),\n",
       "  (12917368283846924199, 45, 55),\n",
       "  (12917368283846924199, 46, 55),\n",
       "  (12917368283846924199, 47, 55),\n",
       "  (12917368283846924199, 48, 55),\n",
       "  (12917368283846924199, 49, 55),\n",
       "  (12917368283846924199, 27, 56),\n",
       "  (12917368283846924199, 28, 56),\n",
       "  (12917368283846924199, 29, 56),\n",
       "  (12917368283846924199, 30, 56),\n",
       "  (12917368283846924199, 31, 56),\n",
       "  (12917368283846924199, 32, 56),\n",
       "  (12917368283846924199, 33, 56),\n",
       "  (12917368283846924199, 34, 56),\n",
       "  (12917368283846924199, 35, 56),\n",
       "  (12917368283846924199, 36, 56),\n",
       "  (12917368283846924199, 37, 56),\n",
       "  (12917368283846924199, 38, 56),\n",
       "  (12917368283846924199, 39, 56),\n",
       "  (12917368283846924199, 40, 56),\n",
       "  (12917368283846924199, 41, 56),\n",
       "  (12917368283846924199, 42, 56),\n",
       "  (12917368283846924199, 43, 56),\n",
       "  (12917368283846924199, 44, 56),\n",
       "  (12917368283846924199, 45, 56),\n",
       "  (12917368283846924199, 46, 56),\n",
       "  (12917368283846924199, 47, 56),\n",
       "  (12917368283846924199, 48, 56),\n",
       "  (12917368283846924199, 49, 56),\n",
       "  (12917368283846924199, 27, 57),\n",
       "  (12917368283846924199, 28, 57),\n",
       "  (12917368283846924199, 29, 57),\n",
       "  (12917368283846924199, 30, 57),\n",
       "  (12917368283846924199, 31, 57),\n",
       "  (12917368283846924199, 32, 57),\n",
       "  (12917368283846924199, 33, 57),\n",
       "  (12917368283846924199, 34, 57),\n",
       "  (12917368283846924199, 35, 57),\n",
       "  (12917368283846924199, 36, 57),\n",
       "  (12917368283846924199, 37, 57),\n",
       "  (12917368283846924199, 38, 57),\n",
       "  (12917368283846924199, 39, 57),\n",
       "  (12917368283846924199, 40, 57),\n",
       "  (12917368283846924199, 41, 57),\n",
       "  (12917368283846924199, 42, 57),\n",
       "  (12917368283846924199, 43, 57),\n",
       "  (12917368283846924199, 44, 57),\n",
       "  (12917368283846924199, 45, 57),\n",
       "  (12917368283846924199, 46, 57),\n",
       "  (12917368283846924199, 47, 57),\n",
       "  (12917368283846924199, 48, 57),\n",
       "  (12917368283846924199, 49, 57),\n",
       "  (12917368283846924199, 27, 58),\n",
       "  (12917368283846924199, 28, 58),\n",
       "  (12917368283846924199, 29, 58),\n",
       "  (12917368283846924199, 30, 58),\n",
       "  (12917368283846924199, 31, 58),\n",
       "  (12917368283846924199, 32, 58),\n",
       "  (12917368283846924199, 33, 58),\n",
       "  (12917368283846924199, 34, 58),\n",
       "  (12917368283846924199, 35, 58),\n",
       "  (12917368283846924199, 36, 58),\n",
       "  (12917368283846924199, 37, 58),\n",
       "  (12917368283846924199, 38, 58),\n",
       "  (12917368283846924199, 39, 58),\n",
       "  (12917368283846924199, 40, 58),\n",
       "  (12917368283846924199, 41, 58),\n",
       "  (12917368283846924199, 42, 58),\n",
       "  (12917368283846924199, 43, 58),\n",
       "  (12917368283846924199, 44, 58),\n",
       "  (12917368283846924199, 45, 58),\n",
       "  (12917368283846924199, 46, 58),\n",
       "  (12917368283846924199, 47, 58),\n",
       "  (12917368283846924199, 48, 58),\n",
       "  (12917368283846924199, 49, 58),\n",
       "  (12917368283846924199, 27, 59),\n",
       "  (12917368283846924199, 28, 59),\n",
       "  (12917368283846924199, 29, 59),\n",
       "  (12917368283846924199, 30, 59),\n",
       "  (12917368283846924199, 31, 59),\n",
       "  (12917368283846924199, 32, 59),\n",
       "  (12917368283846924199, 33, 59),\n",
       "  (12917368283846924199, 34, 59),\n",
       "  (12917368283846924199, 35, 59),\n",
       "  (12917368283846924199, 36, 59),\n",
       "  (12917368283846924199, 37, 59),\n",
       "  (12917368283846924199, 38, 59),\n",
       "  (12917368283846924199, 39, 59),\n",
       "  (12917368283846924199, 40, 59),\n",
       "  (12917368283846924199, 41, 59),\n",
       "  (12917368283846924199, 42, 59),\n",
       "  (12917368283846924199, 43, 59),\n",
       "  (12917368283846924199, 44, 59),\n",
       "  (12917368283846924199, 45, 59),\n",
       "  (12917368283846924199, 46, 59),\n",
       "  (12917368283846924199, 47, 59),\n",
       "  (12917368283846924199, 48, 59),\n",
       "  (12917368283846924199, 49, 59),\n",
       "  (12917368283846924199, 27, 60),\n",
       "  (12917368283846924199, 28, 60),\n",
       "  (12917368283846924199, 29, 60),\n",
       "  (12917368283846924199, 30, 60),\n",
       "  (12917368283846924199, 31, 60),\n",
       "  (12917368283846924199, 32, 60),\n",
       "  (12917368283846924199, 33, 60),\n",
       "  (12917368283846924199, 34, 60),\n",
       "  (12917368283846924199, 35, 60),\n",
       "  (12917368283846924199, 36, 60),\n",
       "  (12917368283846924199, 37, 60),\n",
       "  (12917368283846924199, 38, 60),\n",
       "  (12917368283846924199, 39, 60),\n",
       "  (12917368283846924199, 40, 60),\n",
       "  (12917368283846924199, 41, 60),\n",
       "  (12917368283846924199, 42, 60),\n",
       "  (12917368283846924199, 43, 60),\n",
       "  (12917368283846924199, 44, 60),\n",
       "  (12917368283846924199, 45, 60),\n",
       "  (12917368283846924199, 46, 60),\n",
       "  (12917368283846924199, 47, 60),\n",
       "  (12917368283846924199, 48, 60),\n",
       "  (12917368283846924199, 49, 60),\n",
       "  (12917368283846924199, 27, 61),\n",
       "  (12917368283846924199, 28, 61),\n",
       "  (12917368283846924199, 29, 61),\n",
       "  (12917368283846924199, 30, 61),\n",
       "  (12917368283846924199, 31, 61),\n",
       "  (12917368283846924199, 32, 61),\n",
       "  (12917368283846924199, 33, 61),\n",
       "  (12917368283846924199, 34, 61),\n",
       "  (12917368283846924199, 35, 61),\n",
       "  (12917368283846924199, 36, 61),\n",
       "  (12917368283846924199, 37, 61),\n",
       "  (12917368283846924199, 38, 61),\n",
       "  (12917368283846924199, 39, 61),\n",
       "  (12917368283846924199, 40, 61),\n",
       "  (12917368283846924199, 41, 61),\n",
       "  (12917368283846924199, 42, 61),\n",
       "  (12917368283846924199, 43, 61),\n",
       "  (12917368283846924199, 44, 61),\n",
       "  (12917368283846924199, 45, 61),\n",
       "  (12917368283846924199, 46, 61),\n",
       "  (12917368283846924199, 47, 61),\n",
       "  (12917368283846924199, 48, 61),\n",
       "  (12917368283846924199, 49, 61),\n",
       "  (12917368283846924199, 309, 311),\n",
       "  (12917368283846924199, 310, 311)],\n",
       " [],\n",
       " [],\n",
       " [(12917368283846924199, 149, 154),\n",
       "  (12917368283846924199, 150, 154),\n",
       "  (12917368283846924199, 151, 154),\n",
       "  (12917368283846924199, 152, 154),\n",
       "  (12917368283846924199, 153, 154),\n",
       "  (12917368283846924199, 170, 172),\n",
       "  (12917368283846924199, 171, 172),\n",
       "  (12917368283846924199, 2775, 2788),\n",
       "  (12917368283846924199, 2776, 2788),\n",
       "  (12917368283846924199, 2777, 2788),\n",
       "  (12917368283846924199, 2778, 2788),\n",
       "  (12917368283846924199, 2779, 2788),\n",
       "  (12917368283846924199, 2780, 2788),\n",
       "  (12917368283846924199, 2781, 2788),\n",
       "  (12917368283846924199, 2782, 2788),\n",
       "  (12917368283846924199, 2783, 2788),\n",
       "  (12917368283846924199, 2784, 2788),\n",
       "  (12917368283846924199, 2785, 2788),\n",
       "  (12917368283846924199, 2786, 2788),\n",
       "  (12917368283846924199, 2787, 2788),\n",
       "  (12917368283846924199, 2775, 2789),\n",
       "  (12917368283846924199, 2776, 2789),\n",
       "  (12917368283846924199, 2777, 2789),\n",
       "  (12917368283846924199, 2778, 2789),\n",
       "  (12917368283846924199, 2779, 2789),\n",
       "  (12917368283846924199, 2780, 2789),\n",
       "  (12917368283846924199, 2781, 2789),\n",
       "  (12917368283846924199, 2782, 2789),\n",
       "  (12917368283846924199, 2783, 2789),\n",
       "  (12917368283846924199, 2784, 2789),\n",
       "  (12917368283846924199, 2785, 2789),\n",
       "  (12917368283846924199, 2786, 2789),\n",
       "  (12917368283846924199, 2787, 2789),\n",
       "  (12917368283846924199, 2775, 2790),\n",
       "  (12917368283846924199, 2776, 2790),\n",
       "  (12917368283846924199, 2777, 2790),\n",
       "  (12917368283846924199, 2778, 2790),\n",
       "  (12917368283846924199, 2779, 2790),\n",
       "  (12917368283846924199, 2780, 2790),\n",
       "  (12917368283846924199, 2781, 2790),\n",
       "  (12917368283846924199, 2782, 2790),\n",
       "  (12917368283846924199, 2783, 2790),\n",
       "  (12917368283846924199, 2784, 2790),\n",
       "  (12917368283846924199, 2785, 2790),\n",
       "  (12917368283846924199, 2786, 2790),\n",
       "  (12917368283846924199, 2787, 2790),\n",
       "  (12917368283846924199, 2775, 2791),\n",
       "  (12917368283846924199, 2776, 2791),\n",
       "  (12917368283846924199, 2777, 2791),\n",
       "  (12917368283846924199, 2778, 2791),\n",
       "  (12917368283846924199, 2779, 2791),\n",
       "  (12917368283846924199, 2780, 2791),\n",
       "  (12917368283846924199, 2781, 2791),\n",
       "  (12917368283846924199, 2782, 2791),\n",
       "  (12917368283846924199, 2783, 2791),\n",
       "  (12917368283846924199, 2784, 2791),\n",
       "  (12917368283846924199, 2785, 2791),\n",
       "  (12917368283846924199, 2786, 2791),\n",
       "  (12917368283846924199, 2787, 2791),\n",
       "  (12917368283846924199, 2775, 2792),\n",
       "  (12917368283846924199, 2776, 2792),\n",
       "  (12917368283846924199, 2777, 2792),\n",
       "  (12917368283846924199, 2778, 2792),\n",
       "  (12917368283846924199, 2779, 2792),\n",
       "  (12917368283846924199, 2780, 2792),\n",
       "  (12917368283846924199, 2781, 2792),\n",
       "  (12917368283846924199, 2782, 2792),\n",
       "  (12917368283846924199, 2783, 2792),\n",
       "  (12917368283846924199, 2784, 2792),\n",
       "  (12917368283846924199, 2785, 2792),\n",
       "  (12917368283846924199, 2786, 2792),\n",
       "  (12917368283846924199, 2787, 2792),\n",
       "  (12917368283846924199, 2775, 2793),\n",
       "  (12917368283846924199, 2776, 2793),\n",
       "  (12917368283846924199, 2777, 2793),\n",
       "  (12917368283846924199, 2778, 2793),\n",
       "  (12917368283846924199, 2779, 2793),\n",
       "  (12917368283846924199, 2780, 2793),\n",
       "  (12917368283846924199, 2781, 2793),\n",
       "  (12917368283846924199, 2782, 2793),\n",
       "  (12917368283846924199, 2783, 2793),\n",
       "  (12917368283846924199, 2784, 2793),\n",
       "  (12917368283846924199, 2785, 2793),\n",
       "  (12917368283846924199, 2786, 2793),\n",
       "  (12917368283846924199, 2787, 2793),\n",
       "  (12917368283846924199, 4028, 4033),\n",
       "  (12917368283846924199, 4029, 4033),\n",
       "  (12917368283846924199, 4030, 4033),\n",
       "  (12917368283846924199, 4031, 4033),\n",
       "  (12917368283846924199, 4032, 4033),\n",
       "  (12917368283846924199, 4028, 4034),\n",
       "  (12917368283846924199, 4029, 4034),\n",
       "  (12917368283846924199, 4030, 4034),\n",
       "  (12917368283846924199, 4031, 4034),\n",
       "  (12917368283846924199, 4032, 4034)],\n",
       " [(12917368283846924199, 281, 287),\n",
       "  (12917368283846924199, 282, 287),\n",
       "  (12917368283846924199, 283, 287),\n",
       "  (12917368283846924199, 284, 287),\n",
       "  (12917368283846924199, 285, 287),\n",
       "  (12917368283846924199, 286, 287),\n",
       "  (12917368283846924199, 503, 515),\n",
       "  (12917368283846924199, 504, 515),\n",
       "  (12917368283846924199, 505, 515),\n",
       "  (12917368283846924199, 506, 515),\n",
       "  (12917368283846924199, 507, 515),\n",
       "  (12917368283846924199, 508, 515),\n",
       "  (12917368283846924199, 509, 515),\n",
       "  (12917368283846924199, 510, 515),\n",
       "  (12917368283846924199, 511, 515),\n",
       "  (12917368283846924199, 512, 515),\n",
       "  (12917368283846924199, 513, 515),\n",
       "  (12917368283846924199, 514, 515),\n",
       "  (12917368283846924199, 503, 516),\n",
       "  (12917368283846924199, 504, 516),\n",
       "  (12917368283846924199, 505, 516),\n",
       "  (12917368283846924199, 506, 516),\n",
       "  (12917368283846924199, 507, 516),\n",
       "  (12917368283846924199, 508, 516),\n",
       "  (12917368283846924199, 509, 516),\n",
       "  (12917368283846924199, 510, 516),\n",
       "  (12917368283846924199, 511, 516),\n",
       "  (12917368283846924199, 512, 516),\n",
       "  (12917368283846924199, 513, 516),\n",
       "  (12917368283846924199, 514, 516),\n",
       "  (12917368283846924199, 503, 517),\n",
       "  (12917368283846924199, 504, 517),\n",
       "  (12917368283846924199, 505, 517),\n",
       "  (12917368283846924199, 506, 517),\n",
       "  (12917368283846924199, 507, 517),\n",
       "  (12917368283846924199, 508, 517),\n",
       "  (12917368283846924199, 509, 517),\n",
       "  (12917368283846924199, 510, 517),\n",
       "  (12917368283846924199, 511, 517),\n",
       "  (12917368283846924199, 512, 517),\n",
       "  (12917368283846924199, 513, 517),\n",
       "  (12917368283846924199, 514, 517),\n",
       "  (12917368283846924199, 503, 518),\n",
       "  (12917368283846924199, 504, 518),\n",
       "  (12917368283846924199, 505, 518),\n",
       "  (12917368283846924199, 506, 518),\n",
       "  (12917368283846924199, 507, 518),\n",
       "  (12917368283846924199, 508, 518),\n",
       "  (12917368283846924199, 509, 518),\n",
       "  (12917368283846924199, 510, 518),\n",
       "  (12917368283846924199, 511, 518),\n",
       "  (12917368283846924199, 512, 518),\n",
       "  (12917368283846924199, 513, 518),\n",
       "  (12917368283846924199, 514, 518),\n",
       "  (12917368283846924199, 503, 519),\n",
       "  (12917368283846924199, 504, 519),\n",
       "  (12917368283846924199, 505, 519),\n",
       "  (12917368283846924199, 506, 519),\n",
       "  (12917368283846924199, 507, 519),\n",
       "  (12917368283846924199, 508, 519),\n",
       "  (12917368283846924199, 509, 519),\n",
       "  (12917368283846924199, 510, 519),\n",
       "  (12917368283846924199, 511, 519),\n",
       "  (12917368283846924199, 512, 519),\n",
       "  (12917368283846924199, 513, 519),\n",
       "  (12917368283846924199, 514, 519),\n",
       "  (12917368283846924199, 503, 520),\n",
       "  (12917368283846924199, 504, 520),\n",
       "  (12917368283846924199, 505, 520),\n",
       "  (12917368283846924199, 506, 520),\n",
       "  (12917368283846924199, 507, 520),\n",
       "  (12917368283846924199, 508, 520),\n",
       "  (12917368283846924199, 509, 520),\n",
       "  (12917368283846924199, 510, 520),\n",
       "  (12917368283846924199, 511, 520),\n",
       "  (12917368283846924199, 512, 520),\n",
       "  (12917368283846924199, 513, 520),\n",
       "  (12917368283846924199, 514, 520),\n",
       "  (12917368283846924199, 503, 521),\n",
       "  (12917368283846924199, 504, 521),\n",
       "  (12917368283846924199, 505, 521),\n",
       "  (12917368283846924199, 506, 521),\n",
       "  (12917368283846924199, 507, 521),\n",
       "  (12917368283846924199, 508, 521),\n",
       "  (12917368283846924199, 509, 521),\n",
       "  (12917368283846924199, 510, 521),\n",
       "  (12917368283846924199, 511, 521),\n",
       "  (12917368283846924199, 512, 521),\n",
       "  (12917368283846924199, 513, 521),\n",
       "  (12917368283846924199, 514, 521),\n",
       "  (12917368283846924199, 503, 522),\n",
       "  (12917368283846924199, 504, 522),\n",
       "  (12917368283846924199, 505, 522),\n",
       "  (12917368283846924199, 506, 522),\n",
       "  (12917368283846924199, 507, 522),\n",
       "  (12917368283846924199, 508, 522),\n",
       "  (12917368283846924199, 509, 522),\n",
       "  (12917368283846924199, 510, 522),\n",
       "  (12917368283846924199, 511, 522),\n",
       "  (12917368283846924199, 512, 522),\n",
       "  (12917368283846924199, 513, 522),\n",
       "  (12917368283846924199, 514, 522),\n",
       "  (12917368283846924199, 503, 523),\n",
       "  (12917368283846924199, 504, 523),\n",
       "  (12917368283846924199, 505, 523),\n",
       "  (12917368283846924199, 506, 523),\n",
       "  (12917368283846924199, 507, 523),\n",
       "  (12917368283846924199, 508, 523),\n",
       "  (12917368283846924199, 509, 523),\n",
       "  (12917368283846924199, 510, 523),\n",
       "  (12917368283846924199, 511, 523),\n",
       "  (12917368283846924199, 512, 523),\n",
       "  (12917368283846924199, 513, 523),\n",
       "  (12917368283846924199, 514, 523),\n",
       "  (12917368283846924199, 503, 524),\n",
       "  (12917368283846924199, 504, 524),\n",
       "  (12917368283846924199, 505, 524),\n",
       "  (12917368283846924199, 506, 524),\n",
       "  (12917368283846924199, 507, 524),\n",
       "  (12917368283846924199, 508, 524),\n",
       "  (12917368283846924199, 509, 524),\n",
       "  (12917368283846924199, 510, 524),\n",
       "  (12917368283846924199, 511, 524),\n",
       "  (12917368283846924199, 512, 524),\n",
       "  (12917368283846924199, 513, 524),\n",
       "  (12917368283846924199, 514, 524),\n",
       "  (12917368283846924199, 503, 525),\n",
       "  (12917368283846924199, 504, 525),\n",
       "  (12917368283846924199, 505, 525),\n",
       "  (12917368283846924199, 506, 525),\n",
       "  (12917368283846924199, 507, 525),\n",
       "  (12917368283846924199, 508, 525),\n",
       "  (12917368283846924199, 509, 525),\n",
       "  (12917368283846924199, 510, 525),\n",
       "  (12917368283846924199, 511, 525),\n",
       "  (12917368283846924199, 512, 525),\n",
       "  (12917368283846924199, 513, 525),\n",
       "  (12917368283846924199, 514, 525),\n",
       "  (12917368283846924199, 503, 526),\n",
       "  (12917368283846924199, 504, 526),\n",
       "  (12917368283846924199, 505, 526),\n",
       "  (12917368283846924199, 506, 526),\n",
       "  (12917368283846924199, 507, 526),\n",
       "  (12917368283846924199, 508, 526),\n",
       "  (12917368283846924199, 509, 526),\n",
       "  (12917368283846924199, 510, 526),\n",
       "  (12917368283846924199, 511, 526),\n",
       "  (12917368283846924199, 512, 526),\n",
       "  (12917368283846924199, 513, 526),\n",
       "  (12917368283846924199, 514, 526),\n",
       "  (12917368283846924199, 503, 527),\n",
       "  (12917368283846924199, 504, 527),\n",
       "  (12917368283846924199, 505, 527),\n",
       "  (12917368283846924199, 506, 527),\n",
       "  (12917368283846924199, 507, 527),\n",
       "  (12917368283846924199, 508, 527),\n",
       "  (12917368283846924199, 509, 527),\n",
       "  (12917368283846924199, 510, 527),\n",
       "  (12917368283846924199, 511, 527),\n",
       "  (12917368283846924199, 512, 527),\n",
       "  (12917368283846924199, 513, 527),\n",
       "  (12917368283846924199, 514, 527),\n",
       "  (12917368283846924199, 503, 528),\n",
       "  (12917368283846924199, 504, 528),\n",
       "  (12917368283846924199, 505, 528),\n",
       "  (12917368283846924199, 506, 528),\n",
       "  (12917368283846924199, 507, 528),\n",
       "  (12917368283846924199, 508, 528),\n",
       "  (12917368283846924199, 509, 528),\n",
       "  (12917368283846924199, 510, 528),\n",
       "  (12917368283846924199, 511, 528),\n",
       "  (12917368283846924199, 512, 528),\n",
       "  (12917368283846924199, 513, 528),\n",
       "  (12917368283846924199, 514, 528),\n",
       "  (12917368283846924199, 503, 529),\n",
       "  (12917368283846924199, 504, 529),\n",
       "  (12917368283846924199, 505, 529),\n",
       "  (12917368283846924199, 506, 529),\n",
       "  (12917368283846924199, 507, 529),\n",
       "  (12917368283846924199, 508, 529),\n",
       "  (12917368283846924199, 509, 529),\n",
       "  (12917368283846924199, 510, 529),\n",
       "  (12917368283846924199, 511, 529),\n",
       "  (12917368283846924199, 512, 529),\n",
       "  (12917368283846924199, 513, 529),\n",
       "  (12917368283846924199, 514, 529),\n",
       "  (12917368283846924199, 503, 530),\n",
       "  (12917368283846924199, 504, 530),\n",
       "  (12917368283846924199, 505, 530),\n",
       "  (12917368283846924199, 506, 530),\n",
       "  (12917368283846924199, 507, 530),\n",
       "  (12917368283846924199, 508, 530),\n",
       "  (12917368283846924199, 509, 530),\n",
       "  (12917368283846924199, 510, 530),\n",
       "  (12917368283846924199, 511, 530),\n",
       "  (12917368283846924199, 512, 530),\n",
       "  (12917368283846924199, 513, 530),\n",
       "  (12917368283846924199, 514, 530),\n",
       "  (12917368283846924199, 503, 531),\n",
       "  (12917368283846924199, 504, 531),\n",
       "  (12917368283846924199, 505, 531),\n",
       "  (12917368283846924199, 506, 531),\n",
       "  (12917368283846924199, 507, 531),\n",
       "  (12917368283846924199, 508, 531),\n",
       "  (12917368283846924199, 509, 531),\n",
       "  (12917368283846924199, 510, 531),\n",
       "  (12917368283846924199, 511, 531),\n",
       "  (12917368283846924199, 512, 531),\n",
       "  (12917368283846924199, 513, 531),\n",
       "  (12917368283846924199, 514, 531),\n",
       "  (12917368283846924199, 503, 532),\n",
       "  (12917368283846924199, 504, 532),\n",
       "  (12917368283846924199, 505, 532),\n",
       "  (12917368283846924199, 506, 532),\n",
       "  (12917368283846924199, 507, 532),\n",
       "  (12917368283846924199, 508, 532),\n",
       "  (12917368283846924199, 509, 532),\n",
       "  (12917368283846924199, 510, 532),\n",
       "  (12917368283846924199, 511, 532),\n",
       "  (12917368283846924199, 512, 532),\n",
       "  (12917368283846924199, 513, 532),\n",
       "  (12917368283846924199, 514, 532),\n",
       "  (12917368283846924199, 503, 533),\n",
       "  (12917368283846924199, 504, 533),\n",
       "  (12917368283846924199, 505, 533),\n",
       "  (12917368283846924199, 506, 533),\n",
       "  (12917368283846924199, 507, 533),\n",
       "  (12917368283846924199, 508, 533),\n",
       "  (12917368283846924199, 509, 533),\n",
       "  (12917368283846924199, 510, 533),\n",
       "  (12917368283846924199, 511, 533),\n",
       "  (12917368283846924199, 512, 533),\n",
       "  (12917368283846924199, 513, 533),\n",
       "  (12917368283846924199, 514, 533),\n",
       "  (12917368283846924199, 503, 534),\n",
       "  (12917368283846924199, 504, 534),\n",
       "  (12917368283846924199, 505, 534),\n",
       "  (12917368283846924199, 506, 534),\n",
       "  (12917368283846924199, 507, 534),\n",
       "  (12917368283846924199, 508, 534),\n",
       "  (12917368283846924199, 509, 534),\n",
       "  (12917368283846924199, 510, 534),\n",
       "  (12917368283846924199, 511, 534),\n",
       "  (12917368283846924199, 512, 534),\n",
       "  (12917368283846924199, 513, 534),\n",
       "  (12917368283846924199, 514, 534),\n",
       "  (12917368283846924199, 503, 535),\n",
       "  (12917368283846924199, 504, 535),\n",
       "  (12917368283846924199, 505, 535),\n",
       "  (12917368283846924199, 506, 535),\n",
       "  (12917368283846924199, 507, 535),\n",
       "  (12917368283846924199, 508, 535),\n",
       "  (12917368283846924199, 509, 535),\n",
       "  (12917368283846924199, 510, 535),\n",
       "  (12917368283846924199, 511, 535),\n",
       "  (12917368283846924199, 512, 535),\n",
       "  (12917368283846924199, 513, 535),\n",
       "  (12917368283846924199, 514, 535),\n",
       "  (12917368283846924199, 503, 536),\n",
       "  (12917368283846924199, 504, 536),\n",
       "  (12917368283846924199, 505, 536),\n",
       "  (12917368283846924199, 506, 536),\n",
       "  (12917368283846924199, 507, 536),\n",
       "  (12917368283846924199, 508, 536),\n",
       "  (12917368283846924199, 509, 536),\n",
       "  (12917368283846924199, 510, 536),\n",
       "  (12917368283846924199, 511, 536),\n",
       "  (12917368283846924199, 512, 536),\n",
       "  (12917368283846924199, 513, 536),\n",
       "  (12917368283846924199, 514, 536),\n",
       "  (12917368283846924199, 503, 537),\n",
       "  (12917368283846924199, 504, 537),\n",
       "  (12917368283846924199, 505, 537),\n",
       "  (12917368283846924199, 506, 537),\n",
       "  (12917368283846924199, 507, 537),\n",
       "  (12917368283846924199, 508, 537),\n",
       "  (12917368283846924199, 509, 537),\n",
       "  (12917368283846924199, 510, 537),\n",
       "  (12917368283846924199, 511, 537),\n",
       "  (12917368283846924199, 512, 537),\n",
       "  (12917368283846924199, 513, 537),\n",
       "  (12917368283846924199, 514, 537),\n",
       "  (12917368283846924199, 503, 538),\n",
       "  (12917368283846924199, 504, 538),\n",
       "  (12917368283846924199, 505, 538),\n",
       "  (12917368283846924199, 506, 538),\n",
       "  (12917368283846924199, 507, 538),\n",
       "  (12917368283846924199, 508, 538),\n",
       "  (12917368283846924199, 509, 538),\n",
       "  (12917368283846924199, 510, 538),\n",
       "  (12917368283846924199, 511, 538),\n",
       "  (12917368283846924199, 512, 538),\n",
       "  (12917368283846924199, 513, 538),\n",
       "  (12917368283846924199, 514, 538),\n",
       "  (12917368283846924199, 503, 539),\n",
       "  (12917368283846924199, 504, 539),\n",
       "  (12917368283846924199, 505, 539),\n",
       "  (12917368283846924199, 506, 539),\n",
       "  (12917368283846924199, 507, 539),\n",
       "  (12917368283846924199, 508, 539),\n",
       "  (12917368283846924199, 509, 539),\n",
       "  (12917368283846924199, 510, 539),\n",
       "  (12917368283846924199, 511, 539),\n",
       "  (12917368283846924199, 512, 539),\n",
       "  (12917368283846924199, 513, 539),\n",
       "  (12917368283846924199, 514, 539)],\n",
       " [(12917368283846924199, 1037, 1043),\n",
       "  (12917368283846924199, 1038, 1043),\n",
       "  (12917368283846924199, 1039, 1043),\n",
       "  (12917368283846924199, 1040, 1043),\n",
       "  (12917368283846924199, 1041, 1043),\n",
       "  (12917368283846924199, 1042, 1043)],\n",
       " [(12917368283846924199, 246, 249),\n",
       "  (12917368283846924199, 247, 249),\n",
       "  (12917368283846924199, 248, 249)],\n",
       " [],\n",
       " [(12917368283846924199, 61, 78),\n",
       "  (12917368283846924199, 62, 78),\n",
       "  (12917368283846924199, 63, 78),\n",
       "  (12917368283846924199, 64, 78),\n",
       "  (12917368283846924199, 65, 78),\n",
       "  (12917368283846924199, 66, 78),\n",
       "  (12917368283846924199, 67, 78),\n",
       "  (12917368283846924199, 68, 78),\n",
       "  (12917368283846924199, 69, 78),\n",
       "  (12917368283846924199, 70, 78),\n",
       "  (12917368283846924199, 71, 78),\n",
       "  (12917368283846924199, 72, 78),\n",
       "  (12917368283846924199, 73, 78),\n",
       "  (12917368283846924199, 74, 78),\n",
       "  (12917368283846924199, 75, 78),\n",
       "  (12917368283846924199, 76, 78),\n",
       "  (12917368283846924199, 77, 78),\n",
       "  (12917368283846924199, 61, 79),\n",
       "  (12917368283846924199, 62, 79),\n",
       "  (12917368283846924199, 63, 79),\n",
       "  (12917368283846924199, 64, 79),\n",
       "  (12917368283846924199, 65, 79),\n",
       "  (12917368283846924199, 66, 79),\n",
       "  (12917368283846924199, 67, 79),\n",
       "  (12917368283846924199, 68, 79),\n",
       "  (12917368283846924199, 69, 79),\n",
       "  (12917368283846924199, 70, 79),\n",
       "  (12917368283846924199, 71, 79),\n",
       "  (12917368283846924199, 72, 79),\n",
       "  (12917368283846924199, 73, 79),\n",
       "  (12917368283846924199, 74, 79),\n",
       "  (12917368283846924199, 75, 79),\n",
       "  (12917368283846924199, 76, 79),\n",
       "  (12917368283846924199, 77, 79)],\n",
       " [],\n",
       " [],\n",
       " [(12917368283846924199, 23, 47),\n",
       "  (12917368283846924199, 24, 47),\n",
       "  (12917368283846924199, 25, 47),\n",
       "  (12917368283846924199, 26, 47),\n",
       "  (12917368283846924199, 27, 47),\n",
       "  (12917368283846924199, 28, 47),\n",
       "  (12917368283846924199, 29, 47),\n",
       "  (12917368283846924199, 30, 47),\n",
       "  (12917368283846924199, 31, 47),\n",
       "  (12917368283846924199, 32, 47),\n",
       "  (12917368283846924199, 33, 47),\n",
       "  (12917368283846924199, 34, 47),\n",
       "  (12917368283846924199, 35, 47),\n",
       "  (12917368283846924199, 36, 47),\n",
       "  (12917368283846924199, 37, 47),\n",
       "  (12917368283846924199, 38, 47),\n",
       "  (12917368283846924199, 39, 47),\n",
       "  (12917368283846924199, 40, 47),\n",
       "  (12917368283846924199, 41, 47),\n",
       "  (12917368283846924199, 42, 47),\n",
       "  (12917368283846924199, 43, 47),\n",
       "  (12917368283846924199, 44, 47),\n",
       "  (12917368283846924199, 45, 47),\n",
       "  (12917368283846924199, 46, 47),\n",
       "  (12917368283846924199, 23, 48),\n",
       "  (12917368283846924199, 24, 48),\n",
       "  (12917368283846924199, 25, 48),\n",
       "  (12917368283846924199, 26, 48),\n",
       "  (12917368283846924199, 27, 48),\n",
       "  (12917368283846924199, 28, 48),\n",
       "  (12917368283846924199, 29, 48),\n",
       "  (12917368283846924199, 30, 48),\n",
       "  (12917368283846924199, 31, 48),\n",
       "  (12917368283846924199, 32, 48),\n",
       "  (12917368283846924199, 33, 48),\n",
       "  (12917368283846924199, 34, 48),\n",
       "  (12917368283846924199, 35, 48),\n",
       "  (12917368283846924199, 36, 48),\n",
       "  (12917368283846924199, 37, 48),\n",
       "  (12917368283846924199, 38, 48),\n",
       "  (12917368283846924199, 39, 48),\n",
       "  (12917368283846924199, 40, 48),\n",
       "  (12917368283846924199, 41, 48),\n",
       "  (12917368283846924199, 42, 48),\n",
       "  (12917368283846924199, 43, 48),\n",
       "  (12917368283846924199, 44, 48),\n",
       "  (12917368283846924199, 45, 48),\n",
       "  (12917368283846924199, 46, 48),\n",
       "  (12917368283846924199, 23, 49),\n",
       "  (12917368283846924199, 24, 49),\n",
       "  (12917368283846924199, 25, 49),\n",
       "  (12917368283846924199, 26, 49),\n",
       "  (12917368283846924199, 27, 49),\n",
       "  (12917368283846924199, 28, 49),\n",
       "  (12917368283846924199, 29, 49),\n",
       "  (12917368283846924199, 30, 49),\n",
       "  (12917368283846924199, 31, 49),\n",
       "  (12917368283846924199, 32, 49),\n",
       "  (12917368283846924199, 33, 49),\n",
       "  (12917368283846924199, 34, 49),\n",
       "  (12917368283846924199, 35, 49),\n",
       "  (12917368283846924199, 36, 49),\n",
       "  (12917368283846924199, 37, 49),\n",
       "  (12917368283846924199, 38, 49),\n",
       "  (12917368283846924199, 39, 49),\n",
       "  (12917368283846924199, 40, 49),\n",
       "  (12917368283846924199, 41, 49),\n",
       "  (12917368283846924199, 42, 49),\n",
       "  (12917368283846924199, 43, 49),\n",
       "  (12917368283846924199, 44, 49),\n",
       "  (12917368283846924199, 45, 49),\n",
       "  (12917368283846924199, 46, 49),\n",
       "  (12917368283846924199, 23, 50),\n",
       "  (12917368283846924199, 24, 50),\n",
       "  (12917368283846924199, 25, 50),\n",
       "  (12917368283846924199, 26, 50),\n",
       "  (12917368283846924199, 27, 50),\n",
       "  (12917368283846924199, 28, 50),\n",
       "  (12917368283846924199, 29, 50),\n",
       "  (12917368283846924199, 30, 50),\n",
       "  (12917368283846924199, 31, 50),\n",
       "  (12917368283846924199, 32, 50),\n",
       "  (12917368283846924199, 33, 50),\n",
       "  (12917368283846924199, 34, 50),\n",
       "  (12917368283846924199, 35, 50),\n",
       "  (12917368283846924199, 36, 50),\n",
       "  (12917368283846924199, 37, 50),\n",
       "  (12917368283846924199, 38, 50),\n",
       "  (12917368283846924199, 39, 50),\n",
       "  (12917368283846924199, 40, 50),\n",
       "  (12917368283846924199, 41, 50),\n",
       "  (12917368283846924199, 42, 50),\n",
       "  (12917368283846924199, 43, 50),\n",
       "  (12917368283846924199, 44, 50),\n",
       "  (12917368283846924199, 45, 50),\n",
       "  (12917368283846924199, 46, 50),\n",
       "  (12917368283846924199, 23, 51),\n",
       "  (12917368283846924199, 24, 51),\n",
       "  (12917368283846924199, 25, 51),\n",
       "  (12917368283846924199, 26, 51),\n",
       "  (12917368283846924199, 27, 51),\n",
       "  (12917368283846924199, 28, 51),\n",
       "  (12917368283846924199, 29, 51),\n",
       "  (12917368283846924199, 30, 51),\n",
       "  (12917368283846924199, 31, 51),\n",
       "  (12917368283846924199, 32, 51),\n",
       "  (12917368283846924199, 33, 51),\n",
       "  (12917368283846924199, 34, 51),\n",
       "  (12917368283846924199, 35, 51),\n",
       "  (12917368283846924199, 36, 51),\n",
       "  (12917368283846924199, 37, 51),\n",
       "  (12917368283846924199, 38, 51),\n",
       "  (12917368283846924199, 39, 51),\n",
       "  (12917368283846924199, 40, 51),\n",
       "  (12917368283846924199, 41, 51),\n",
       "  (12917368283846924199, 42, 51),\n",
       "  (12917368283846924199, 43, 51),\n",
       "  (12917368283846924199, 44, 51),\n",
       "  (12917368283846924199, 45, 51),\n",
       "  (12917368283846924199, 46, 51),\n",
       "  (12917368283846924199, 23, 52),\n",
       "  (12917368283846924199, 24, 52),\n",
       "  (12917368283846924199, 25, 52),\n",
       "  (12917368283846924199, 26, 52),\n",
       "  (12917368283846924199, 27, 52),\n",
       "  (12917368283846924199, 28, 52),\n",
       "  (12917368283846924199, 29, 52),\n",
       "  (12917368283846924199, 30, 52),\n",
       "  (12917368283846924199, 31, 52),\n",
       "  (12917368283846924199, 32, 52),\n",
       "  (12917368283846924199, 33, 52),\n",
       "  (12917368283846924199, 34, 52),\n",
       "  (12917368283846924199, 35, 52),\n",
       "  (12917368283846924199, 36, 52),\n",
       "  (12917368283846924199, 37, 52),\n",
       "  (12917368283846924199, 38, 52),\n",
       "  (12917368283846924199, 39, 52),\n",
       "  (12917368283846924199, 40, 52),\n",
       "  (12917368283846924199, 41, 52),\n",
       "  (12917368283846924199, 42, 52),\n",
       "  (12917368283846924199, 43, 52),\n",
       "  (12917368283846924199, 44, 52),\n",
       "  (12917368283846924199, 45, 52),\n",
       "  (12917368283846924199, 46, 52),\n",
       "  (12917368283846924199, 23, 53),\n",
       "  (12917368283846924199, 24, 53),\n",
       "  (12917368283846924199, 25, 53),\n",
       "  (12917368283846924199, 26, 53),\n",
       "  (12917368283846924199, 27, 53),\n",
       "  (12917368283846924199, 28, 53),\n",
       "  (12917368283846924199, 29, 53),\n",
       "  (12917368283846924199, 30, 53),\n",
       "  (12917368283846924199, 31, 53),\n",
       "  (12917368283846924199, 32, 53),\n",
       "  (12917368283846924199, 33, 53),\n",
       "  (12917368283846924199, 34, 53),\n",
       "  (12917368283846924199, 35, 53),\n",
       "  (12917368283846924199, 36, 53),\n",
       "  (12917368283846924199, 37, 53),\n",
       "  (12917368283846924199, 38, 53),\n",
       "  (12917368283846924199, 39, 53),\n",
       "  (12917368283846924199, 40, 53),\n",
       "  (12917368283846924199, 41, 53),\n",
       "  (12917368283846924199, 42, 53),\n",
       "  (12917368283846924199, 43, 53),\n",
       "  (12917368283846924199, 44, 53),\n",
       "  (12917368283846924199, 45, 53),\n",
       "  (12917368283846924199, 46, 53),\n",
       "  (12917368283846924199, 23, 54),\n",
       "  (12917368283846924199, 24, 54),\n",
       "  (12917368283846924199, 25, 54),\n",
       "  (12917368283846924199, 26, 54),\n",
       "  (12917368283846924199, 27, 54),\n",
       "  (12917368283846924199, 28, 54),\n",
       "  (12917368283846924199, 29, 54),\n",
       "  (12917368283846924199, 30, 54),\n",
       "  (12917368283846924199, 31, 54),\n",
       "  (12917368283846924199, 32, 54),\n",
       "  (12917368283846924199, 33, 54),\n",
       "  (12917368283846924199, 34, 54),\n",
       "  (12917368283846924199, 35, 54),\n",
       "  (12917368283846924199, 36, 54),\n",
       "  (12917368283846924199, 37, 54),\n",
       "  (12917368283846924199, 38, 54),\n",
       "  (12917368283846924199, 39, 54),\n",
       "  (12917368283846924199, 40, 54),\n",
       "  (12917368283846924199, 41, 54),\n",
       "  (12917368283846924199, 42, 54),\n",
       "  (12917368283846924199, 43, 54),\n",
       "  (12917368283846924199, 44, 54),\n",
       "  (12917368283846924199, 45, 54),\n",
       "  (12917368283846924199, 46, 54),\n",
       "  (12917368283846924199, 23, 55),\n",
       "  (12917368283846924199, 24, 55),\n",
       "  (12917368283846924199, 25, 55),\n",
       "  (12917368283846924199, 26, 55),\n",
       "  (12917368283846924199, 27, 55),\n",
       "  (12917368283846924199, 28, 55),\n",
       "  (12917368283846924199, 29, 55),\n",
       "  (12917368283846924199, 30, 55),\n",
       "  (12917368283846924199, 31, 55),\n",
       "  (12917368283846924199, 32, 55),\n",
       "  (12917368283846924199, 33, 55),\n",
       "  (12917368283846924199, 34, 55),\n",
       "  (12917368283846924199, 35, 55),\n",
       "  (12917368283846924199, 36, 55),\n",
       "  (12917368283846924199, 37, 55),\n",
       "  (12917368283846924199, 38, 55),\n",
       "  (12917368283846924199, 39, 55),\n",
       "  (12917368283846924199, 40, 55),\n",
       "  (12917368283846924199, 41, 55),\n",
       "  (12917368283846924199, 42, 55),\n",
       "  (12917368283846924199, 43, 55),\n",
       "  (12917368283846924199, 44, 55),\n",
       "  (12917368283846924199, 45, 55),\n",
       "  (12917368283846924199, 46, 55)],\n",
       " [],\n",
       " [(12917368283846924199, 18, 24),\n",
       "  (12917368283846924199, 19, 24),\n",
       "  (12917368283846924199, 20, 24),\n",
       "  (12917368283846924199, 21, 24),\n",
       "  (12917368283846924199, 22, 24),\n",
       "  (12917368283846924199, 23, 24),\n",
       "  (12917368283846924199, 18, 25),\n",
       "  (12917368283846924199, 19, 25),\n",
       "  (12917368283846924199, 20, 25),\n",
       "  (12917368283846924199, 21, 25),\n",
       "  (12917368283846924199, 22, 25),\n",
       "  (12917368283846924199, 23, 25),\n",
       "  (12917368283846924199, 18, 26),\n",
       "  (12917368283846924199, 19, 26),\n",
       "  (12917368283846924199, 20, 26),\n",
       "  (12917368283846924199, 21, 26),\n",
       "  (12917368283846924199, 22, 26),\n",
       "  (12917368283846924199, 23, 26),\n",
       "  (12917368283846924199, 38, 45),\n",
       "  (12917368283846924199, 39, 45),\n",
       "  (12917368283846924199, 40, 45),\n",
       "  (12917368283846924199, 41, 45),\n",
       "  (12917368283846924199, 42, 45),\n",
       "  (12917368283846924199, 43, 45),\n",
       "  (12917368283846924199, 44, 45),\n",
       "  (12917368283846924199, 310, 312),\n",
       "  (12917368283846924199, 311, 312)],\n",
       " [(12917368283846924199, 27, 29),\n",
       "  (12917368283846924199, 28, 29),\n",
       "  (12917368283846924199, 27, 30),\n",
       "  (12917368283846924199, 28, 30),\n",
       "  (12917368283846924199, 27, 31),\n",
       "  (12917368283846924199, 28, 31),\n",
       "  (12917368283846924199, 27, 32),\n",
       "  (12917368283846924199, 28, 32),\n",
       "  (12917368283846924199, 27, 33),\n",
       "  (12917368283846924199, 28, 33),\n",
       "  (12917368283846924199, 27, 34),\n",
       "  (12917368283846924199, 28, 34),\n",
       "  (12917368283846924199, 27, 35),\n",
       "  (12917368283846924199, 28, 35),\n",
       "  (12917368283846924199, 27, 36),\n",
       "  (12917368283846924199, 28, 36),\n",
       "  (12917368283846924199, 27, 37),\n",
       "  (12917368283846924199, 28, 37),\n",
       "  (12917368283846924199, 27, 38),\n",
       "  (12917368283846924199, 28, 38),\n",
       "  (12917368283846924199, 27, 39),\n",
       "  (12917368283846924199, 28, 39),\n",
       "  (12917368283846924199, 27, 40),\n",
       "  (12917368283846924199, 28, 40),\n",
       "  (12917368283846924199, 27, 41),\n",
       "  (12917368283846924199, 28, 41),\n",
       "  (12917368283846924199, 27, 42),\n",
       "  (12917368283846924199, 28, 42),\n",
       "  (12917368283846924199, 27, 43),\n",
       "  (12917368283846924199, 28, 43),\n",
       "  (12917368283846924199, 27, 44),\n",
       "  (12917368283846924199, 28, 44),\n",
       "  (12917368283846924199, 27, 45),\n",
       "  (12917368283846924199, 28, 45),\n",
       "  (12917368283846924199, 27, 46),\n",
       "  (12917368283846924199, 28, 46),\n",
       "  (12917368283846924199, 27, 47),\n",
       "  (12917368283846924199, 28, 47),\n",
       "  (12917368283846924199, 27, 48),\n",
       "  (12917368283846924199, 28, 48),\n",
       "  (12917368283846924199, 27, 49),\n",
       "  (12917368283846924199, 28, 49),\n",
       "  (12917368283846924199, 27, 50),\n",
       "  (12917368283846924199, 28, 50),\n",
       "  (12917368283846924199, 27, 51),\n",
       "  (12917368283846924199, 28, 51),\n",
       "  (12917368283846924199, 27, 52),\n",
       "  (12917368283846924199, 28, 52)],\n",
       " [],\n",
       " [(12917368283846924199, 978, 984),\n",
       "  (12917368283846924199, 979, 984),\n",
       "  (12917368283846924199, 980, 984),\n",
       "  (12917368283846924199, 981, 984),\n",
       "  (12917368283846924199, 982, 984),\n",
       "  (12917368283846924199, 983, 984),\n",
       "  (12917368283846924199, 978, 985),\n",
       "  (12917368283846924199, 979, 985),\n",
       "  (12917368283846924199, 980, 985),\n",
       "  (12917368283846924199, 981, 985),\n",
       "  (12917368283846924199, 982, 985),\n",
       "  (12917368283846924199, 983, 985),\n",
       "  (12917368283846924199, 978, 986),\n",
       "  (12917368283846924199, 979, 986),\n",
       "  (12917368283846924199, 980, 986),\n",
       "  (12917368283846924199, 981, 986),\n",
       "  (12917368283846924199, 982, 986),\n",
       "  (12917368283846924199, 983, 986)],\n",
       " [(12917368283846924199, 328, 330),\n",
       "  (12917368283846924199, 329, 330),\n",
       "  (12917368283846924199, 332, 334),\n",
       "  (12917368283846924199, 333, 334),\n",
       "  (12917368283846924199, 332, 335),\n",
       "  (12917368283846924199, 333, 335),\n",
       "  (12917368283846924199, 332, 336),\n",
       "  (12917368283846924199, 333, 336),\n",
       "  (12917368283846924199, 332, 337),\n",
       "  (12917368283846924199, 333, 337),\n",
       "  (12917368283846924199, 332, 338),\n",
       "  (12917368283846924199, 333, 338),\n",
       "  (12917368283846924199, 349, 353),\n",
       "  (12917368283846924199, 350, 353),\n",
       "  (12917368283846924199, 351, 353),\n",
       "  (12917368283846924199, 352, 353),\n",
       "  (12917368283846924199, 460, 463),\n",
       "  (12917368283846924199, 461, 463),\n",
       "  (12917368283846924199, 462, 463)],\n",
       " [(12917368283846924199, 57, 74),\n",
       "  (12917368283846924199, 58, 74),\n",
       "  (12917368283846924199, 59, 74),\n",
       "  (12917368283846924199, 60, 74),\n",
       "  (12917368283846924199, 61, 74),\n",
       "  (12917368283846924199, 62, 74),\n",
       "  (12917368283846924199, 63, 74),\n",
       "  (12917368283846924199, 64, 74),\n",
       "  (12917368283846924199, 65, 74),\n",
       "  (12917368283846924199, 66, 74),\n",
       "  (12917368283846924199, 67, 74),\n",
       "  (12917368283846924199, 68, 74),\n",
       "  (12917368283846924199, 69, 74),\n",
       "  (12917368283846924199, 70, 74),\n",
       "  (12917368283846924199, 71, 74),\n",
       "  (12917368283846924199, 72, 74),\n",
       "  (12917368283846924199, 73, 74)],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [(12917368283846924199, 32, 38),\n",
       "  (12917368283846924199, 33, 38),\n",
       "  (12917368283846924199, 34, 38),\n",
       "  (12917368283846924199, 35, 38),\n",
       "  (12917368283846924199, 36, 38),\n",
       "  (12917368283846924199, 37, 38)],\n",
       " [(12917368283846924199, 154, 161),\n",
       "  (12917368283846924199, 155, 161),\n",
       "  (12917368283846924199, 156, 161),\n",
       "  (12917368283846924199, 157, 161),\n",
       "  (12917368283846924199, 158, 161),\n",
       "  (12917368283846924199, 159, 161),\n",
       "  (12917368283846924199, 160, 161),\n",
       "  (12917368283846924199, 210, 214),\n",
       "  (12917368283846924199, 211, 214),\n",
       "  (12917368283846924199, 212, 214),\n",
       "  (12917368283846924199, 213, 214),\n",
       "  (12917368283846924199, 210, 215),\n",
       "  (12917368283846924199, 211, 215),\n",
       "  (12917368283846924199, 212, 215),\n",
       "  (12917368283846924199, 213, 215),\n",
       "  (12917368283846924199, 210, 216),\n",
       "  (12917368283846924199, 211, 216),\n",
       "  (12917368283846924199, 212, 216),\n",
       "  (12917368283846924199, 213, 216),\n",
       "  (12917368283846924199, 210, 217),\n",
       "  (12917368283846924199, 211, 217),\n",
       "  (12917368283846924199, 212, 217),\n",
       "  (12917368283846924199, 213, 217),\n",
       "  (12917368283846924199, 210, 218),\n",
       "  (12917368283846924199, 211, 218),\n",
       "  (12917368283846924199, 212, 218),\n",
       "  (12917368283846924199, 213, 218),\n",
       "  (12917368283846924199, 210, 219),\n",
       "  (12917368283846924199, 211, 219),\n",
       "  (12917368283846924199, 212, 219),\n",
       "  (12917368283846924199, 213, 219),\n",
       "  (12917368283846924199, 210, 220),\n",
       "  (12917368283846924199, 211, 220),\n",
       "  (12917368283846924199, 212, 220),\n",
       "  (12917368283846924199, 213, 220),\n",
       "  (12917368283846924199, 210, 221),\n",
       "  (12917368283846924199, 211, 221),\n",
       "  (12917368283846924199, 212, 221),\n",
       "  (12917368283846924199, 213, 221),\n",
       "  (12917368283846924199, 210, 222),\n",
       "  (12917368283846924199, 211, 222),\n",
       "  (12917368283846924199, 212, 222),\n",
       "  (12917368283846924199, 213, 222),\n",
       "  (12917368283846924199, 210, 223),\n",
       "  (12917368283846924199, 211, 223),\n",
       "  (12917368283846924199, 212, 223),\n",
       "  (12917368283846924199, 213, 223),\n",
       "  (12917368283846924199, 214, 223),\n",
       "  (12917368283846924199, 215, 223),\n",
       "  (12917368283846924199, 216, 223),\n",
       "  (12917368283846924199, 217, 223),\n",
       "  (12917368283846924199, 218, 223),\n",
       "  (12917368283846924199, 219, 223),\n",
       "  (12917368283846924199, 220, 223),\n",
       "  (12917368283846924199, 221, 223),\n",
       "  (12917368283846924199, 222, 223),\n",
       "  (12917368283846924199, 224, 226),\n",
       "  (12917368283846924199, 225, 226),\n",
       "  (12917368283846924199, 224, 227),\n",
       "  (12917368283846924199, 225, 227),\n",
       "  (12917368283846924199, 224, 228),\n",
       "  (12917368283846924199, 225, 228),\n",
       "  (12917368283846924199, 224, 229),\n",
       "  (12917368283846924199, 225, 229)],\n",
       " [(12917368283846924199, 700, 701),\n",
       "  (12917368283846924199, 700, 702),\n",
       "  (12917368283846924199, 715, 719),\n",
       "  (12917368283846924199, 716, 719),\n",
       "  (12917368283846924199, 717, 719),\n",
       "  (12917368283846924199, 718, 719),\n",
       "  (12917368283846924199, 741, 761),\n",
       "  (12917368283846924199, 742, 761),\n",
       "  (12917368283846924199, 743, 761),\n",
       "  (12917368283846924199, 744, 761),\n",
       "  (12917368283846924199, 745, 761),\n",
       "  (12917368283846924199, 746, 761),\n",
       "  (12917368283846924199, 747, 761),\n",
       "  (12917368283846924199, 748, 761),\n",
       "  (12917368283846924199, 749, 761),\n",
       "  (12917368283846924199, 750, 761),\n",
       "  (12917368283846924199, 751, 761),\n",
       "  (12917368283846924199, 752, 761),\n",
       "  (12917368283846924199, 753, 761),\n",
       "  (12917368283846924199, 754, 761),\n",
       "  (12917368283846924199, 755, 761),\n",
       "  (12917368283846924199, 756, 761),\n",
       "  (12917368283846924199, 757, 761),\n",
       "  (12917368283846924199, 758, 761),\n",
       "  (12917368283846924199, 759, 761),\n",
       "  (12917368283846924199, 760, 761),\n",
       "  (12917368283846924199, 741, 762),\n",
       "  (12917368283846924199, 742, 762),\n",
       "  (12917368283846924199, 743, 762),\n",
       "  (12917368283846924199, 744, 762),\n",
       "  (12917368283846924199, 745, 762),\n",
       "  (12917368283846924199, 746, 762),\n",
       "  (12917368283846924199, 747, 762),\n",
       "  (12917368283846924199, 748, 762),\n",
       "  (12917368283846924199, 749, 762),\n",
       "  (12917368283846924199, 750, 762),\n",
       "  (12917368283846924199, 751, 762),\n",
       "  (12917368283846924199, 752, 762),\n",
       "  (12917368283846924199, 753, 762),\n",
       "  (12917368283846924199, 754, 762),\n",
       "  (12917368283846924199, 755, 762),\n",
       "  (12917368283846924199, 756, 762),\n",
       "  (12917368283846924199, 757, 762),\n",
       "  (12917368283846924199, 758, 762),\n",
       "  (12917368283846924199, 759, 762),\n",
       "  (12917368283846924199, 760, 762),\n",
       "  (12917368283846924199, 741, 763),\n",
       "  (12917368283846924199, 742, 763),\n",
       "  (12917368283846924199, 743, 763),\n",
       "  (12917368283846924199, 744, 763),\n",
       "  (12917368283846924199, 745, 763),\n",
       "  (12917368283846924199, 746, 763),\n",
       "  (12917368283846924199, 747, 763),\n",
       "  (12917368283846924199, 748, 763),\n",
       "  (12917368283846924199, 749, 763),\n",
       "  (12917368283846924199, 750, 763),\n",
       "  (12917368283846924199, 751, 763),\n",
       "  (12917368283846924199, 752, 763),\n",
       "  (12917368283846924199, 753, 763),\n",
       "  (12917368283846924199, 754, 763),\n",
       "  (12917368283846924199, 755, 763),\n",
       "  (12917368283846924199, 756, 763),\n",
       "  (12917368283846924199, 757, 763),\n",
       "  (12917368283846924199, 758, 763),\n",
       "  (12917368283846924199, 759, 763),\n",
       "  (12917368283846924199, 760, 763),\n",
       "  (12917368283846924199, 741, 764),\n",
       "  (12917368283846924199, 742, 764),\n",
       "  (12917368283846924199, 743, 764),\n",
       "  (12917368283846924199, 744, 764),\n",
       "  (12917368283846924199, 745, 764),\n",
       "  (12917368283846924199, 746, 764),\n",
       "  (12917368283846924199, 747, 764),\n",
       "  (12917368283846924199, 748, 764),\n",
       "  (12917368283846924199, 749, 764),\n",
       "  (12917368283846924199, 750, 764),\n",
       "  (12917368283846924199, 751, 764),\n",
       "  (12917368283846924199, 752, 764),\n",
       "  (12917368283846924199, 753, 764),\n",
       "  (12917368283846924199, 754, 764),\n",
       "  (12917368283846924199, 755, 764),\n",
       "  (12917368283846924199, 756, 764),\n",
       "  (12917368283846924199, 757, 764),\n",
       "  (12917368283846924199, 758, 764),\n",
       "  (12917368283846924199, 759, 764),\n",
       "  (12917368283846924199, 760, 764)],\n",
       " [(12917368283846924199, 19, 41),\n",
       "  (12917368283846924199, 20, 41),\n",
       "  (12917368283846924199, 21, 41),\n",
       "  (12917368283846924199, 22, 41),\n",
       "  (12917368283846924199, 23, 41),\n",
       "  (12917368283846924199, 24, 41),\n",
       "  (12917368283846924199, 25, 41),\n",
       "  (12917368283846924199, 26, 41),\n",
       "  (12917368283846924199, 27, 41),\n",
       "  (12917368283846924199, 28, 41),\n",
       "  (12917368283846924199, 29, 41),\n",
       "  (12917368283846924199, 30, 41),\n",
       "  (12917368283846924199, 31, 41),\n",
       "  (12917368283846924199, 32, 41),\n",
       "  (12917368283846924199, 33, 41),\n",
       "  (12917368283846924199, 34, 41),\n",
       "  (12917368283846924199, 35, 41),\n",
       "  (12917368283846924199, 36, 41),\n",
       "  (12917368283846924199, 37, 41),\n",
       "  (12917368283846924199, 38, 41),\n",
       "  (12917368283846924199, 39, 41),\n",
       "  (12917368283846924199, 40, 41),\n",
       "  (12917368283846924199, 47, 54),\n",
       "  (12917368283846924199, 48, 54),\n",
       "  (12917368283846924199, 49, 54),\n",
       "  (12917368283846924199, 50, 54),\n",
       "  (12917368283846924199, 51, 54),\n",
       "  (12917368283846924199, 52, 54),\n",
       "  (12917368283846924199, 53, 54),\n",
       "  (12917368283846924199, 47, 55),\n",
       "  (12917368283846924199, 48, 55),\n",
       "  (12917368283846924199, 49, 55),\n",
       "  (12917368283846924199, 50, 55),\n",
       "  (12917368283846924199, 51, 55),\n",
       "  (12917368283846924199, 52, 55),\n",
       "  (12917368283846924199, 53, 55),\n",
       "  (12917368283846924199, 47, 56),\n",
       "  (12917368283846924199, 48, 56),\n",
       "  (12917368283846924199, 49, 56),\n",
       "  (12917368283846924199, 50, 56),\n",
       "  (12917368283846924199, 51, 56),\n",
       "  (12917368283846924199, 52, 56),\n",
       "  (12917368283846924199, 53, 56),\n",
       "  (12917368283846924199, 47, 57),\n",
       "  (12917368283846924199, 48, 57),\n",
       "  (12917368283846924199, 49, 57),\n",
       "  (12917368283846924199, 50, 57),\n",
       "  (12917368283846924199, 51, 57),\n",
       "  (12917368283846924199, 52, 57),\n",
       "  (12917368283846924199, 53, 57),\n",
       "  (12917368283846924199, 47, 58),\n",
       "  (12917368283846924199, 48, 58),\n",
       "  (12917368283846924199, 49, 58),\n",
       "  (12917368283846924199, 50, 58),\n",
       "  (12917368283846924199, 51, 58),\n",
       "  (12917368283846924199, 52, 58),\n",
       "  (12917368283846924199, 53, 58),\n",
       "  (12917368283846924199, 47, 59),\n",
       "  (12917368283846924199, 48, 59),\n",
       "  (12917368283846924199, 49, 59),\n",
       "  (12917368283846924199, 50, 59),\n",
       "  (12917368283846924199, 51, 59),\n",
       "  (12917368283846924199, 52, 59),\n",
       "  (12917368283846924199, 53, 59),\n",
       "  (12917368283846924199, 47, 60),\n",
       "  (12917368283846924199, 48, 60),\n",
       "  (12917368283846924199, 49, 60),\n",
       "  (12917368283846924199, 50, 60),\n",
       "  (12917368283846924199, 51, 60),\n",
       "  (12917368283846924199, 52, 60),\n",
       "  (12917368283846924199, 53, 60),\n",
       "  (12917368283846924199, 47, 62),\n",
       "  (12917368283846924199, 48, 62),\n",
       "  (12917368283846924199, 49, 62),\n",
       "  (12917368283846924199, 50, 62),\n",
       "  (12917368283846924199, 51, 62),\n",
       "  (12917368283846924199, 52, 62),\n",
       "  (12917368283846924199, 53, 62),\n",
       "  (12917368283846924199, 54, 62),\n",
       "  (12917368283846924199, 55, 62),\n",
       "  (12917368283846924199, 56, 62),\n",
       "  (12917368283846924199, 57, 62),\n",
       "  (12917368283846924199, 58, 62),\n",
       "  (12917368283846924199, 59, 62),\n",
       "  (12917368283846924199, 60, 62),\n",
       "  (12917368283846924199, 61, 62),\n",
       "  (12917368283846924199, 47, 63),\n",
       "  (12917368283846924199, 48, 63),\n",
       "  (12917368283846924199, 49, 63),\n",
       "  (12917368283846924199, 50, 63),\n",
       "  (12917368283846924199, 51, 63),\n",
       "  (12917368283846924199, 52, 63),\n",
       "  (12917368283846924199, 53, 63),\n",
       "  (12917368283846924199, 54, 63),\n",
       "  (12917368283846924199, 55, 63),\n",
       "  (12917368283846924199, 56, 63),\n",
       "  (12917368283846924199, 57, 63),\n",
       "  (12917368283846924199, 58, 63),\n",
       "  (12917368283846924199, 59, 63),\n",
       "  (12917368283846924199, 60, 63),\n",
       "  (12917368283846924199, 61, 63),\n",
       "  (12917368283846924199, 47, 64),\n",
       "  (12917368283846924199, 48, 64),\n",
       "  (12917368283846924199, 49, 64),\n",
       "  (12917368283846924199, 50, 64),\n",
       "  (12917368283846924199, 51, 64),\n",
       "  (12917368283846924199, 52, 64),\n",
       "  (12917368283846924199, 53, 64),\n",
       "  (12917368283846924199, 54, 64),\n",
       "  (12917368283846924199, 55, 64),\n",
       "  (12917368283846924199, 56, 64),\n",
       "  (12917368283846924199, 57, 64),\n",
       "  (12917368283846924199, 58, 64),\n",
       "  (12917368283846924199, 59, 64),\n",
       "  (12917368283846924199, 60, 64),\n",
       "  (12917368283846924199, 61, 64),\n",
       "  (12917368283846924199, 47, 65),\n",
       "  (12917368283846924199, 48, 65),\n",
       "  (12917368283846924199, 49, 65),\n",
       "  (12917368283846924199, 50, 65),\n",
       "  (12917368283846924199, 51, 65),\n",
       "  (12917368283846924199, 52, 65),\n",
       "  (12917368283846924199, 53, 65),\n",
       "  (12917368283846924199, 54, 65),\n",
       "  (12917368283846924199, 55, 65),\n",
       "  (12917368283846924199, 56, 65),\n",
       "  (12917368283846924199, 57, 65),\n",
       "  (12917368283846924199, 58, 65),\n",
       "  (12917368283846924199, 59, 65),\n",
       "  (12917368283846924199, 60, 65),\n",
       "  (12917368283846924199, 61, 65),\n",
       "  (12917368283846924199, 47, 66),\n",
       "  (12917368283846924199, 48, 66),\n",
       "  (12917368283846924199, 49, 66),\n",
       "  (12917368283846924199, 50, 66),\n",
       "  (12917368283846924199, 51, 66),\n",
       "  (12917368283846924199, 52, 66),\n",
       "  (12917368283846924199, 53, 66),\n",
       "  (12917368283846924199, 54, 66),\n",
       "  (12917368283846924199, 55, 66),\n",
       "  (12917368283846924199, 56, 66),\n",
       "  (12917368283846924199, 57, 66),\n",
       "  (12917368283846924199, 58, 66),\n",
       "  (12917368283846924199, 59, 66),\n",
       "  (12917368283846924199, 60, 66),\n",
       "  (12917368283846924199, 61, 66),\n",
       "  (12917368283846924199, 47, 67),\n",
       "  (12917368283846924199, 48, 67),\n",
       "  (12917368283846924199, 49, 67),\n",
       "  (12917368283846924199, 50, 67),\n",
       "  (12917368283846924199, 51, 67),\n",
       "  (12917368283846924199, 52, 67),\n",
       "  (12917368283846924199, 53, 67),\n",
       "  (12917368283846924199, 54, 67),\n",
       "  (12917368283846924199, 55, 67),\n",
       "  (12917368283846924199, 56, 67),\n",
       "  (12917368283846924199, 57, 67),\n",
       "  (12917368283846924199, 58, 67),\n",
       "  (12917368283846924199, 59, 67),\n",
       "  (12917368283846924199, 60, 67),\n",
       "  (12917368283846924199, 61, 67),\n",
       "  (12917368283846924199, 396, 420),\n",
       "  (12917368283846924199, 397, 420),\n",
       "  (12917368283846924199, 398, 420),\n",
       "  (12917368283846924199, 399, 420),\n",
       "  (12917368283846924199, 400, 420),\n",
       "  (12917368283846924199, 401, 420),\n",
       "  (12917368283846924199, 402, 420),\n",
       "  (12917368283846924199, 403, 420),\n",
       "  (12917368283846924199, 404, 420),\n",
       "  (12917368283846924199, 405, 420),\n",
       "  (12917368283846924199, 406, 420),\n",
       "  (12917368283846924199, 407, 420),\n",
       "  (12917368283846924199, 408, 420),\n",
       "  (12917368283846924199, 409, 420),\n",
       "  (12917368283846924199, 410, 420),\n",
       "  (12917368283846924199, 411, 420),\n",
       "  (12917368283846924199, 412, 420),\n",
       "  (12917368283846924199, 413, 420),\n",
       "  (12917368283846924199, 414, 420),\n",
       "  (12917368283846924199, 415, 420),\n",
       "  (12917368283846924199, 416, 420),\n",
       "  (12917368283846924199, 417, 420),\n",
       "  (12917368283846924199, 418, 420),\n",
       "  (12917368283846924199, 419, 420),\n",
       "  (12917368283846924199, 396, 421),\n",
       "  (12917368283846924199, 397, 421),\n",
       "  (12917368283846924199, 398, 421),\n",
       "  (12917368283846924199, 399, 421),\n",
       "  (12917368283846924199, 400, 421),\n",
       "  (12917368283846924199, 401, 421),\n",
       "  (12917368283846924199, 402, 421),\n",
       "  (12917368283846924199, 403, 421),\n",
       "  (12917368283846924199, 404, 421),\n",
       "  (12917368283846924199, 405, 421),\n",
       "  (12917368283846924199, 406, 421),\n",
       "  (12917368283846924199, 407, 421),\n",
       "  (12917368283846924199, 408, 421),\n",
       "  (12917368283846924199, 409, 421),\n",
       "  (12917368283846924199, 410, 421),\n",
       "  (12917368283846924199, 411, 421),\n",
       "  (12917368283846924199, 412, 421),\n",
       "  (12917368283846924199, 413, 421),\n",
       "  (12917368283846924199, 414, 421),\n",
       "  (12917368283846924199, 415, 421),\n",
       "  (12917368283846924199, 416, 421),\n",
       "  (12917368283846924199, 417, 421),\n",
       "  (12917368283846924199, 418, 421),\n",
       "  (12917368283846924199, 419, 421),\n",
       "  (12917368283846924199, 396, 422),\n",
       "  (12917368283846924199, 397, 422),\n",
       "  (12917368283846924199, 398, 422),\n",
       "  (12917368283846924199, 399, 422),\n",
       "  (12917368283846924199, 400, 422),\n",
       "  (12917368283846924199, 401, 422),\n",
       "  (12917368283846924199, 402, 422),\n",
       "  (12917368283846924199, 403, 422),\n",
       "  (12917368283846924199, 404, 422),\n",
       "  (12917368283846924199, 405, 422),\n",
       "  (12917368283846924199, 406, 422),\n",
       "  (12917368283846924199, 407, 422),\n",
       "  (12917368283846924199, 408, 422),\n",
       "  (12917368283846924199, 409, 422),\n",
       "  (12917368283846924199, 410, 422),\n",
       "  (12917368283846924199, 411, 422),\n",
       "  (12917368283846924199, 412, 422),\n",
       "  (12917368283846924199, 413, 422),\n",
       "  (12917368283846924199, 414, 422),\n",
       "  (12917368283846924199, 415, 422),\n",
       "  (12917368283846924199, 416, 422),\n",
       "  (12917368283846924199, 417, 422),\n",
       "  (12917368283846924199, 418, 422),\n",
       "  (12917368283846924199, 419, 422),\n",
       "  (12917368283846924199, 396, 423),\n",
       "  (12917368283846924199, 397, 423),\n",
       "  (12917368283846924199, 398, 423),\n",
       "  (12917368283846924199, 399, 423),\n",
       "  (12917368283846924199, 400, 423),\n",
       "  (12917368283846924199, 401, 423),\n",
       "  (12917368283846924199, 402, 423),\n",
       "  (12917368283846924199, 403, 423),\n",
       "  (12917368283846924199, 404, 423),\n",
       "  (12917368283846924199, 405, 423),\n",
       "  (12917368283846924199, 406, 423),\n",
       "  (12917368283846924199, 407, 423),\n",
       "  (12917368283846924199, 408, 423),\n",
       "  (12917368283846924199, 409, 423),\n",
       "  (12917368283846924199, 410, 423),\n",
       "  (12917368283846924199, 411, 423),\n",
       "  (12917368283846924199, 412, 423),\n",
       "  (12917368283846924199, 413, 423),\n",
       "  (12917368283846924199, 414, 423),\n",
       "  (12917368283846924199, 415, 423),\n",
       "  (12917368283846924199, 416, 423),\n",
       "  (12917368283846924199, 417, 423),\n",
       "  (12917368283846924199, 418, 423),\n",
       "  (12917368283846924199, 419, 423)],\n",
       " [(12917368283846924199, 38, 55),\n",
       "  (12917368283846924199, 39, 55),\n",
       "  (12917368283846924199, 40, 55),\n",
       "  (12917368283846924199, 41, 55),\n",
       "  (12917368283846924199, 42, 55),\n",
       "  (12917368283846924199, 43, 55),\n",
       "  (12917368283846924199, 44, 55),\n",
       "  (12917368283846924199, 45, 55),\n",
       "  (12917368283846924199, 46, 55),\n",
       "  (12917368283846924199, 47, 55),\n",
       "  (12917368283846924199, 48, 55),\n",
       "  (12917368283846924199, 49, 55),\n",
       "  (12917368283846924199, 50, 55),\n",
       "  (12917368283846924199, 51, 55),\n",
       "  (12917368283846924199, 52, 55),\n",
       "  (12917368283846924199, 53, 55),\n",
       "  (12917368283846924199, 54, 55),\n",
       "  (12917368283846924199, 38, 56),\n",
       "  (12917368283846924199, 39, 56),\n",
       "  (12917368283846924199, 40, 56),\n",
       "  (12917368283846924199, 41, 56),\n",
       "  (12917368283846924199, 42, 56),\n",
       "  (12917368283846924199, 43, 56),\n",
       "  (12917368283846924199, 44, 56),\n",
       "  (12917368283846924199, 45, 56),\n",
       "  (12917368283846924199, 46, 56),\n",
       "  (12917368283846924199, 47, 56),\n",
       "  (12917368283846924199, 48, 56),\n",
       "  (12917368283846924199, 49, 56),\n",
       "  (12917368283846924199, 50, 56),\n",
       "  (12917368283846924199, 51, 56),\n",
       "  (12917368283846924199, 52, 56),\n",
       "  (12917368283846924199, 53, 56),\n",
       "  (12917368283846924199, 54, 56),\n",
       "  (12917368283846924199, 38, 57),\n",
       "  (12917368283846924199, 39, 57),\n",
       "  (12917368283846924199, 40, 57),\n",
       "  (12917368283846924199, 41, 57),\n",
       "  (12917368283846924199, 42, 57),\n",
       "  (12917368283846924199, 43, 57),\n",
       "  (12917368283846924199, 44, 57),\n",
       "  (12917368283846924199, 45, 57),\n",
       "  (12917368283846924199, 46, 57),\n",
       "  (12917368283846924199, 47, 57),\n",
       "  (12917368283846924199, 48, 57),\n",
       "  (12917368283846924199, 49, 57),\n",
       "  (12917368283846924199, 50, 57),\n",
       "  (12917368283846924199, 51, 57),\n",
       "  (12917368283846924199, 52, 57),\n",
       "  (12917368283846924199, 53, 57),\n",
       "  (12917368283846924199, 54, 57),\n",
       "  (12917368283846924199, 38, 58),\n",
       "  (12917368283846924199, 39, 58),\n",
       "  (12917368283846924199, 40, 58),\n",
       "  (12917368283846924199, 41, 58),\n",
       "  (12917368283846924199, 42, 58),\n",
       "  (12917368283846924199, 43, 58),\n",
       "  (12917368283846924199, 44, 58),\n",
       "  (12917368283846924199, 45, 58),\n",
       "  (12917368283846924199, 46, 58),\n",
       "  (12917368283846924199, 47, 58),\n",
       "  (12917368283846924199, 48, 58),\n",
       "  (12917368283846924199, 49, 58),\n",
       "  (12917368283846924199, 50, 58),\n",
       "  (12917368283846924199, 51, 58),\n",
       "  (12917368283846924199, 52, 58),\n",
       "  (12917368283846924199, 53, 58),\n",
       "  (12917368283846924199, 54, 58),\n",
       "  (12917368283846924199, 38, 59),\n",
       "  (12917368283846924199, 39, 59),\n",
       "  (12917368283846924199, 40, 59),\n",
       "  (12917368283846924199, 41, 59),\n",
       "  (12917368283846924199, 42, 59),\n",
       "  (12917368283846924199, 43, 59),\n",
       "  (12917368283846924199, 44, 59),\n",
       "  (12917368283846924199, 45, 59),\n",
       "  (12917368283846924199, 46, 59),\n",
       "  (12917368283846924199, 47, 59),\n",
       "  (12917368283846924199, 48, 59),\n",
       "  (12917368283846924199, 49, 59),\n",
       "  (12917368283846924199, 50, 59),\n",
       "  (12917368283846924199, 51, 59),\n",
       "  (12917368283846924199, 52, 59),\n",
       "  (12917368283846924199, 53, 59),\n",
       "  (12917368283846924199, 54, 59),\n",
       "  (12917368283846924199, 38, 60),\n",
       "  (12917368283846924199, 39, 60),\n",
       "  (12917368283846924199, 40, 60),\n",
       "  (12917368283846924199, 41, 60),\n",
       "  (12917368283846924199, 42, 60),\n",
       "  (12917368283846924199, 43, 60),\n",
       "  (12917368283846924199, 44, 60),\n",
       "  (12917368283846924199, 45, 60),\n",
       "  (12917368283846924199, 46, 60),\n",
       "  (12917368283846924199, 47, 60),\n",
       "  (12917368283846924199, 48, 60),\n",
       "  (12917368283846924199, 49, 60),\n",
       "  (12917368283846924199, 50, 60),\n",
       "  (12917368283846924199, 51, 60),\n",
       "  (12917368283846924199, 52, 60),\n",
       "  (12917368283846924199, 53, 60),\n",
       "  (12917368283846924199, 54, 60),\n",
       "  (12917368283846924199, 38, 61),\n",
       "  (12917368283846924199, 39, 61),\n",
       "  (12917368283846924199, 40, 61),\n",
       "  (12917368283846924199, 41, 61),\n",
       "  (12917368283846924199, 42, 61),\n",
       "  (12917368283846924199, 43, 61),\n",
       "  (12917368283846924199, 44, 61),\n",
       "  (12917368283846924199, 45, 61),\n",
       "  (12917368283846924199, 46, 61),\n",
       "  (12917368283846924199, 47, 61),\n",
       "  (12917368283846924199, 48, 61),\n",
       "  (12917368283846924199, 49, 61),\n",
       "  (12917368283846924199, 50, 61),\n",
       "  (12917368283846924199, 51, 61),\n",
       "  (12917368283846924199, 52, 61),\n",
       "  (12917368283846924199, 53, 61),\n",
       "  (12917368283846924199, 54, 61),\n",
       "  (12917368283846924199, 38, 62),\n",
       "  (12917368283846924199, 39, 62),\n",
       "  (12917368283846924199, 40, 62),\n",
       "  (12917368283846924199, 41, 62),\n",
       "  (12917368283846924199, 42, 62),\n",
       "  (12917368283846924199, 43, 62),\n",
       "  (12917368283846924199, 44, 62),\n",
       "  (12917368283846924199, 45, 62),\n",
       "  (12917368283846924199, 46, 62),\n",
       "  (12917368283846924199, 47, 62),\n",
       "  (12917368283846924199, 48, 62),\n",
       "  (12917368283846924199, 49, 62),\n",
       "  (12917368283846924199, 50, 62),\n",
       "  (12917368283846924199, 51, 62),\n",
       "  (12917368283846924199, 52, 62),\n",
       "  (12917368283846924199, 53, 62),\n",
       "  (12917368283846924199, 54, 62)]]"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "found_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "4e299dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12917368283846924199 skills 26 33 Indian Institute of Technology Kanpur Technical Skills\n",
      "12917368283846924199 skills 27 33 Institute of Technology Kanpur Technical Skills\n",
      "12917368283846924199 skills 28 33 of Technology Kanpur Technical Skills\n",
      "12917368283846924199 skills 29 33 Technology Kanpur Technical Skills\n",
      "12917368283846924199 skills 30 33 Kanpur Technical Skills\n",
      "12917368283846924199 skills 31 33 Technical Skills\n",
      "12917368283846924199 skills 32 33 Skills\n",
      "12917368283846924199 skills 26 34 Indian Institute of Technology Kanpur Technical Skills Programming\n",
      "12917368283846924199 skills 27 34 Institute of Technology Kanpur Technical Skills Programming\n",
      "12917368283846924199 skills 28 34 of Technology Kanpur Technical Skills Programming\n",
      "12917368283846924199 skills 29 34 Technology Kanpur Technical Skills Programming\n",
      "12917368283846924199 skills 30 34 Kanpur Technical Skills Programming\n",
      "12917368283846924199 skills 31 34 Technical Skills Programming\n",
      "12917368283846924199 skills 32 34 Skills Programming\n",
      "12917368283846924199 skills 26 35 Indian Institute of Technology Kanpur Technical Skills Programming languages\n",
      "12917368283846924199 skills 27 35 Institute of Technology Kanpur Technical Skills Programming languages\n",
      "12917368283846924199 skills 28 35 of Technology Kanpur Technical Skills Programming languages\n",
      "12917368283846924199 skills 29 35 Technology Kanpur Technical Skills Programming languages\n",
      "12917368283846924199 skills 30 35 Kanpur Technical Skills Programming languages\n",
      "12917368283846924199 skills 31 35 Technical Skills Programming languages\n",
      "12917368283846924199 skills 32 35 Skills Programming languages\n",
      "12917368283846924199 skills 26 36 Indian Institute of Technology Kanpur Technical Skills Programming languages C\n",
      "12917368283846924199 skills 27 36 Institute of Technology Kanpur Technical Skills Programming languages C\n",
      "12917368283846924199 skills 28 36 of Technology Kanpur Technical Skills Programming languages C\n",
      "12917368283846924199 skills 29 36 Technology Kanpur Technical Skills Programming languages C\n",
      "12917368283846924199 skills 30 36 Kanpur Technical Skills Programming languages C\n",
      "12917368283846924199 skills 31 36 Technical Skills Programming languages C\n",
      "12917368283846924199 skills 32 36 Skills Programming languages C\n",
      "12917368283846924199 skills 37 49 Department of Mechanical EngineeringThesis on Machine Learning applications in rural healthcareTechnical Skills\n",
      "12917368283846924199 skills 38 49 of Mechanical EngineeringThesis on Machine Learning applications in rural healthcareTechnical Skills\n",
      "12917368283846924199 skills 39 49 Mechanical EngineeringThesis on Machine Learning applications in rural healthcareTechnical Skills\n",
      "12917368283846924199 skills 40 49 EngineeringThesis on Machine Learning applications in rural healthcareTechnical Skills\n",
      "12917368283846924199 skills 41 49 on Machine Learning applications in rural healthcareTechnical Skills\n",
      "12917368283846924199 skills 42 49 Machine Learning applications in rural healthcareTechnical Skills\n",
      "12917368283846924199 skills 43 49 Learning applications in rural healthcareTechnical Skills\n",
      "12917368283846924199 skills 44 49 applications in rural healthcareTechnical Skills\n",
      "12917368283846924199 skills 45 49 in rural healthcareTechnical Skills\n",
      "12917368283846924199 skills 46 49 rural healthcareTechnical Skills\n",
      "12917368283846924199 skills 47 49 healthcareTechnical Skills\n",
      "12917368283846924199 skills 48 49 Skills\n",
      "12917368283846924199 skills 150 152 TECHNICAL SKILLS\n",
      "12917368283846924199 skills 151 152 SKILLS\n",
      "12917368283846924199 skills 150 153 TECHNICAL SKILLS SUMMARY\n",
      "12917368283846924199 skills 151 153 SKILLS SUMMARY\n",
      "12917368283846924199 skills 35 53 To work in pragmatic way in an organisation where I can show my talent and enhance my skills\n",
      "12917368283846924199 skills 36 53 work in pragmatic way in an organisation where I can show my talent and enhance my skills\n",
      "12917368283846924199 skills 37 53 in pragmatic way in an organisation where I can show my talent and enhance my skills\n",
      "12917368283846924199 skills 38 53 pragmatic way in an organisation where I can show my talent and enhance my skills\n",
      "12917368283846924199 skills 39 53 way in an organisation where I can show my talent and enhance my skills\n",
      "12917368283846924199 skills 40 53 in an organisation where I can show my talent and enhance my skills\n",
      "12917368283846924199 skills 41 53 an organisation where I can show my talent and enhance my skills\n",
      "12917368283846924199 skills 42 53 organisation where I can show my talent and enhance my skills\n",
      "12917368283846924199 skills 43 53 where I can show my talent and enhance my skills\n",
      "12917368283846924199 skills 44 53 I can show my talent and enhance my skills\n",
      "12917368283846924199 skills 45 53 can show my talent and enhance my skills\n",
      "12917368283846924199 skills 46 53 show my talent and enhance my skills\n",
      "12917368283846924199 skills 47 53 my talent and enhance my skills\n",
      "12917368283846924199 skills 48 53 talent and enhance my skills\n",
      "12917368283846924199 skills 49 53 and enhance my skills\n",
      "12917368283846924199 skills 50 53 enhance my skills\n",
      "12917368283846924199 skills 51 53 my skills\n",
      "12917368283846924199 skills 52 53 skills\n",
      "12917368283846924199 skills 35 54 To work in pragmatic way in an organisation where I can show my talent and enhance my skills to\n",
      "12917368283846924199 skills 36 54 work in pragmatic way in an organisation where I can show my talent and enhance my skills to\n",
      "12917368283846924199 skills 37 54 in pragmatic way in an organisation where I can show my talent and enhance my skills to\n",
      "12917368283846924199 skills 38 54 pragmatic way in an organisation where I can show my talent and enhance my skills to\n",
      "12917368283846924199 skills 39 54 way in an organisation where I can show my talent and enhance my skills to\n",
      "12917368283846924199 skills 40 54 in an organisation where I can show my talent and enhance my skills to\n",
      "12917368283846924199 skills 41 54 an organisation where I can show my talent and enhance my skills to\n",
      "12917368283846924199 skills 42 54 organisation where I can show my talent and enhance my skills to\n",
      "12917368283846924199 skills 43 54 where I can show my talent and enhance my skills to\n",
      "12917368283846924199 skills 44 54 I can show my talent and enhance my skills to\n",
      "12917368283846924199 skills 45 54 can show my talent and enhance my skills to\n",
      "12917368283846924199 skills 46 54 show my talent and enhance my skills to\n",
      "12917368283846924199 skills 47 54 my talent and enhance my skills to\n",
      "12917368283846924199 skills 48 54 talent and enhance my skills to\n",
      "12917368283846924199 skills 49 54 and enhance my skills to\n",
      "12917368283846924199 skills 50 54 enhance my skills to\n",
      "12917368283846924199 skills 51 54 my skills to\n",
      "12917368283846924199 skills 52 54 skills to\n",
      "12917368283846924199 skills 35 55 To work in pragmatic way in an organisation where I can show my talent and enhance my skills to meet\n",
      "12917368283846924199 skills 36 55 work in pragmatic way in an organisation where I can show my talent and enhance my skills to meet\n",
      "12917368283846924199 skills 37 55 in pragmatic way in an organisation where I can show my talent and enhance my skills to meet\n",
      "12917368283846924199 skills 38 55 pragmatic way in an organisation where I can show my talent and enhance my skills to meet\n",
      "12917368283846924199 skills 39 55 way in an organisation where I can show my talent and enhance my skills to meet\n",
      "12917368283846924199 skills 40 55 in an organisation where I can show my talent and enhance my skills to meet\n",
      "12917368283846924199 skills 41 55 an organisation where I can show my talent and enhance my skills to meet\n",
      "12917368283846924199 skills 42 55 organisation where I can show my talent and enhance my skills to meet\n",
      "12917368283846924199 skills 43 55 where I can show my talent and enhance my skills to meet\n",
      "12917368283846924199 skills 44 55 I can show my talent and enhance my skills to meet\n",
      "12917368283846924199 skills 45 55 can show my talent and enhance my skills to meet\n",
      "12917368283846924199 skills 46 55 show my talent and enhance my skills to meet\n",
      "12917368283846924199 skills 47 55 my talent and enhance my skills to meet\n",
      "12917368283846924199 skills 48 55 talent and enhance my skills to meet\n",
      "12917368283846924199 skills 49 55 and enhance my skills to meet\n",
      "12917368283846924199 skills 50 55 enhance my skills to meet\n",
      "12917368283846924199 skills 51 55 my skills to meet\n",
      "12917368283846924199 skills 52 55 skills to meet\n",
      "12917368283846924199 skills 35 56 To work in pragmatic way in an organisation where I can show my talent and enhance my skills to meet company\n",
      "12917368283846924199 skills 36 56 work in pragmatic way in an organisation where I can show my talent and enhance my skills to meet company\n",
      "12917368283846924199 skills 37 56 in pragmatic way in an organisation where I can show my talent and enhance my skills to meet company\n",
      "12917368283846924199 skills 38 56 pragmatic way in an organisation where I can show my talent and enhance my skills to meet company\n",
      "12917368283846924199 skills 39 56 way in an organisation where I can show my talent and enhance my skills to meet company\n",
      "12917368283846924199 skills 40 56 in an organisation where I can show my talent and enhance my skills to meet company\n",
      "12917368283846924199 skills 41 56 an organisation where I can show my talent and enhance my skills to meet company\n",
      "12917368283846924199 skills 42 56 organisation where I can show my talent and enhance my skills to meet company\n",
      "12917368283846924199 skills 43 56 where I can show my talent and enhance my skills to meet company\n",
      "12917368283846924199 skills 44 56 I can show my talent and enhance my skills to meet company\n",
      "12917368283846924199 skills 45 56 can show my talent and enhance my skills to meet company\n",
      "12917368283846924199 skills 46 56 show my talent and enhance my skills to meet company\n",
      "12917368283846924199 skills 47 56 my talent and enhance my skills to meet company\n",
      "12917368283846924199 skills 48 56 talent and enhance my skills to meet company\n",
      "12917368283846924199 skills 49 56 and enhance my skills to meet company\n",
      "12917368283846924199 skills 50 56 enhance my skills to meet company\n",
      "12917368283846924199 skills 51 56 my skills to meet company\n",
      "12917368283846924199 skills 52 56 skills to meet company\n",
      "12917368283846924199 skills 35 57 To work in pragmatic way in an organisation where I can show my talent and enhance my skills to meet company goals\n",
      "12917368283846924199 skills 36 57 work in pragmatic way in an organisation where I can show my talent and enhance my skills to meet company goals\n",
      "12917368283846924199 skills 37 57 in pragmatic way in an organisation where I can show my talent and enhance my skills to meet company goals\n",
      "12917368283846924199 skills 38 57 pragmatic way in an organisation where I can show my talent and enhance my skills to meet company goals\n",
      "12917368283846924199 skills 39 57 way in an organisation where I can show my talent and enhance my skills to meet company goals\n",
      "12917368283846924199 skills 40 57 in an organisation where I can show my talent and enhance my skills to meet company goals\n",
      "12917368283846924199 skills 41 57 an organisation where I can show my talent and enhance my skills to meet company goals\n",
      "12917368283846924199 skills 42 57 organisation where I can show my talent and enhance my skills to meet company goals\n",
      "12917368283846924199 skills 43 57 where I can show my talent and enhance my skills to meet company goals\n",
      "12917368283846924199 skills 44 57 I can show my talent and enhance my skills to meet company goals\n",
      "12917368283846924199 skills 45 57 can show my talent and enhance my skills to meet company goals\n",
      "12917368283846924199 skills 46 57 show my talent and enhance my skills to meet company goals\n",
      "12917368283846924199 skills 47 57 my talent and enhance my skills to meet company goals\n",
      "12917368283846924199 skills 48 57 talent and enhance my skills to meet company goals\n",
      "12917368283846924199 skills 49 57 and enhance my skills to meet company goals\n",
      "12917368283846924199 skills 50 57 enhance my skills to meet company goals\n",
      "12917368283846924199 skills 51 57 my skills to meet company goals\n",
      "12917368283846924199 skills 52 57 skills to meet company goals\n",
      "12917368283846924199 skills 35 58 To work in pragmatic way in an organisation where I can show my talent and enhance my skills to meet company goals and\n",
      "12917368283846924199 skills 36 58 work in pragmatic way in an organisation where I can show my talent and enhance my skills to meet company goals and\n",
      "12917368283846924199 skills 37 58 in pragmatic way in an organisation where I can show my talent and enhance my skills to meet company goals and\n",
      "12917368283846924199 skills 38 58 pragmatic way in an organisation where I can show my talent and enhance my skills to meet company goals and\n",
      "12917368283846924199 skills 39 58 way in an organisation where I can show my talent and enhance my skills to meet company goals and\n",
      "12917368283846924199 skills 40 58 in an organisation where I can show my talent and enhance my skills to meet company goals and\n",
      "12917368283846924199 skills 41 58 an organisation where I can show my talent and enhance my skills to meet company goals and\n",
      "12917368283846924199 skills 42 58 organisation where I can show my talent and enhance my skills to meet company goals and\n",
      "12917368283846924199 skills 43 58 where I can show my talent and enhance my skills to meet company goals and\n",
      "12917368283846924199 skills 44 58 I can show my talent and enhance my skills to meet company goals and\n",
      "12917368283846924199 skills 45 58 can show my talent and enhance my skills to meet company goals and\n",
      "12917368283846924199 skills 46 58 show my talent and enhance my skills to meet company goals and\n",
      "12917368283846924199 skills 47 58 my talent and enhance my skills to meet company goals and\n",
      "12917368283846924199 skills 48 58 talent and enhance my skills to meet company goals and\n",
      "12917368283846924199 skills 49 58 and enhance my skills to meet company goals and\n",
      "12917368283846924199 skills 50 58 enhance my skills to meet company goals and\n",
      "12917368283846924199 skills 51 58 my skills to meet company goals and\n",
      "12917368283846924199 skills 52 58 skills to meet company goals and\n",
      "12917368283846924199 skills 35 59 To work in pragmatic way in an organisation where I can show my talent and enhance my skills to meet company goals and objectives\n",
      "12917368283846924199 skills 36 59 work in pragmatic way in an organisation where I can show my talent and enhance my skills to meet company goals and objectives\n",
      "12917368283846924199 skills 37 59 in pragmatic way in an organisation where I can show my talent and enhance my skills to meet company goals and objectives\n",
      "12917368283846924199 skills 38 59 pragmatic way in an organisation where I can show my talent and enhance my skills to meet company goals and objectives\n",
      "12917368283846924199 skills 39 59 way in an organisation where I can show my talent and enhance my skills to meet company goals and objectives\n",
      "12917368283846924199 skills 40 59 in an organisation where I can show my talent and enhance my skills to meet company goals and objectives\n",
      "12917368283846924199 skills 41 59 an organisation where I can show my talent and enhance my skills to meet company goals and objectives\n",
      "12917368283846924199 skills 42 59 organisation where I can show my talent and enhance my skills to meet company goals and objectives\n",
      "12917368283846924199 skills 43 59 where I can show my talent and enhance my skills to meet company goals and objectives\n",
      "12917368283846924199 skills 44 59 I can show my talent and enhance my skills to meet company goals and objectives\n",
      "12917368283846924199 skills 45 59 can show my talent and enhance my skills to meet company goals and objectives\n",
      "12917368283846924199 skills 46 59 show my talent and enhance my skills to meet company goals and objectives\n",
      "12917368283846924199 skills 47 59 my talent and enhance my skills to meet company goals and objectives\n",
      "12917368283846924199 skills 48 59 talent and enhance my skills to meet company goals and objectives\n",
      "12917368283846924199 skills 49 59 and enhance my skills to meet company goals and objectives\n",
      "12917368283846924199 skills 50 59 enhance my skills to meet company goals and objectives\n",
      "12917368283846924199 skills 51 59 my skills to meet company goals and objectives\n",
      "12917368283846924199 skills 52 59 skills to meet company goals and objectives\n",
      "12917368283846924199 skills 35 60 To work in pragmatic way in an organisation where I can show my talent and enhance my skills to meet company goals and objectives with\n",
      "12917368283846924199 skills 36 60 work in pragmatic way in an organisation where I can show my talent and enhance my skills to meet company goals and objectives with\n",
      "12917368283846924199 skills 37 60 in pragmatic way in an organisation where I can show my talent and enhance my skills to meet company goals and objectives with\n",
      "12917368283846924199 skills 38 60 pragmatic way in an organisation where I can show my talent and enhance my skills to meet company goals and objectives with\n",
      "12917368283846924199 skills 39 60 way in an organisation where I can show my talent and enhance my skills to meet company goals and objectives with\n",
      "12917368283846924199 skills 40 60 in an organisation where I can show my talent and enhance my skills to meet company goals and objectives with\n",
      "12917368283846924199 skills 41 60 an organisation where I can show my talent and enhance my skills to meet company goals and objectives with\n",
      "12917368283846924199 skills 42 60 organisation where I can show my talent and enhance my skills to meet company goals and objectives with\n",
      "12917368283846924199 skills 43 60 where I can show my talent and enhance my skills to meet company goals and objectives with\n",
      "12917368283846924199 skills 44 60 I can show my talent and enhance my skills to meet company goals and objectives with\n",
      "12917368283846924199 skills 45 60 can show my talent and enhance my skills to meet company goals and objectives with\n",
      "12917368283846924199 skills 46 60 show my talent and enhance my skills to meet company goals and objectives with\n",
      "12917368283846924199 skills 47 60 my talent and enhance my skills to meet company goals and objectives with\n",
      "12917368283846924199 skills 48 60 talent and enhance my skills to meet company goals and objectives with\n",
      "12917368283846924199 skills 49 60 and enhance my skills to meet company goals and objectives with\n",
      "12917368283846924199 skills 50 60 enhance my skills to meet company goals and objectives with\n",
      "12917368283846924199 skills 51 60 my skills to meet company goals and objectives with\n",
      "12917368283846924199 skills 52 60 skills to meet company goals and objectives with\n",
      "12917368283846924199 skills 35 61 To work in pragmatic way in an organisation where I can show my talent and enhance my skills to meet company goals and objectives with full\n",
      "12917368283846924199 skills 36 61 work in pragmatic way in an organisation where I can show my talent and enhance my skills to meet company goals and objectives with full\n",
      "12917368283846924199 skills 37 61 in pragmatic way in an organisation where I can show my talent and enhance my skills to meet company goals and objectives with full\n",
      "12917368283846924199 skills 38 61 pragmatic way in an organisation where I can show my talent and enhance my skills to meet company goals and objectives with full\n",
      "12917368283846924199 skills 39 61 way in an organisation where I can show my talent and enhance my skills to meet company goals and objectives with full\n",
      "12917368283846924199 skills 40 61 in an organisation where I can show my talent and enhance my skills to meet company goals and objectives with full\n",
      "12917368283846924199 skills 41 61 an organisation where I can show my talent and enhance my skills to meet company goals and objectives with full\n",
      "12917368283846924199 skills 42 61 organisation where I can show my talent and enhance my skills to meet company goals and objectives with full\n",
      "12917368283846924199 skills 43 61 where I can show my talent and enhance my skills to meet company goals and objectives with full\n",
      "12917368283846924199 skills 44 61 I can show my talent and enhance my skills to meet company goals and objectives with full\n",
      "12917368283846924199 skills 45 61 can show my talent and enhance my skills to meet company goals and objectives with full\n",
      "12917368283846924199 skills 46 61 show my talent and enhance my skills to meet company goals and objectives with full\n",
      "12917368283846924199 skills 47 61 my talent and enhance my skills to meet company goals and objectives with full\n",
      "12917368283846924199 skills 48 61 talent and enhance my skills to meet company goals and objectives with full\n",
      "12917368283846924199 skills 49 61 and enhance my skills to meet company goals and objectives with full\n",
      "12917368283846924199 skills 50 61 enhance my skills to meet company goals and objectives with full\n",
      "12917368283846924199 skills 51 61 my skills to meet company goals and objectives with full\n",
      "12917368283846924199 skills 52 61 skills to meet company goals and objectives with full\n",
      "12917368283846924199 skills 35 62 To work in pragmatic way in an organisation where I can show my talent and enhance my skills to meet company goals and objectives with full intensity\n",
      "12917368283846924199 skills 36 62 work in pragmatic way in an organisation where I can show my talent and enhance my skills to meet company goals and objectives with full intensity\n",
      "12917368283846924199 skills 37 62 in pragmatic way in an organisation where I can show my talent and enhance my skills to meet company goals and objectives with full intensity\n",
      "12917368283846924199 skills 38 62 pragmatic way in an organisation where I can show my talent and enhance my skills to meet company goals and objectives with full intensity\n",
      "12917368283846924199 skills 39 62 way in an organisation where I can show my talent and enhance my skills to meet company goals and objectives with full intensity\n",
      "12917368283846924199 skills 40 62 in an organisation where I can show my talent and enhance my skills to meet company goals and objectives with full intensity\n",
      "12917368283846924199 skills 41 62 an organisation where I can show my talent and enhance my skills to meet company goals and objectives with full intensity\n",
      "12917368283846924199 skills 42 62 organisation where I can show my talent and enhance my skills to meet company goals and objectives with full intensity\n",
      "12917368283846924199 skills 43 62 where I can show my talent and enhance my skills to meet company goals and objectives with full intensity\n",
      "12917368283846924199 skills 44 62 I can show my talent and enhance my skills to meet company goals and objectives with full intensity\n",
      "12917368283846924199 skills 45 62 can show my talent and enhance my skills to meet company goals and objectives with full intensity\n",
      "12917368283846924199 skills 46 62 show my talent and enhance my skills to meet company goals and objectives with full intensity\n",
      "12917368283846924199 skills 47 62 my talent and enhance my skills to meet company goals and objectives with full intensity\n",
      "12917368283846924199 skills 48 62 talent and enhance my skills to meet company goals and objectives with full intensity\n",
      "12917368283846924199 skills 49 62 and enhance my skills to meet company goals and objectives with full intensity\n",
      "12917368283846924199 skills 50 62 enhance my skills to meet company goals and objectives with full intensity\n",
      "12917368283846924199 skills 51 62 my skills to meet company goals and objectives with full intensity\n",
      "12917368283846924199 skills 52 62 skills to meet company goals and objectives with full intensity\n",
      "12917368283846924199 skills 144 148 Machine learning TECHNICAL SKILLS\n",
      "12917368283846924199 skills 145 148 learning TECHNICAL SKILLS\n",
      "12917368283846924199 skills 146 148 TECHNICAL SKILLS\n",
      "12917368283846924199 skills 147 148 SKILLS\n",
      "12917368283846924199 skills 1 7 EDUCATION INTERNSHIP PROJECTS RELEVANT COURSEWORK SKILLS\n",
      "12917368283846924199 skills 2 7 INTERNSHIP PROJECTS RELEVANT COURSEWORK SKILLS\n",
      "12917368283846924199 skills 3 7 PROJECTS RELEVANT COURSEWORK SKILLS\n",
      "12917368283846924199 skills 4 7 RELEVANT COURSEWORK SKILLS\n",
      "12917368283846924199 skills 5 7 COURSEWORK SKILLS\n",
      "12917368283846924199 skills 6 7 SKILLS\n",
      "12917368283846924199 skills 1 8 EDUCATION INTERNSHIP PROJECTS RELEVANT COURSEWORK SKILLS POSITIONS\n",
      "12917368283846924199 skills 2 8 INTERNSHIP PROJECTS RELEVANT COURSEWORK SKILLS POSITIONS\n",
      "12917368283846924199 skills 3 8 PROJECTS RELEVANT COURSEWORK SKILLS POSITIONS\n",
      "12917368283846924199 skills 4 8 RELEVANT COURSEWORK SKILLS POSITIONS\n",
      "12917368283846924199 skills 5 8 COURSEWORK SKILLS POSITIONS\n",
      "12917368283846924199 skills 6 8 SKILLS POSITIONS\n",
      "12917368283846924199 skills 1 9 EDUCATION INTERNSHIP PROJECTS RELEVANT COURSEWORK SKILLS POSITIONS OF\n",
      "12917368283846924199 skills 2 9 INTERNSHIP PROJECTS RELEVANT COURSEWORK SKILLS POSITIONS OF\n",
      "12917368283846924199 skills 3 9 PROJECTS RELEVANT COURSEWORK SKILLS POSITIONS OF\n",
      "12917368283846924199 skills 4 9 RELEVANT COURSEWORK SKILLS POSITIONS OF\n",
      "12917368283846924199 skills 5 9 COURSEWORK SKILLS POSITIONS OF\n",
      "12917368283846924199 skills 6 9 SKILLS POSITIONS OF\n",
      "12917368283846924199 skills 1 10 EDUCATION INTERNSHIP PROJECTS RELEVANT COURSEWORK SKILLS POSITIONS OF RESPONSIBILITY\n",
      "12917368283846924199 skills 2 10 INTERNSHIP PROJECTS RELEVANT COURSEWORK SKILLS POSITIONS OF RESPONSIBILITY\n",
      "12917368283846924199 skills 3 10 PROJECTS RELEVANT COURSEWORK SKILLS POSITIONS OF RESPONSIBILITY\n",
      "12917368283846924199 skills 4 10 RELEVANT COURSEWORK SKILLS POSITIONS OF RESPONSIBILITY\n",
      "12917368283846924199 skills 5 10 COURSEWORK SKILLS POSITIONS OF RESPONSIBILITY\n",
      "12917368283846924199 skills 6 10 SKILLS POSITIONS OF RESPONSIBILITY\n",
      "12917368283846924199 skills 1 11 EDUCATION INTERNSHIP PROJECTS RELEVANT COURSEWORK SKILLS POSITIONS OF RESPONSIBILITY AND\n",
      "12917368283846924199 skills 2 11 INTERNSHIP PROJECTS RELEVANT COURSEWORK SKILLS POSITIONS OF RESPONSIBILITY AND\n",
      "12917368283846924199 skills 3 11 PROJECTS RELEVANT COURSEWORK SKILLS POSITIONS OF RESPONSIBILITY AND\n",
      "12917368283846924199 skills 4 11 RELEVANT COURSEWORK SKILLS POSITIONS OF RESPONSIBILITY AND\n",
      "12917368283846924199 skills 5 11 COURSEWORK SKILLS POSITIONS OF RESPONSIBILITY AND\n",
      "12917368283846924199 skills 6 11 SKILLS POSITIONS OF RESPONSIBILITY AND\n",
      "12917368283846924199 skills 1 12 EDUCATION INTERNSHIP PROJECTS RELEVANT COURSEWORK SKILLS POSITIONS OF RESPONSIBILITY AND AWARDS\n",
      "12917368283846924199 skills 2 12 INTERNSHIP PROJECTS RELEVANT COURSEWORK SKILLS POSITIONS OF RESPONSIBILITY AND AWARDS\n",
      "12917368283846924199 skills 3 12 PROJECTS RELEVANT COURSEWORK SKILLS POSITIONS OF RESPONSIBILITY AND AWARDS\n",
      "12917368283846924199 skills 4 12 RELEVANT COURSEWORK SKILLS POSITIONS OF RESPONSIBILITY AND AWARDS\n",
      "12917368283846924199 skills 5 12 COURSEWORK SKILLS POSITIONS OF RESPONSIBILITY AND AWARDS\n",
      "12917368283846924199 skills 6 12 SKILLS POSITIONS OF RESPONSIBILITY AND AWARDS\n",
      "12917368283846924199 skills 1 13 EDUCATION INTERNSHIP PROJECTS RELEVANT COURSEWORK SKILLS POSITIONS OF RESPONSIBILITY AND AWARDS WORK\n",
      "12917368283846924199 skills 2 13 INTERNSHIP PROJECTS RELEVANT COURSEWORK SKILLS POSITIONS OF RESPONSIBILITY AND AWARDS WORK\n",
      "12917368283846924199 skills 3 13 PROJECTS RELEVANT COURSEWORK SKILLS POSITIONS OF RESPONSIBILITY AND AWARDS WORK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12917368283846924199 skills 4 13 RELEVANT COURSEWORK SKILLS POSITIONS OF RESPONSIBILITY AND AWARDS WORK\n",
      "12917368283846924199 skills 5 13 COURSEWORK SKILLS POSITIONS OF RESPONSIBILITY AND AWARDS WORK\n",
      "12917368283846924199 skills 6 13 SKILLS POSITIONS OF RESPONSIBILITY AND AWARDS WORK\n",
      "12917368283846924199 skills 1 14 EDUCATION INTERNSHIP PROJECTS RELEVANT COURSEWORK SKILLS POSITIONS OF RESPONSIBILITY AND AWARDS WORK EXPERIENCE\n",
      "12917368283846924199 skills 2 14 INTERNSHIP PROJECTS RELEVANT COURSEWORK SKILLS POSITIONS OF RESPONSIBILITY AND AWARDS WORK EXPERIENCE\n",
      "12917368283846924199 skills 3 14 PROJECTS RELEVANT COURSEWORK SKILLS POSITIONS OF RESPONSIBILITY AND AWARDS WORK EXPERIENCE\n",
      "12917368283846924199 skills 4 14 RELEVANT COURSEWORK SKILLS POSITIONS OF RESPONSIBILITY AND AWARDS WORK EXPERIENCE\n",
      "12917368283846924199 skills 5 14 COURSEWORK SKILLS POSITIONS OF RESPONSIBILITY AND AWARDS WORK EXPERIENCE\n",
      "12917368283846924199 skills 6 14 SKILLS POSITIONS OF RESPONSIBILITY AND AWARDS WORK EXPERIENCE\n",
      "12917368283846924199 skills 1 15 EDUCATION INTERNSHIP PROJECTS RELEVANT COURSEWORK SKILLS POSITIONS OF RESPONSIBILITY AND AWARDS WORK EXPERIENCE COMPETITION\n",
      "12917368283846924199 skills 2 15 INTERNSHIP PROJECTS RELEVANT COURSEWORK SKILLS POSITIONS OF RESPONSIBILITY AND AWARDS WORK EXPERIENCE COMPETITION\n",
      "12917368283846924199 skills 3 15 PROJECTS RELEVANT COURSEWORK SKILLS POSITIONS OF RESPONSIBILITY AND AWARDS WORK EXPERIENCE COMPETITION\n",
      "12917368283846924199 skills 4 15 RELEVANT COURSEWORK SKILLS POSITIONS OF RESPONSIBILITY AND AWARDS WORK EXPERIENCE COMPETITION\n",
      "12917368283846924199 skills 5 15 COURSEWORK SKILLS POSITIONS OF RESPONSIBILITY AND AWARDS WORK EXPERIENCE COMPETITION\n",
      "12917368283846924199 skills 6 15 SKILLS POSITIONS OF RESPONSIBILITY AND AWARDS WORK EXPERIENCE COMPETITION\n",
      "12917368283846924199 skills 1 16 EDUCATION INTERNSHIP PROJECTS RELEVANT COURSEWORK SKILLS POSITIONS OF RESPONSIBILITY AND AWARDS WORK EXPERIENCE COMPETITION AYUSH\n",
      "12917368283846924199 skills 2 16 INTERNSHIP PROJECTS RELEVANT COURSEWORK SKILLS POSITIONS OF RESPONSIBILITY AND AWARDS WORK EXPERIENCE COMPETITION AYUSH\n",
      "12917368283846924199 skills 3 16 PROJECTS RELEVANT COURSEWORK SKILLS POSITIONS OF RESPONSIBILITY AND AWARDS WORK EXPERIENCE COMPETITION AYUSH\n",
      "12917368283846924199 skills 4 16 RELEVANT COURSEWORK SKILLS POSITIONS OF RESPONSIBILITY AND AWARDS WORK EXPERIENCE COMPETITION AYUSH\n",
      "12917368283846924199 skills 5 16 COURSEWORK SKILLS POSITIONS OF RESPONSIBILITY AND AWARDS WORK EXPERIENCE COMPETITION AYUSH\n",
      "12917368283846924199 skills 6 16 SKILLS POSITIONS OF RESPONSIBILITY AND AWARDS WORK EXPERIENCE COMPETITION AYUSH\n",
      "12917368283846924199 skills 1 17 EDUCATION INTERNSHIP PROJECTS RELEVANT COURSEWORK SKILLS POSITIONS OF RESPONSIBILITY AND AWARDS WORK EXPERIENCE COMPETITION AYUSH PALIWAL\n",
      "12917368283846924199 skills 2 17 INTERNSHIP PROJECTS RELEVANT COURSEWORK SKILLS POSITIONS OF RESPONSIBILITY AND AWARDS WORK EXPERIENCE COMPETITION AYUSH PALIWAL\n",
      "12917368283846924199 skills 3 17 PROJECTS RELEVANT COURSEWORK SKILLS POSITIONS OF RESPONSIBILITY AND AWARDS WORK EXPERIENCE COMPETITION AYUSH PALIWAL\n",
      "12917368283846924199 skills 4 17 RELEVANT COURSEWORK SKILLS POSITIONS OF RESPONSIBILITY AND AWARDS WORK EXPERIENCE COMPETITION AYUSH PALIWAL\n",
      "12917368283846924199 skills 5 17 COURSEWORK SKILLS POSITIONS OF RESPONSIBILITY AND AWARDS WORK EXPERIENCE COMPETITION AYUSH PALIWAL\n",
      "12917368283846924199 skills 6 17 SKILLS POSITIONS OF RESPONSIBILITY AND AWARDS WORK EXPERIENCE COMPETITION AYUSH PALIWAL\n",
      "12917368283846924199 skills 1 18 EDUCATION INTERNSHIP PROJECTS RELEVANT COURSEWORK SKILLS POSITIONS OF RESPONSIBILITY AND AWARDS WORK EXPERIENCE COMPETITION AYUSH PALIWAL AEROSPACE\n",
      "12917368283846924199 skills 2 18 INTERNSHIP PROJECTS RELEVANT COURSEWORK SKILLS POSITIONS OF RESPONSIBILITY AND AWARDS WORK EXPERIENCE COMPETITION AYUSH PALIWAL AEROSPACE\n",
      "12917368283846924199 skills 3 18 PROJECTS RELEVANT COURSEWORK SKILLS POSITIONS OF RESPONSIBILITY AND AWARDS WORK EXPERIENCE COMPETITION AYUSH PALIWAL AEROSPACE\n",
      "12917368283846924199 skills 4 18 RELEVANT COURSEWORK SKILLS POSITIONS OF RESPONSIBILITY AND AWARDS WORK EXPERIENCE COMPETITION AYUSH PALIWAL AEROSPACE\n",
      "12917368283846924199 skills 5 18 COURSEWORK SKILLS POSITIONS OF RESPONSIBILITY AND AWARDS WORK EXPERIENCE COMPETITION AYUSH PALIWAL AEROSPACE\n",
      "12917368283846924199 skills 6 18 SKILLS POSITIONS OF RESPONSIBILITY AND AWARDS WORK EXPERIENCE COMPETITION AYUSH PALIWAL AEROSPACE\n",
      "12917368283846924199 skills 1 19 EDUCATION INTERNSHIP PROJECTS RELEVANT COURSEWORK SKILLS POSITIONS OF RESPONSIBILITY AND AWARDS WORK EXPERIENCE COMPETITION AYUSH PALIWAL AEROSPACE ENGINEER\n",
      "12917368283846924199 skills 2 19 INTERNSHIP PROJECTS RELEVANT COURSEWORK SKILLS POSITIONS OF RESPONSIBILITY AND AWARDS WORK EXPERIENCE COMPETITION AYUSH PALIWAL AEROSPACE ENGINEER\n",
      "12917368283846924199 skills 3 19 PROJECTS RELEVANT COURSEWORK SKILLS POSITIONS OF RESPONSIBILITY AND AWARDS WORK EXPERIENCE COMPETITION AYUSH PALIWAL AEROSPACE ENGINEER\n",
      "12917368283846924199 skills 4 19 RELEVANT COURSEWORK SKILLS POSITIONS OF RESPONSIBILITY AND AWARDS WORK EXPERIENCE COMPETITION AYUSH PALIWAL AEROSPACE ENGINEER\n",
      "12917368283846924199 skills 5 19 COURSEWORK SKILLS POSITIONS OF RESPONSIBILITY AND AWARDS WORK EXPERIENCE COMPETITION AYUSH PALIWAL AEROSPACE ENGINEER\n",
      "12917368283846924199 skills 6 19 SKILLS POSITIONS OF RESPONSIBILITY AND AWARDS WORK EXPERIENCE COMPETITION AYUSH PALIWAL AEROSPACE ENGINEER\n",
      "12917368283846924199 skills 666 678 among various other models was found to be the optimal model SKILLS\n",
      "12917368283846924199 skills 667 678 various other models was found to be the optimal model SKILLS\n",
      "12917368283846924199 skills 668 678 other models was found to be the optimal model SKILLS\n",
      "12917368283846924199 skills 669 678 models was found to be the optimal model SKILLS\n",
      "12917368283846924199 skills 670 678 was found to be the optimal model SKILLS\n",
      "12917368283846924199 skills 671 678 found to be the optimal model SKILLS\n",
      "12917368283846924199 skills 672 678 to be the optimal model SKILLS\n",
      "12917368283846924199 skills 673 678 be the optimal model SKILLS\n",
      "12917368283846924199 skills 674 678 the optimal model SKILLS\n",
      "12917368283846924199 skills 675 678 optimal model SKILLS\n",
      "12917368283846924199 skills 676 678 model SKILLS\n",
      "12917368283846924199 skills 677 678 SKILLS\n",
      "12917368283846924199 skills 666 679 among various other models was found to be the optimal model SKILLS AND\n",
      "12917368283846924199 skills 667 679 various other models was found to be the optimal model SKILLS AND\n",
      "12917368283846924199 skills 668 679 other models was found to be the optimal model SKILLS AND\n",
      "12917368283846924199 skills 669 679 models was found to be the optimal model SKILLS AND\n",
      "12917368283846924199 skills 670 679 was found to be the optimal model SKILLS AND\n",
      "12917368283846924199 skills 671 679 found to be the optimal model SKILLS AND\n",
      "12917368283846924199 skills 672 679 to be the optimal model SKILLS AND\n",
      "12917368283846924199 skills 673 679 be the optimal model SKILLS AND\n",
      "12917368283846924199 skills 674 679 the optimal model SKILLS AND\n",
      "12917368283846924199 skills 675 679 optimal model SKILLS AND\n",
      "12917368283846924199 skills 676 679 model SKILLS AND\n",
      "12917368283846924199 skills 677 679 SKILLS AND\n",
      "12917368283846924199 skills 666 680 among various other models was found to be the optimal model SKILLS AND EXPERTISE\n",
      "12917368283846924199 skills 667 680 various other models was found to be the optimal model SKILLS AND EXPERTISE\n",
      "12917368283846924199 skills 668 680 other models was found to be the optimal model SKILLS AND EXPERTISE\n",
      "12917368283846924199 skills 669 680 models was found to be the optimal model SKILLS AND EXPERTISE\n",
      "12917368283846924199 skills 670 680 was found to be the optimal model SKILLS AND EXPERTISE\n",
      "12917368283846924199 skills 671 680 found to be the optimal model SKILLS AND EXPERTISE\n",
      "12917368283846924199 skills 672 680 to be the optimal model SKILLS AND EXPERTISE\n",
      "12917368283846924199 skills 673 680 be the optimal model SKILLS AND EXPERTISE\n",
      "12917368283846924199 skills 674 680 the optimal model SKILLS AND EXPERTISE\n",
      "12917368283846924199 skills 675 680 optimal model SKILLS AND EXPERTISE\n",
      "12917368283846924199 skills 676 680 model SKILLS AND EXPERTISE\n",
      "12917368283846924199 skills 677 680 SKILLS AND EXPERTISE\n",
      "12917368283846924199 skills 666 681 among various other models was found to be the optimal model SKILLS AND EXPERTISE Programming\n",
      "12917368283846924199 skills 667 681 various other models was found to be the optimal model SKILLS AND EXPERTISE Programming\n",
      "12917368283846924199 skills 668 681 other models was found to be the optimal model SKILLS AND EXPERTISE Programming\n",
      "12917368283846924199 skills 669 681 models was found to be the optimal model SKILLS AND EXPERTISE Programming\n",
      "12917368283846924199 skills 670 681 was found to be the optimal model SKILLS AND EXPERTISE Programming\n",
      "12917368283846924199 skills 671 681 found to be the optimal model SKILLS AND EXPERTISE Programming\n",
      "12917368283846924199 skills 672 681 to be the optimal model SKILLS AND EXPERTISE Programming\n",
      "12917368283846924199 skills 673 681 be the optimal model SKILLS AND EXPERTISE Programming\n",
      "12917368283846924199 skills 674 681 the optimal model SKILLS AND EXPERTISE Programming\n",
      "12917368283846924199 skills 675 681 optimal model SKILLS AND EXPERTISE Programming\n",
      "12917368283846924199 skills 676 681 model SKILLS AND EXPERTISE Programming\n",
      "12917368283846924199 skills 677 681 SKILLS AND EXPERTISE Programming\n",
      "12917368283846924199 skills 666 682 among various other models was found to be the optimal model SKILLS AND EXPERTISE Programming Languages\n",
      "12917368283846924199 skills 667 682 various other models was found to be the optimal model SKILLS AND EXPERTISE Programming Languages\n",
      "12917368283846924199 skills 668 682 other models was found to be the optimal model SKILLS AND EXPERTISE Programming Languages\n",
      "12917368283846924199 skills 669 682 models was found to be the optimal model SKILLS AND EXPERTISE Programming Languages\n",
      "12917368283846924199 skills 670 682 was found to be the optimal model SKILLS AND EXPERTISE Programming Languages\n",
      "12917368283846924199 skills 671 682 found to be the optimal model SKILLS AND EXPERTISE Programming Languages\n",
      "12917368283846924199 skills 672 682 to be the optimal model SKILLS AND EXPERTISE Programming Languages\n",
      "12917368283846924199 skills 673 682 be the optimal model SKILLS AND EXPERTISE Programming Languages\n",
      "12917368283846924199 skills 674 682 the optimal model SKILLS AND EXPERTISE Programming Languages\n",
      "12917368283846924199 skills 675 682 optimal model SKILLS AND EXPERTISE Programming Languages\n",
      "12917368283846924199 skills 676 682 model SKILLS AND EXPERTISE Programming Languages\n",
      "12917368283846924199 skills 677 682 SKILLS AND EXPERTISE Programming Languages\n",
      "12917368283846924199 skills 57 58 SKILLS\n",
      "12917368283846924199 skills 57 59 SKILLS AND\n",
      "12917368283846924199 skills 57 60 SKILLS AND EXPERTISE\n",
      "12917368283846924199 skills 57 61 SKILLS AND EXPERTISE HITEC\n",
      "12917368283846924199 skills 57 62 SKILLS AND EXPERTISE HITEC City\n",
      "12917368283846924199 skills 34 35 SKILLS\n",
      "12917368283846924199 skills 34 36 SKILLS Languages\n",
      "12917368283846924199 skills 34 37 SKILLS Languages ML\n",
      "12917368283846924199 skills 12 14 EDUCATION SKILLS\n",
      "12917368283846924199 skills 13 14 SKILLS\n",
      "12917368283846924199 skills 12 15 EDUCATION SKILLS MBA\n",
      "12917368283846924199 skills 13 15 SKILLS MBA\n",
      "12917368283846924199 skills 12 16 EDUCATION SKILLS MBA in\n",
      "12917368283846924199 skills 13 16 SKILLS MBA in\n",
      "12917368283846924199 skills 12 17 EDUCATION SKILLS MBA in Information\n",
      "12917368283846924199 skills 13 17 SKILLS MBA in Information\n",
      "12917368283846924199 skills 12 18 EDUCATION SKILLS MBA in Information Technology\n",
      "12917368283846924199 skills 13 18 SKILLS MBA in Information Technology\n",
      "12917368283846924199 skills 27 50 Computer Engineering student at Army Institute of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills\n",
      "12917368283846924199 skills 28 50 Engineering student at Army Institute of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills\n",
      "12917368283846924199 skills 29 50 student at Army Institute of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills\n",
      "12917368283846924199 skills 30 50 at Army Institute of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills\n",
      "12917368283846924199 skills 31 50 Army Institute of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills\n",
      "12917368283846924199 skills 32 50 Institute of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills\n",
      "12917368283846924199 skills 33 50 of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills\n",
      "12917368283846924199 skills 34 50 Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills\n",
      "12917368283846924199 skills 35 50 seeking a position in Data Science where I can utilize my Analytical and Visualization skills\n",
      "12917368283846924199 skills 36 50 a position in Data Science where I can utilize my Analytical and Visualization skills\n",
      "12917368283846924199 skills 37 50 position in Data Science where I can utilize my Analytical and Visualization skills\n",
      "12917368283846924199 skills 38 50 in Data Science where I can utilize my Analytical and Visualization skills\n",
      "12917368283846924199 skills 39 50 Data Science where I can utilize my Analytical and Visualization skills\n",
      "12917368283846924199 skills 40 50 Science where I can utilize my Analytical and Visualization skills\n",
      "12917368283846924199 skills 41 50 where I can utilize my Analytical and Visualization skills\n",
      "12917368283846924199 skills 42 50 I can utilize my Analytical and Visualization skills\n",
      "12917368283846924199 skills 43 50 can utilize my Analytical and Visualization skills\n",
      "12917368283846924199 skills 44 50 utilize my Analytical and Visualization skills\n",
      "12917368283846924199 skills 45 50 my Analytical and Visualization skills\n",
      "12917368283846924199 skills 46 50 Analytical and Visualization skills\n",
      "12917368283846924199 skills 47 50 and Visualization skills\n",
      "12917368283846924199 skills 48 50 Visualization skills\n",
      "12917368283846924199 skills 49 50 skills\n",
      "12917368283846924199 skills 27 51 Computer Engineering student at Army Institute of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for\n",
      "12917368283846924199 skills 28 51 Engineering student at Army Institute of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for\n",
      "12917368283846924199 skills 29 51 student at Army Institute of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for\n",
      "12917368283846924199 skills 30 51 at Army Institute of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for\n",
      "12917368283846924199 skills 31 51 Army Institute of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for\n",
      "12917368283846924199 skills 32 51 Institute of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for\n",
      "12917368283846924199 skills 33 51 of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for\n",
      "12917368283846924199 skills 34 51 Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for\n",
      "12917368283846924199 skills 35 51 seeking a position in Data Science where I can utilize my Analytical and Visualization skills for\n",
      "12917368283846924199 skills 36 51 a position in Data Science where I can utilize my Analytical and Visualization skills for\n",
      "12917368283846924199 skills 37 51 position in Data Science where I can utilize my Analytical and Visualization skills for\n",
      "12917368283846924199 skills 38 51 in Data Science where I can utilize my Analytical and Visualization skills for\n",
      "12917368283846924199 skills 39 51 Data Science where I can utilize my Analytical and Visualization skills for\n",
      "12917368283846924199 skills 40 51 Science where I can utilize my Analytical and Visualization skills for\n",
      "12917368283846924199 skills 41 51 where I can utilize my Analytical and Visualization skills for\n",
      "12917368283846924199 skills 42 51 I can utilize my Analytical and Visualization skills for\n",
      "12917368283846924199 skills 43 51 can utilize my Analytical and Visualization skills for\n",
      "12917368283846924199 skills 44 51 utilize my Analytical and Visualization skills for\n",
      "12917368283846924199 skills 45 51 my Analytical and Visualization skills for\n",
      "12917368283846924199 skills 46 51 Analytical and Visualization skills for\n",
      "12917368283846924199 skills 47 51 and Visualization skills for\n",
      "12917368283846924199 skills 48 51 Visualization skills for\n",
      "12917368283846924199 skills 49 51 skills for\n",
      "12917368283846924199 skills 27 52 Computer Engineering student at Army Institute of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling\n",
      "12917368283846924199 skills 28 52 Engineering student at Army Institute of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling\n",
      "12917368283846924199 skills 29 52 student at Army Institute of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling\n",
      "12917368283846924199 skills 30 52 at Army Institute of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling\n",
      "12917368283846924199 skills 31 52 Army Institute of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling\n",
      "12917368283846924199 skills 32 52 Institute of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling\n",
      "12917368283846924199 skills 33 52 of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling\n",
      "12917368283846924199 skills 34 52 Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling\n",
      "12917368283846924199 skills 35 52 seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling\n",
      "12917368283846924199 skills 36 52 a position in Data Science where I can utilize my Analytical and Visualization skills for modelling\n",
      "12917368283846924199 skills 37 52 position in Data Science where I can utilize my Analytical and Visualization skills for modelling\n",
      "12917368283846924199 skills 38 52 in Data Science where I can utilize my Analytical and Visualization skills for modelling\n",
      "12917368283846924199 skills 39 52 Data Science where I can utilize my Analytical and Visualization skills for modelling\n",
      "12917368283846924199 skills 40 52 Science where I can utilize my Analytical and Visualization skills for modelling\n",
      "12917368283846924199 skills 41 52 where I can utilize my Analytical and Visualization skills for modelling\n",
      "12917368283846924199 skills 42 52 I can utilize my Analytical and Visualization skills for modelling\n",
      "12917368283846924199 skills 43 52 can utilize my Analytical and Visualization skills for modelling\n",
      "12917368283846924199 skills 44 52 utilize my Analytical and Visualization skills for modelling\n",
      "12917368283846924199 skills 45 52 my Analytical and Visualization skills for modelling\n",
      "12917368283846924199 skills 46 52 Analytical and Visualization skills for modelling\n",
      "12917368283846924199 skills 47 52 and Visualization skills for modelling\n",
      "12917368283846924199 skills 48 52 Visualization skills for modelling\n",
      "12917368283846924199 skills 49 52 skills for modelling\n",
      "12917368283846924199 skills 27 53 Computer Engineering student at Army Institute of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the\n",
      "12917368283846924199 skills 28 53 Engineering student at Army Institute of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the\n",
      "12917368283846924199 skills 29 53 student at Army Institute of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the\n",
      "12917368283846924199 skills 30 53 at Army Institute of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the\n",
      "12917368283846924199 skills 31 53 Army Institute of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the\n",
      "12917368283846924199 skills 32 53 Institute of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the\n",
      "12917368283846924199 skills 33 53 of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the\n",
      "12917368283846924199 skills 34 53 Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the\n",
      "12917368283846924199 skills 35 53 seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the\n",
      "12917368283846924199 skills 36 53 a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the\n",
      "12917368283846924199 skills 37 53 position in Data Science where I can utilize my Analytical and Visualization skills for modelling the\n",
      "12917368283846924199 skills 38 53 in Data Science where I can utilize my Analytical and Visualization skills for modelling the\n",
      "12917368283846924199 skills 39 53 Data Science where I can utilize my Analytical and Visualization skills for modelling the\n",
      "12917368283846924199 skills 40 53 Science where I can utilize my Analytical and Visualization skills for modelling the\n",
      "12917368283846924199 skills 41 53 where I can utilize my Analytical and Visualization skills for modelling the\n",
      "12917368283846924199 skills 42 53 I can utilize my Analytical and Visualization skills for modelling the\n",
      "12917368283846924199 skills 43 53 can utilize my Analytical and Visualization skills for modelling the\n",
      "12917368283846924199 skills 44 53 utilize my Analytical and Visualization skills for modelling the\n",
      "12917368283846924199 skills 45 53 my Analytical and Visualization skills for modelling the\n",
      "12917368283846924199 skills 46 53 Analytical and Visualization skills for modelling the\n",
      "12917368283846924199 skills 47 53 and Visualization skills for modelling the\n",
      "12917368283846924199 skills 48 53 Visualization skills for modelling the\n",
      "12917368283846924199 skills 49 53 skills for modelling the\n",
      "12917368283846924199 skills 27 54 Computer Engineering student at Army Institute of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data\n",
      "12917368283846924199 skills 28 54 Engineering student at Army Institute of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data\n",
      "12917368283846924199 skills 29 54 student at Army Institute of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data\n",
      "12917368283846924199 skills 30 54 at Army Institute of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data\n",
      "12917368283846924199 skills 31 54 Army Institute of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data\n",
      "12917368283846924199 skills 32 54 Institute of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data\n",
      "12917368283846924199 skills 33 54 of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data\n",
      "12917368283846924199 skills 34 54 Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data\n",
      "12917368283846924199 skills 35 54 seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data\n",
      "12917368283846924199 skills 36 54 a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data\n",
      "12917368283846924199 skills 37 54 position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data\n",
      "12917368283846924199 skills 38 54 in Data Science where I can utilize my Analytical and Visualization skills for modelling the data\n",
      "12917368283846924199 skills 39 54 Data Science where I can utilize my Analytical and Visualization skills for modelling the data\n",
      "12917368283846924199 skills 40 54 Science where I can utilize my Analytical and Visualization skills for modelling the data\n",
      "12917368283846924199 skills 41 54 where I can utilize my Analytical and Visualization skills for modelling the data\n",
      "12917368283846924199 skills 42 54 I can utilize my Analytical and Visualization skills for modelling the data\n",
      "12917368283846924199 skills 43 54 can utilize my Analytical and Visualization skills for modelling the data\n",
      "12917368283846924199 skills 44 54 utilize my Analytical and Visualization skills for modelling the data\n",
      "12917368283846924199 skills 45 54 my Analytical and Visualization skills for modelling the data\n",
      "12917368283846924199 skills 46 54 Analytical and Visualization skills for modelling the data\n",
      "12917368283846924199 skills 47 54 and Visualization skills for modelling the data\n",
      "12917368283846924199 skills 48 54 Visualization skills for modelling the data\n",
      "12917368283846924199 skills 49 54 skills for modelling the data\n",
      "12917368283846924199 skills 27 55 Computer Engineering student at Army Institute of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with\n",
      "12917368283846924199 skills 28 55 Engineering student at Army Institute of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with\n",
      "12917368283846924199 skills 29 55 student at Army Institute of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with\n",
      "12917368283846924199 skills 30 55 at Army Institute of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with\n",
      "12917368283846924199 skills 31 55 Army Institute of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with\n",
      "12917368283846924199 skills 32 55 Institute of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with\n",
      "12917368283846924199 skills 33 55 of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with\n",
      "12917368283846924199 skills 34 55 Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with\n",
      "12917368283846924199 skills 35 55 seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12917368283846924199 skills 36 55 a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with\n",
      "12917368283846924199 skills 37 55 position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with\n",
      "12917368283846924199 skills 38 55 in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with\n",
      "12917368283846924199 skills 39 55 Data Science where I can utilize my Analytical and Visualization skills for modelling the data with\n",
      "12917368283846924199 skills 40 55 Science where I can utilize my Analytical and Visualization skills for modelling the data with\n",
      "12917368283846924199 skills 41 55 where I can utilize my Analytical and Visualization skills for modelling the data with\n",
      "12917368283846924199 skills 42 55 I can utilize my Analytical and Visualization skills for modelling the data with\n",
      "12917368283846924199 skills 43 55 can utilize my Analytical and Visualization skills for modelling the data with\n",
      "12917368283846924199 skills 44 55 utilize my Analytical and Visualization skills for modelling the data with\n",
      "12917368283846924199 skills 45 55 my Analytical and Visualization skills for modelling the data with\n",
      "12917368283846924199 skills 46 55 Analytical and Visualization skills for modelling the data with\n",
      "12917368283846924199 skills 47 55 and Visualization skills for modelling the data with\n",
      "12917368283846924199 skills 48 55 Visualization skills for modelling the data with\n",
      "12917368283846924199 skills 49 55 skills for modelling the data with\n",
      "12917368283846924199 skills 27 56 Computer Engineering student at Army Institute of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the\n",
      "12917368283846924199 skills 28 56 Engineering student at Army Institute of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the\n",
      "12917368283846924199 skills 29 56 student at Army Institute of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the\n",
      "12917368283846924199 skills 30 56 at Army Institute of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the\n",
      "12917368283846924199 skills 31 56 Army Institute of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the\n",
      "12917368283846924199 skills 32 56 Institute of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the\n",
      "12917368283846924199 skills 33 56 of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the\n",
      "12917368283846924199 skills 34 56 Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the\n",
      "12917368283846924199 skills 35 56 seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the\n",
      "12917368283846924199 skills 36 56 a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the\n",
      "12917368283846924199 skills 37 56 position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the\n",
      "12917368283846924199 skills 38 56 in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the\n",
      "12917368283846924199 skills 39 56 Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the\n",
      "12917368283846924199 skills 40 56 Science where I can utilize my Analytical and Visualization skills for modelling the data with the\n",
      "12917368283846924199 skills 41 56 where I can utilize my Analytical and Visualization skills for modelling the data with the\n",
      "12917368283846924199 skills 42 56 I can utilize my Analytical and Visualization skills for modelling the data with the\n",
      "12917368283846924199 skills 43 56 can utilize my Analytical and Visualization skills for modelling the data with the\n",
      "12917368283846924199 skills 44 56 utilize my Analytical and Visualization skills for modelling the data with the\n",
      "12917368283846924199 skills 45 56 my Analytical and Visualization skills for modelling the data with the\n",
      "12917368283846924199 skills 46 56 Analytical and Visualization skills for modelling the data with the\n",
      "12917368283846924199 skills 47 56 and Visualization skills for modelling the data with the\n",
      "12917368283846924199 skills 48 56 Visualization skills for modelling the data with the\n",
      "12917368283846924199 skills 49 56 skills for modelling the data with the\n",
      "12917368283846924199 skills 27 57 Computer Engineering student at Army Institute of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the goal\n",
      "12917368283846924199 skills 28 57 Engineering student at Army Institute of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the goal\n",
      "12917368283846924199 skills 29 57 student at Army Institute of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the goal\n",
      "12917368283846924199 skills 30 57 at Army Institute of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the goal\n",
      "12917368283846924199 skills 31 57 Army Institute of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the goal\n",
      "12917368283846924199 skills 32 57 Institute of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the goal\n",
      "12917368283846924199 skills 33 57 of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the goal\n",
      "12917368283846924199 skills 34 57 Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the goal\n",
      "12917368283846924199 skills 35 57 seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the goal\n",
      "12917368283846924199 skills 36 57 a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the goal\n",
      "12917368283846924199 skills 37 57 position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the goal\n",
      "12917368283846924199 skills 38 57 in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the goal\n",
      "12917368283846924199 skills 39 57 Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the goal\n",
      "12917368283846924199 skills 40 57 Science where I can utilize my Analytical and Visualization skills for modelling the data with the goal\n",
      "12917368283846924199 skills 41 57 where I can utilize my Analytical and Visualization skills for modelling the data with the goal\n",
      "12917368283846924199 skills 42 57 I can utilize my Analytical and Visualization skills for modelling the data with the goal\n",
      "12917368283846924199 skills 43 57 can utilize my Analytical and Visualization skills for modelling the data with the goal\n",
      "12917368283846924199 skills 44 57 utilize my Analytical and Visualization skills for modelling the data with the goal\n",
      "12917368283846924199 skills 45 57 my Analytical and Visualization skills for modelling the data with the goal\n",
      "12917368283846924199 skills 46 57 Analytical and Visualization skills for modelling the data with the goal\n",
      "12917368283846924199 skills 47 57 and Visualization skills for modelling the data with the goal\n",
      "12917368283846924199 skills 48 57 Visualization skills for modelling the data with the goal\n",
      "12917368283846924199 skills 49 57 skills for modelling the data with the goal\n",
      "12917368283846924199 skills 27 58 Computer Engineering student at Army Institute of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the goal of\n",
      "12917368283846924199 skills 28 58 Engineering student at Army Institute of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the goal of\n",
      "12917368283846924199 skills 29 58 student at Army Institute of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the goal of\n",
      "12917368283846924199 skills 30 58 at Army Institute of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the goal of\n",
      "12917368283846924199 skills 31 58 Army Institute of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the goal of\n",
      "12917368283846924199 skills 32 58 Institute of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the goal of\n",
      "12917368283846924199 skills 33 58 of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the goal of\n",
      "12917368283846924199 skills 34 58 Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the goal of\n",
      "12917368283846924199 skills 35 58 seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the goal of\n",
      "12917368283846924199 skills 36 58 a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the goal of\n",
      "12917368283846924199 skills 37 58 position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the goal of\n",
      "12917368283846924199 skills 38 58 in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the goal of\n",
      "12917368283846924199 skills 39 58 Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the goal of\n",
      "12917368283846924199 skills 40 58 Science where I can utilize my Analytical and Visualization skills for modelling the data with the goal of\n",
      "12917368283846924199 skills 41 58 where I can utilize my Analytical and Visualization skills for modelling the data with the goal of\n",
      "12917368283846924199 skills 42 58 I can utilize my Analytical and Visualization skills for modelling the data with the goal of\n",
      "12917368283846924199 skills 43 58 can utilize my Analytical and Visualization skills for modelling the data with the goal of\n",
      "12917368283846924199 skills 44 58 utilize my Analytical and Visualization skills for modelling the data with the goal of\n",
      "12917368283846924199 skills 45 58 my Analytical and Visualization skills for modelling the data with the goal of\n",
      "12917368283846924199 skills 46 58 Analytical and Visualization skills for modelling the data with the goal of\n",
      "12917368283846924199 skills 47 58 and Visualization skills for modelling the data with the goal of\n",
      "12917368283846924199 skills 48 58 Visualization skills for modelling the data with the goal of\n",
      "12917368283846924199 skills 49 58 skills for modelling the data with the goal of\n",
      "12917368283846924199 skills 27 59 Computer Engineering student at Army Institute of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the goal of discovering\n",
      "12917368283846924199 skills 28 59 Engineering student at Army Institute of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the goal of discovering\n",
      "12917368283846924199 skills 29 59 student at Army Institute of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the goal of discovering\n",
      "12917368283846924199 skills 30 59 at Army Institute of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the goal of discovering\n",
      "12917368283846924199 skills 31 59 Army Institute of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the goal of discovering\n",
      "12917368283846924199 skills 32 59 Institute of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the goal of discovering\n",
      "12917368283846924199 skills 33 59 of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the goal of discovering\n",
      "12917368283846924199 skills 34 59 Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the goal of discovering\n",
      "12917368283846924199 skills 35 59 seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the goal of discovering\n",
      "12917368283846924199 skills 36 59 a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the goal of discovering\n",
      "12917368283846924199 skills 37 59 position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the goal of discovering\n",
      "12917368283846924199 skills 38 59 in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the goal of discovering\n",
      "12917368283846924199 skills 39 59 Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the goal of discovering\n",
      "12917368283846924199 skills 40 59 Science where I can utilize my Analytical and Visualization skills for modelling the data with the goal of discovering\n",
      "12917368283846924199 skills 41 59 where I can utilize my Analytical and Visualization skills for modelling the data with the goal of discovering\n",
      "12917368283846924199 skills 42 59 I can utilize my Analytical and Visualization skills for modelling the data with the goal of discovering\n",
      "12917368283846924199 skills 43 59 can utilize my Analytical and Visualization skills for modelling the data with the goal of discovering\n",
      "12917368283846924199 skills 44 59 utilize my Analytical and Visualization skills for modelling the data with the goal of discovering\n",
      "12917368283846924199 skills 45 59 my Analytical and Visualization skills for modelling the data with the goal of discovering\n",
      "12917368283846924199 skills 46 59 Analytical and Visualization skills for modelling the data with the goal of discovering\n",
      "12917368283846924199 skills 47 59 and Visualization skills for modelling the data with the goal of discovering\n",
      "12917368283846924199 skills 48 59 Visualization skills for modelling the data with the goal of discovering\n",
      "12917368283846924199 skills 49 59 skills for modelling the data with the goal of discovering\n",
      "12917368283846924199 skills 27 60 Computer Engineering student at Army Institute of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the goal of discovering significant\n",
      "12917368283846924199 skills 28 60 Engineering student at Army Institute of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the goal of discovering significant\n",
      "12917368283846924199 skills 29 60 student at Army Institute of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the goal of discovering significant\n",
      "12917368283846924199 skills 30 60 at Army Institute of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the goal of discovering significant\n",
      "12917368283846924199 skills 31 60 Army Institute of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the goal of discovering significant\n",
      "12917368283846924199 skills 32 60 Institute of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the goal of discovering significant\n",
      "12917368283846924199 skills 33 60 of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the goal of discovering significant\n",
      "12917368283846924199 skills 34 60 Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the goal of discovering significant\n",
      "12917368283846924199 skills 35 60 seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the goal of discovering significant\n",
      "12917368283846924199 skills 36 60 a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the goal of discovering significant\n",
      "12917368283846924199 skills 37 60 position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the goal of discovering significant\n",
      "12917368283846924199 skills 38 60 in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the goal of discovering significant\n",
      "12917368283846924199 skills 39 60 Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the goal of discovering significant\n",
      "12917368283846924199 skills 40 60 Science where I can utilize my Analytical and Visualization skills for modelling the data with the goal of discovering significant\n",
      "12917368283846924199 skills 41 60 where I can utilize my Analytical and Visualization skills for modelling the data with the goal of discovering significant\n",
      "12917368283846924199 skills 42 60 I can utilize my Analytical and Visualization skills for modelling the data with the goal of discovering significant\n",
      "12917368283846924199 skills 43 60 can utilize my Analytical and Visualization skills for modelling the data with the goal of discovering significant\n",
      "12917368283846924199 skills 44 60 utilize my Analytical and Visualization skills for modelling the data with the goal of discovering significant\n",
      "12917368283846924199 skills 45 60 my Analytical and Visualization skills for modelling the data with the goal of discovering significant\n",
      "12917368283846924199 skills 46 60 Analytical and Visualization skills for modelling the data with the goal of discovering significant\n",
      "12917368283846924199 skills 47 60 and Visualization skills for modelling the data with the goal of discovering significant\n",
      "12917368283846924199 skills 48 60 Visualization skills for modelling the data with the goal of discovering significant\n",
      "12917368283846924199 skills 49 60 skills for modelling the data with the goal of discovering significant\n",
      "12917368283846924199 skills 27 61 Computer Engineering student at Army Institute of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the goal of discovering significant information\n",
      "12917368283846924199 skills 28 61 Engineering student at Army Institute of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the goal of discovering significant information\n",
      "12917368283846924199 skills 29 61 student at Army Institute of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the goal of discovering significant information\n",
      "12917368283846924199 skills 30 61 at Army Institute of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the goal of discovering significant information\n",
      "12917368283846924199 skills 31 61 Army Institute of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the goal of discovering significant information\n",
      "12917368283846924199 skills 32 61 Institute of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the goal of discovering significant information\n",
      "12917368283846924199 skills 33 61 of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the goal of discovering significant information\n",
      "12917368283846924199 skills 34 61 Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the goal of discovering significant information\n",
      "12917368283846924199 skills 35 61 seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the goal of discovering significant information\n",
      "12917368283846924199 skills 36 61 a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the goal of discovering significant information\n",
      "12917368283846924199 skills 37 61 position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the goal of discovering significant information\n",
      "12917368283846924199 skills 38 61 in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the goal of discovering significant information\n",
      "12917368283846924199 skills 39 61 Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the goal of discovering significant information\n",
      "12917368283846924199 skills 40 61 Science where I can utilize my Analytical and Visualization skills for modelling the data with the goal of discovering significant information\n",
      "12917368283846924199 skills 41 61 where I can utilize my Analytical and Visualization skills for modelling the data with the goal of discovering significant information\n",
      "12917368283846924199 skills 42 61 I can utilize my Analytical and Visualization skills for modelling the data with the goal of discovering significant information\n",
      "12917368283846924199 skills 43 61 can utilize my Analytical and Visualization skills for modelling the data with the goal of discovering significant information\n",
      "12917368283846924199 skills 44 61 utilize my Analytical and Visualization skills for modelling the data with the goal of discovering significant information\n",
      "12917368283846924199 skills 45 61 my Analytical and Visualization skills for modelling the data with the goal of discovering significant information\n",
      "12917368283846924199 skills 46 61 Analytical and Visualization skills for modelling the data with the goal of discovering significant information\n",
      "12917368283846924199 skills 47 61 and Visualization skills for modelling the data with the goal of discovering significant information\n",
      "12917368283846924199 skills 48 61 Visualization skills for modelling the data with the goal of discovering significant information\n",
      "12917368283846924199 skills 49 61 skills for modelling the data with the goal of discovering significant information\n",
      "12917368283846924199 skills 309 311 TECHNICAL SKILLS\n",
      "12917368283846924199 skills 310 311 SKILLS\n",
      "12917368283846924199 skills 149 154 Great management and People skills\n",
      "12917368283846924199 skills 150 154 management and People skills\n",
      "12917368283846924199 skills 151 154 and People skills\n",
      "12917368283846924199 skills 152 154 People skills\n",
      "12917368283846924199 skills 153 154 skills\n",
      "12917368283846924199 skills 170 172 KEY SKILLS\n",
      "12917368283846924199 skills 171 172 SKILLS\n",
      "12917368283846924199 skills 2775 2788 Implemented Event Handlers and Error Handling in SSIS packages Advanced extensible reporting skills\n",
      "12917368283846924199 skills 2776 2788 Event Handlers and Error Handling in SSIS packages Advanced extensible reporting skills\n",
      "12917368283846924199 skills 2777 2788 Handlers and Error Handling in SSIS packages Advanced extensible reporting skills\n",
      "12917368283846924199 skills 2778 2788 and Error Handling in SSIS packages Advanced extensible reporting skills\n",
      "12917368283846924199 skills 2779 2788 Error Handling in SSIS packages Advanced extensible reporting skills\n",
      "12917368283846924199 skills 2780 2788 Handling in SSIS packages Advanced extensible reporting skills\n",
      "12917368283846924199 skills 2781 2788 in SSIS packages Advanced extensible reporting skills\n",
      "12917368283846924199 skills 2782 2788 SSIS packages Advanced extensible reporting skills\n",
      "12917368283846924199 skills 2783 2788 packages Advanced extensible reporting skills\n",
      "12917368283846924199 skills 2784 2788 Advanced extensible reporting skills\n",
      "12917368283846924199 skills 2785 2788 extensible reporting skills\n",
      "12917368283846924199 skills 2786 2788 reporting skills\n",
      "12917368283846924199 skills 2787 2788 skills\n",
      "12917368283846924199 skills 2775 2789 Implemented Event Handlers and Error Handling in SSIS packages Advanced extensible reporting skills using\n",
      "12917368283846924199 skills 2776 2789 Event Handlers and Error Handling in SSIS packages Advanced extensible reporting skills using\n",
      "12917368283846924199 skills 2777 2789 Handlers and Error Handling in SSIS packages Advanced extensible reporting skills using\n",
      "12917368283846924199 skills 2778 2789 and Error Handling in SSIS packages Advanced extensible reporting skills using\n",
      "12917368283846924199 skills 2779 2789 Error Handling in SSIS packages Advanced extensible reporting skills using\n",
      "12917368283846924199 skills 2780 2789 Handling in SSIS packages Advanced extensible reporting skills using\n",
      "12917368283846924199 skills 2781 2789 in SSIS packages Advanced extensible reporting skills using\n",
      "12917368283846924199 skills 2782 2789 SSIS packages Advanced extensible reporting skills using\n",
      "12917368283846924199 skills 2783 2789 packages Advanced extensible reporting skills using\n",
      "12917368283846924199 skills 2784 2789 Advanced extensible reporting skills using\n",
      "12917368283846924199 skills 2785 2789 extensible reporting skills using\n",
      "12917368283846924199 skills 2786 2789 reporting skills using\n",
      "12917368283846924199 skills 2787 2789 skills using\n",
      "12917368283846924199 skills 2775 2790 Implemented Event Handlers and Error Handling in SSIS packages Advanced extensible reporting skills using SQL\n",
      "12917368283846924199 skills 2776 2790 Event Handlers and Error Handling in SSIS packages Advanced extensible reporting skills using SQL\n",
      "12917368283846924199 skills 2777 2790 Handlers and Error Handling in SSIS packages Advanced extensible reporting skills using SQL\n",
      "12917368283846924199 skills 2778 2790 and Error Handling in SSIS packages Advanced extensible reporting skills using SQL\n",
      "12917368283846924199 skills 2779 2790 Error Handling in SSIS packages Advanced extensible reporting skills using SQL\n",
      "12917368283846924199 skills 2780 2790 Handling in SSIS packages Advanced extensible reporting skills using SQL\n",
      "12917368283846924199 skills 2781 2790 in SSIS packages Advanced extensible reporting skills using SQL\n",
      "12917368283846924199 skills 2782 2790 SSIS packages Advanced extensible reporting skills using SQL\n",
      "12917368283846924199 skills 2783 2790 packages Advanced extensible reporting skills using SQL\n",
      "12917368283846924199 skills 2784 2790 Advanced extensible reporting skills using SQL\n",
      "12917368283846924199 skills 2785 2790 extensible reporting skills using SQL\n",
      "12917368283846924199 skills 2786 2790 reporting skills using SQL\n",
      "12917368283846924199 skills 2787 2790 skills using SQL\n",
      "12917368283846924199 skills 2775 2791 Implemented Event Handlers and Error Handling in SSIS packages Advanced extensible reporting skills using SQL Server\n",
      "12917368283846924199 skills 2776 2791 Event Handlers and Error Handling in SSIS packages Advanced extensible reporting skills using SQL Server\n",
      "12917368283846924199 skills 2777 2791 Handlers and Error Handling in SSIS packages Advanced extensible reporting skills using SQL Server\n",
      "12917368283846924199 skills 2778 2791 and Error Handling in SSIS packages Advanced extensible reporting skills using SQL Server\n",
      "12917368283846924199 skills 2779 2791 Error Handling in SSIS packages Advanced extensible reporting skills using SQL Server\n",
      "12917368283846924199 skills 2780 2791 Handling in SSIS packages Advanced extensible reporting skills using SQL Server\n",
      "12917368283846924199 skills 2781 2791 in SSIS packages Advanced extensible reporting skills using SQL Server\n",
      "12917368283846924199 skills 2782 2791 SSIS packages Advanced extensible reporting skills using SQL Server\n",
      "12917368283846924199 skills 2783 2791 packages Advanced extensible reporting skills using SQL Server\n",
      "12917368283846924199 skills 2784 2791 Advanced extensible reporting skills using SQL Server\n",
      "12917368283846924199 skills 2785 2791 extensible reporting skills using SQL Server\n",
      "12917368283846924199 skills 2786 2791 reporting skills using SQL Server\n",
      "12917368283846924199 skills 2787 2791 skills using SQL Server\n",
      "12917368283846924199 skills 2775 2792 Implemented Event Handlers and Error Handling in SSIS packages Advanced extensible reporting skills using SQL Server Reporting\n",
      "12917368283846924199 skills 2776 2792 Event Handlers and Error Handling in SSIS packages Advanced extensible reporting skills using SQL Server Reporting\n",
      "12917368283846924199 skills 2777 2792 Handlers and Error Handling in SSIS packages Advanced extensible reporting skills using SQL Server Reporting\n",
      "12917368283846924199 skills 2778 2792 and Error Handling in SSIS packages Advanced extensible reporting skills using SQL Server Reporting\n",
      "12917368283846924199 skills 2779 2792 Error Handling in SSIS packages Advanced extensible reporting skills using SQL Server Reporting\n",
      "12917368283846924199 skills 2780 2792 Handling in SSIS packages Advanced extensible reporting skills using SQL Server Reporting\n",
      "12917368283846924199 skills 2781 2792 in SSIS packages Advanced extensible reporting skills using SQL Server Reporting\n",
      "12917368283846924199 skills 2782 2792 SSIS packages Advanced extensible reporting skills using SQL Server Reporting\n",
      "12917368283846924199 skills 2783 2792 packages Advanced extensible reporting skills using SQL Server Reporting\n",
      "12917368283846924199 skills 2784 2792 Advanced extensible reporting skills using SQL Server Reporting\n",
      "12917368283846924199 skills 2785 2792 extensible reporting skills using SQL Server Reporting\n",
      "12917368283846924199 skills 2786 2792 reporting skills using SQL Server Reporting\n",
      "12917368283846924199 skills 2787 2792 skills using SQL Server Reporting\n",
      "12917368283846924199 skills 2775 2793 Implemented Event Handlers and Error Handling in SSIS packages Advanced extensible reporting skills using SQL Server Reporting Services\n",
      "12917368283846924199 skills 2776 2793 Event Handlers and Error Handling in SSIS packages Advanced extensible reporting skills using SQL Server Reporting Services\n",
      "12917368283846924199 skills 2777 2793 Handlers and Error Handling in SSIS packages Advanced extensible reporting skills using SQL Server Reporting Services\n",
      "12917368283846924199 skills 2778 2793 and Error Handling in SSIS packages Advanced extensible reporting skills using SQL Server Reporting Services\n",
      "12917368283846924199 skills 2779 2793 Error Handling in SSIS packages Advanced extensible reporting skills using SQL Server Reporting Services\n",
      "12917368283846924199 skills 2780 2793 Handling in SSIS packages Advanced extensible reporting skills using SQL Server Reporting Services\n",
      "12917368283846924199 skills 2781 2793 in SSIS packages Advanced extensible reporting skills using SQL Server Reporting Services\n",
      "12917368283846924199 skills 2782 2793 SSIS packages Advanced extensible reporting skills using SQL Server Reporting Services\n",
      "12917368283846924199 skills 2783 2793 packages Advanced extensible reporting skills using SQL Server Reporting Services\n",
      "12917368283846924199 skills 2784 2793 Advanced extensible reporting skills using SQL Server Reporting Services\n",
      "12917368283846924199 skills 2785 2793 extensible reporting skills using SQL Server Reporting Services\n",
      "12917368283846924199 skills 2786 2793 reporting skills using SQL Server Reporting Services\n",
      "12917368283846924199 skills 2787 2793 skills using SQL Server Reporting Services\n",
      "12917368283846924199 skills 4028 4033 Windows XP Professional TECHNICAL SKILLS\n",
      "12917368283846924199 skills 4029 4033 XP Professional TECHNICAL SKILLS\n",
      "12917368283846924199 skills 4030 4033 Professional TECHNICAL SKILLS\n",
      "12917368283846924199 skills 4031 4033 TECHNICAL SKILLS\n",
      "12917368283846924199 skills 4032 4033 SKILLS\n",
      "12917368283846924199 skills 4028 4034 Windows XP Professional TECHNICAL SKILLS Languages\n",
      "12917368283846924199 skills 4029 4034 XP Professional TECHNICAL SKILLS Languages\n",
      "12917368283846924199 skills 4030 4034 Professional TECHNICAL SKILLS Languages\n",
      "12917368283846924199 skills 4031 4034 TECHNICAL SKILLS Languages\n",
      "12917368283846924199 skills 4032 4034 SKILLS Languages\n",
      "12917368283846924199 skills 281 287 Coursera Data Science and Software Skills\n",
      "12917368283846924199 skills 282 287 Data Science and Software Skills\n",
      "12917368283846924199 skills 283 287 Science and Software Skills\n",
      "12917368283846924199 skills 284 287 and Software Skills\n",
      "12917368283846924199 skills 285 287 Software Skills\n",
      "12917368283846924199 skills 286 287 Skills\n",
      "12917368283846924199 skills 503 515 Gaming algorithm TrueSkill Predictor to match players that are close on skill\n",
      "12917368283846924199 skills 504 515 algorithm TrueSkill Predictor to match players that are close on skill\n",
      "12917368283846924199 skills 505 515 TrueSkill Predictor to match players that are close on skill\n",
      "12917368283846924199 skills 506 515 Predictor to match players that are close on skill\n",
      "12917368283846924199 skills 507 515 to match players that are close on skill\n",
      "12917368283846924199 skills 508 515 match players that are close on skill\n",
      "12917368283846924199 skills 509 515 players that are close on skill\n",
      "12917368283846924199 skills 510 515 that are close on skill\n",
      "12917368283846924199 skills 511 515 are close on skill\n",
      "12917368283846924199 skills 512 515 close on skill\n",
      "12917368283846924199 skills 513 515 on skill\n",
      "12917368283846924199 skills 514 515 skill\n",
      "12917368283846924199 skills 503 516 Gaming algorithm TrueSkill Predictor to match players that are close on skill based\n",
      "12917368283846924199 skills 504 516 algorithm TrueSkill Predictor to match players that are close on skill based\n",
      "12917368283846924199 skills 505 516 TrueSkill Predictor to match players that are close on skill based\n",
      "12917368283846924199 skills 506 516 Predictor to match players that are close on skill based\n",
      "12917368283846924199 skills 507 516 to match players that are close on skill based\n",
      "12917368283846924199 skills 508 516 match players that are close on skill based\n",
      "12917368283846924199 skills 509 516 players that are close on skill based\n",
      "12917368283846924199 skills 510 516 that are close on skill based\n",
      "12917368283846924199 skills 511 516 are close on skill based\n",
      "12917368283846924199 skills 512 516 close on skill based\n",
      "12917368283846924199 skills 513 516 on skill based\n",
      "12917368283846924199 skills 514 516 skill based\n",
      "12917368283846924199 skills 503 517 Gaming algorithm TrueSkill Predictor to match players that are close on skill based on\n",
      "12917368283846924199 skills 504 517 algorithm TrueSkill Predictor to match players that are close on skill based on\n",
      "12917368283846924199 skills 505 517 TrueSkill Predictor to match players that are close on skill based on\n",
      "12917368283846924199 skills 506 517 Predictor to match players that are close on skill based on\n",
      "12917368283846924199 skills 507 517 to match players that are close on skill based on\n",
      "12917368283846924199 skills 508 517 match players that are close on skill based on\n",
      "12917368283846924199 skills 509 517 players that are close on skill based on\n",
      "12917368283846924199 skills 510 517 that are close on skill based on\n",
      "12917368283846924199 skills 511 517 are close on skill based on\n",
      "12917368283846924199 skills 512 517 close on skill based on\n",
      "12917368283846924199 skills 513 517 on skill based on\n",
      "12917368283846924199 skills 514 517 skill based on\n",
      "12917368283846924199 skills 503 518 Gaming algorithm TrueSkill Predictor to match players that are close on skill based on their\n",
      "12917368283846924199 skills 504 518 algorithm TrueSkill Predictor to match players that are close on skill based on their\n",
      "12917368283846924199 skills 505 518 TrueSkill Predictor to match players that are close on skill based on their\n",
      "12917368283846924199 skills 506 518 Predictor to match players that are close on skill based on their\n",
      "12917368283846924199 skills 507 518 to match players that are close on skill based on their\n",
      "12917368283846924199 skills 508 518 match players that are close on skill based on their\n",
      "12917368283846924199 skills 509 518 players that are close on skill based on their\n",
      "12917368283846924199 skills 510 518 that are close on skill based on their\n",
      "12917368283846924199 skills 511 518 are close on skill based on their\n",
      "12917368283846924199 skills 512 518 close on skill based on their\n",
      "12917368283846924199 skills 513 518 on skill based on their\n",
      "12917368283846924199 skills 514 518 skill based on their\n",
      "12917368283846924199 skills 503 519 Gaming algorithm TrueSkill Predictor to match players that are close on skill based on their playing\n",
      "12917368283846924199 skills 504 519 algorithm TrueSkill Predictor to match players that are close on skill based on their playing\n",
      "12917368283846924199 skills 505 519 TrueSkill Predictor to match players that are close on skill based on their playing\n",
      "12917368283846924199 skills 506 519 Predictor to match players that are close on skill based on their playing\n",
      "12917368283846924199 skills 507 519 to match players that are close on skill based on their playing\n",
      "12917368283846924199 skills 508 519 match players that are close on skill based on their playing\n",
      "12917368283846924199 skills 509 519 players that are close on skill based on their playing\n",
      "12917368283846924199 skills 510 519 that are close on skill based on their playing\n",
      "12917368283846924199 skills 511 519 are close on skill based on their playing\n",
      "12917368283846924199 skills 512 519 close on skill based on their playing\n",
      "12917368283846924199 skills 513 519 on skill based on their playing\n",
      "12917368283846924199 skills 514 519 skill based on their playing\n",
      "12917368283846924199 skills 503 520 Gaming algorithm TrueSkill Predictor to match players that are close on skill based on their playing history\n",
      "12917368283846924199 skills 504 520 algorithm TrueSkill Predictor to match players that are close on skill based on their playing history\n",
      "12917368283846924199 skills 505 520 TrueSkill Predictor to match players that are close on skill based on their playing history\n",
      "12917368283846924199 skills 506 520 Predictor to match players that are close on skill based on their playing history\n",
      "12917368283846924199 skills 507 520 to match players that are close on skill based on their playing history\n",
      "12917368283846924199 skills 508 520 match players that are close on skill based on their playing history\n",
      "12917368283846924199 skills 509 520 players that are close on skill based on their playing history\n",
      "12917368283846924199 skills 510 520 that are close on skill based on their playing history\n",
      "12917368283846924199 skills 511 520 are close on skill based on their playing history\n",
      "12917368283846924199 skills 512 520 close on skill based on their playing history\n",
      "12917368283846924199 skills 513 520 on skill based on their playing history\n",
      "12917368283846924199 skills 514 520 skill based on their playing history\n",
      "12917368283846924199 skills 503 521 Gaming algorithm TrueSkill Predictor to match players that are close on skill based on their playing history andrank\n",
      "12917368283846924199 skills 504 521 algorithm TrueSkill Predictor to match players that are close on skill based on their playing history andrank\n",
      "12917368283846924199 skills 505 521 TrueSkill Predictor to match players that are close on skill based on their playing history andrank\n",
      "12917368283846924199 skills 506 521 Predictor to match players that are close on skill based on their playing history andrank\n",
      "12917368283846924199 skills 507 521 to match players that are close on skill based on their playing history andrank\n",
      "12917368283846924199 skills 508 521 match players that are close on skill based on their playing history andrank\n",
      "12917368283846924199 skills 509 521 players that are close on skill based on their playing history andrank\n",
      "12917368283846924199 skills 510 521 that are close on skill based on their playing history andrank\n",
      "12917368283846924199 skills 511 521 are close on skill based on their playing history andrank\n",
      "12917368283846924199 skills 512 521 close on skill based on their playing history andrank\n",
      "12917368283846924199 skills 513 521 on skill based on their playing history andrank\n",
      "12917368283846924199 skills 514 521 skill based on their playing history andrank\n",
      "12917368283846924199 skills 503 522 Gaming algorithm TrueSkill Predictor to match players that are close on skill based on their playing history andrank players\n",
      "12917368283846924199 skills 504 522 algorithm TrueSkill Predictor to match players that are close on skill based on their playing history andrank players\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12917368283846924199 skills 505 522 TrueSkill Predictor to match players that are close on skill based on their playing history andrank players\n",
      "12917368283846924199 skills 506 522 Predictor to match players that are close on skill based on their playing history andrank players\n",
      "12917368283846924199 skills 507 522 to match players that are close on skill based on their playing history andrank players\n",
      "12917368283846924199 skills 508 522 match players that are close on skill based on their playing history andrank players\n",
      "12917368283846924199 skills 509 522 players that are close on skill based on their playing history andrank players\n",
      "12917368283846924199 skills 510 522 that are close on skill based on their playing history andrank players\n",
      "12917368283846924199 skills 511 522 are close on skill based on their playing history andrank players\n",
      "12917368283846924199 skills 512 522 close on skill based on their playing history andrank players\n",
      "12917368283846924199 skills 513 522 on skill based on their playing history andrank players\n",
      "12917368283846924199 skills 514 522 skill based on their playing history andrank players\n",
      "12917368283846924199 skills 503 523 Gaming algorithm TrueSkill Predictor to match players that are close on skill based on their playing history andrank players based\n",
      "12917368283846924199 skills 504 523 algorithm TrueSkill Predictor to match players that are close on skill based on their playing history andrank players based\n",
      "12917368283846924199 skills 505 523 TrueSkill Predictor to match players that are close on skill based on their playing history andrank players based\n",
      "12917368283846924199 skills 506 523 Predictor to match players that are close on skill based on their playing history andrank players based\n",
      "12917368283846924199 skills 507 523 to match players that are close on skill based on their playing history andrank players based\n",
      "12917368283846924199 skills 508 523 match players that are close on skill based on their playing history andrank players based\n",
      "12917368283846924199 skills 509 523 players that are close on skill based on their playing history andrank players based\n",
      "12917368283846924199 skills 510 523 that are close on skill based on their playing history andrank players based\n",
      "12917368283846924199 skills 511 523 are close on skill based on their playing history andrank players based\n",
      "12917368283846924199 skills 512 523 close on skill based on their playing history andrank players based\n",
      "12917368283846924199 skills 513 523 on skill based on their playing history andrank players based\n",
      "12917368283846924199 skills 514 523 skill based on their playing history andrank players based\n",
      "12917368283846924199 skills 503 524 Gaming algorithm TrueSkill Predictor to match players that are close on skill based on their playing history andrank players based on\n",
      "12917368283846924199 skills 504 524 algorithm TrueSkill Predictor to match players that are close on skill based on their playing history andrank players based on\n",
      "12917368283846924199 skills 505 524 TrueSkill Predictor to match players that are close on skill based on their playing history andrank players based on\n",
      "12917368283846924199 skills 506 524 Predictor to match players that are close on skill based on their playing history andrank players based on\n",
      "12917368283846924199 skills 507 524 to match players that are close on skill based on their playing history andrank players based on\n",
      "12917368283846924199 skills 508 524 match players that are close on skill based on their playing history andrank players based on\n",
      "12917368283846924199 skills 509 524 players that are close on skill based on their playing history andrank players based on\n",
      "12917368283846924199 skills 510 524 that are close on skill based on their playing history andrank players based on\n",
      "12917368283846924199 skills 511 524 are close on skill based on their playing history andrank players based on\n",
      "12917368283846924199 skills 512 524 close on skill based on their playing history andrank players based on\n",
      "12917368283846924199 skills 513 524 on skill based on their playing history andrank players based on\n",
      "12917368283846924199 skills 514 524 skill based on their playing history andrank players based on\n",
      "12917368283846924199 skills 503 525 Gaming algorithm TrueSkill Predictor to match players that are close on skill based on their playing history andrank players based on the\n",
      "12917368283846924199 skills 504 525 algorithm TrueSkill Predictor to match players that are close on skill based on their playing history andrank players based on the\n",
      "12917368283846924199 skills 505 525 TrueSkill Predictor to match players that are close on skill based on their playing history andrank players based on the\n",
      "12917368283846924199 skills 506 525 Predictor to match players that are close on skill based on their playing history andrank players based on the\n",
      "12917368283846924199 skills 507 525 to match players that are close on skill based on their playing history andrank players based on the\n",
      "12917368283846924199 skills 508 525 match players that are close on skill based on their playing history andrank players based on the\n",
      "12917368283846924199 skills 509 525 players that are close on skill based on their playing history andrank players based on the\n",
      "12917368283846924199 skills 510 525 that are close on skill based on their playing history andrank players based on the\n",
      "12917368283846924199 skills 511 525 are close on skill based on their playing history andrank players based on the\n",
      "12917368283846924199 skills 512 525 close on skill based on their playing history andrank players based on the\n",
      "12917368283846924199 skills 513 525 on skill based on their playing history andrank players based on the\n",
      "12917368283846924199 skills 514 525 skill based on their playing history andrank players based on the\n",
      "12917368283846924199 skills 503 526 Gaming algorithm TrueSkill Predictor to match players that are close on skill based on their playing history andrank players based on the players\n",
      "12917368283846924199 skills 504 526 algorithm TrueSkill Predictor to match players that are close on skill based on their playing history andrank players based on the players\n",
      "12917368283846924199 skills 505 526 TrueSkill Predictor to match players that are close on skill based on their playing history andrank players based on the players\n",
      "12917368283846924199 skills 506 526 Predictor to match players that are close on skill based on their playing history andrank players based on the players\n",
      "12917368283846924199 skills 507 526 to match players that are close on skill based on their playing history andrank players based on the players\n",
      "12917368283846924199 skills 508 526 match players that are close on skill based on their playing history andrank players based on the players\n",
      "12917368283846924199 skills 509 526 players that are close on skill based on their playing history andrank players based on the players\n",
      "12917368283846924199 skills 510 526 that are close on skill based on their playing history andrank players based on the players\n",
      "12917368283846924199 skills 511 526 are close on skill based on their playing history andrank players based on the players\n",
      "12917368283846924199 skills 512 526 close on skill based on their playing history andrank players based on the players\n",
      "12917368283846924199 skills 513 526 on skill based on their playing history andrank players based on the players\n",
      "12917368283846924199 skills 514 526 skill based on their playing history andrank players based on the players\n",
      "12917368283846924199 skills 503 527 Gaming algorithm TrueSkill Predictor to match players that are close on skill based on their playing history andrank players based on the players they\n",
      "12917368283846924199 skills 504 527 algorithm TrueSkill Predictor to match players that are close on skill based on their playing history andrank players based on the players they\n",
      "12917368283846924199 skills 505 527 TrueSkill Predictor to match players that are close on skill based on their playing history andrank players based on the players they\n",
      "12917368283846924199 skills 506 527 Predictor to match players that are close on skill based on their playing history andrank players based on the players they\n",
      "12917368283846924199 skills 507 527 to match players that are close on skill based on their playing history andrank players based on the players they\n",
      "12917368283846924199 skills 508 527 match players that are close on skill based on their playing history andrank players based on the players they\n",
      "12917368283846924199 skills 509 527 players that are close on skill based on their playing history andrank players based on the players they\n",
      "12917368283846924199 skills 510 527 that are close on skill based on their playing history andrank players based on the players they\n",
      "12917368283846924199 skills 511 527 are close on skill based on their playing history andrank players based on the players they\n",
      "12917368283846924199 skills 512 527 close on skill based on their playing history andrank players based on the players they\n",
      "12917368283846924199 skills 513 527 on skill based on their playing history andrank players based on the players they\n",
      "12917368283846924199 skills 514 527 skill based on their playing history andrank players based on the players they\n",
      "12917368283846924199 skills 503 528 Gaming algorithm TrueSkill Predictor to match players that are close on skill based on their playing history andrank players based on the players they have\n",
      "12917368283846924199 skills 504 528 algorithm TrueSkill Predictor to match players that are close on skill based on their playing history andrank players based on the players they have\n",
      "12917368283846924199 skills 505 528 TrueSkill Predictor to match players that are close on skill based on their playing history andrank players based on the players they have\n",
      "12917368283846924199 skills 506 528 Predictor to match players that are close on skill based on their playing history andrank players based on the players they have\n",
      "12917368283846924199 skills 507 528 to match players that are close on skill based on their playing history andrank players based on the players they have\n",
      "12917368283846924199 skills 508 528 match players that are close on skill based on their playing history andrank players based on the players they have\n",
      "12917368283846924199 skills 509 528 players that are close on skill based on their playing history andrank players based on the players they have\n",
      "12917368283846924199 skills 510 528 that are close on skill based on their playing history andrank players based on the players they have\n",
      "12917368283846924199 skills 511 528 are close on skill based on their playing history andrank players based on the players they have\n",
      "12917368283846924199 skills 512 528 close on skill based on their playing history andrank players based on the players they have\n",
      "12917368283846924199 skills 513 528 on skill based on their playing history andrank players based on the players they have\n",
      "12917368283846924199 skills 514 528 skill based on their playing history andrank players based on the players they have\n",
      "12917368283846924199 skills 503 529 Gaming algorithm TrueSkill Predictor to match players that are close on skill based on their playing history andrank players based on the players they have played\n",
      "12917368283846924199 skills 504 529 algorithm TrueSkill Predictor to match players that are close on skill based on their playing history andrank players based on the players they have played\n",
      "12917368283846924199 skills 505 529 TrueSkill Predictor to match players that are close on skill based on their playing history andrank players based on the players they have played\n",
      "12917368283846924199 skills 506 529 Predictor to match players that are close on skill based on their playing history andrank players based on the players they have played\n",
      "12917368283846924199 skills 507 529 to match players that are close on skill based on their playing history andrank players based on the players they have played\n",
      "12917368283846924199 skills 508 529 match players that are close on skill based on their playing history andrank players based on the players they have played\n",
      "12917368283846924199 skills 509 529 players that are close on skill based on their playing history andrank players based on the players they have played\n",
      "12917368283846924199 skills 510 529 that are close on skill based on their playing history andrank players based on the players they have played\n",
      "12917368283846924199 skills 511 529 are close on skill based on their playing history andrank players based on the players they have played\n",
      "12917368283846924199 skills 512 529 close on skill based on their playing history andrank players based on the players they have played\n",
      "12917368283846924199 skills 513 529 on skill based on their playing history andrank players based on the players they have played\n",
      "12917368283846924199 skills 514 529 skill based on their playing history andrank players based on the players they have played\n",
      "12917368283846924199 skills 503 530 Gaming algorithm TrueSkill Predictor to match players that are close on skill based on their playing history andrank players based on the players they have played with\n",
      "12917368283846924199 skills 504 530 algorithm TrueSkill Predictor to match players that are close on skill based on their playing history andrank players based on the players they have played with\n",
      "12917368283846924199 skills 505 530 TrueSkill Predictor to match players that are close on skill based on their playing history andrank players based on the players they have played with\n",
      "12917368283846924199 skills 506 530 Predictor to match players that are close on skill based on their playing history andrank players based on the players they have played with\n",
      "12917368283846924199 skills 507 530 to match players that are close on skill based on their playing history andrank players based on the players they have played with\n",
      "12917368283846924199 skills 508 530 match players that are close on skill based on their playing history andrank players based on the players they have played with\n",
      "12917368283846924199 skills 509 530 players that are close on skill based on their playing history andrank players based on the players they have played with\n",
      "12917368283846924199 skills 510 530 that are close on skill based on their playing history andrank players based on the players they have played with\n",
      "12917368283846924199 skills 511 530 are close on skill based on their playing history andrank players based on the players they have played with\n",
      "12917368283846924199 skills 512 530 close on skill based on their playing history andrank players based on the players they have played with\n",
      "12917368283846924199 skills 513 530 on skill based on their playing history andrank players based on the players they have played with\n",
      "12917368283846924199 skills 514 530 skill based on their playing history andrank players based on the players they have played with\n",
      "12917368283846924199 skills 503 531 Gaming algorithm TrueSkill Predictor to match players that are close on skill based on their playing history andrank players based on the players they have played with in\n",
      "12917368283846924199 skills 504 531 algorithm TrueSkill Predictor to match players that are close on skill based on their playing history andrank players based on the players they have played with in\n",
      "12917368283846924199 skills 505 531 TrueSkill Predictor to match players that are close on skill based on their playing history andrank players based on the players they have played with in\n",
      "12917368283846924199 skills 506 531 Predictor to match players that are close on skill based on their playing history andrank players based on the players they have played with in\n",
      "12917368283846924199 skills 507 531 to match players that are close on skill based on their playing history andrank players based on the players they have played with in\n",
      "12917368283846924199 skills 508 531 match players that are close on skill based on their playing history andrank players based on the players they have played with in\n",
      "12917368283846924199 skills 509 531 players that are close on skill based on their playing history andrank players based on the players they have played with in\n",
      "12917368283846924199 skills 510 531 that are close on skill based on their playing history andrank players based on the players they have played with in\n",
      "12917368283846924199 skills 511 531 are close on skill based on their playing history andrank players based on the players they have played with in\n",
      "12917368283846924199 skills 512 531 close on skill based on their playing history andrank players based on the players they have played with in\n",
      "12917368283846924199 skills 513 531 on skill based on their playing history andrank players based on the players they have played with in\n",
      "12917368283846924199 skills 514 531 skill based on their playing history andrank players based on the players they have played with in\n",
      "12917368283846924199 skills 503 532 Gaming algorithm TrueSkill Predictor to match players that are close on skill based on their playing history andrank players based on the players they have played with in the\n",
      "12917368283846924199 skills 504 532 algorithm TrueSkill Predictor to match players that are close on skill based on their playing history andrank players based on the players they have played with in the\n",
      "12917368283846924199 skills 505 532 TrueSkill Predictor to match players that are close on skill based on their playing history andrank players based on the players they have played with in the\n",
      "12917368283846924199 skills 506 532 Predictor to match players that are close on skill based on their playing history andrank players based on the players they have played with in the\n",
      "12917368283846924199 skills 507 532 to match players that are close on skill based on their playing history andrank players based on the players they have played with in the\n",
      "12917368283846924199 skills 508 532 match players that are close on skill based on their playing history andrank players based on the players they have played with in the\n",
      "12917368283846924199 skills 509 532 players that are close on skill based on their playing history andrank players based on the players they have played with in the\n",
      "12917368283846924199 skills 510 532 that are close on skill based on their playing history andrank players based on the players they have played with in the\n",
      "12917368283846924199 skills 511 532 are close on skill based on their playing history andrank players based on the players they have played with in the\n",
      "12917368283846924199 skills 512 532 close on skill based on their playing history andrank players based on the players they have played with in the\n",
      "12917368283846924199 skills 513 532 on skill based on their playing history andrank players based on the players they have played with in the\n",
      "12917368283846924199 skills 514 532 skill based on their playing history andrank players based on the players they have played with in the\n",
      "12917368283846924199 skills 503 533 Gaming algorithm TrueSkill Predictor to match players that are close on skill based on their playing history andrank players based on the players they have played with in the context\n",
      "12917368283846924199 skills 504 533 algorithm TrueSkill Predictor to match players that are close on skill based on their playing history andrank players based on the players they have played with in the context\n",
      "12917368283846924199 skills 505 533 TrueSkill Predictor to match players that are close on skill based on their playing history andrank players based on the players they have played with in the context\n",
      "12917368283846924199 skills 506 533 Predictor to match players that are close on skill based on their playing history andrank players based on the players they have played with in the context\n",
      "12917368283846924199 skills 507 533 to match players that are close on skill based on their playing history andrank players based on the players they have played with in the context\n",
      "12917368283846924199 skills 508 533 match players that are close on skill based on their playing history andrank players based on the players they have played with in the context\n",
      "12917368283846924199 skills 509 533 players that are close on skill based on their playing history andrank players based on the players they have played with in the context\n",
      "12917368283846924199 skills 510 533 that are close on skill based on their playing history andrank players based on the players they have played with in the context\n",
      "12917368283846924199 skills 511 533 are close on skill based on their playing history andrank players based on the players they have played with in the context\n",
      "12917368283846924199 skills 512 533 close on skill based on their playing history andrank players based on the players they have played with in the context\n",
      "12917368283846924199 skills 513 533 on skill based on their playing history andrank players based on the players they have played with in the context\n",
      "12917368283846924199 skills 514 533 skill based on their playing history andrank players based on the players they have played with in the context\n",
      "12917368283846924199 skills 503 534 Gaming algorithm TrueSkill Predictor to match players that are close on skill based on their playing history andrank players based on the players they have played with in the context of\n",
      "12917368283846924199 skills 504 534 algorithm TrueSkill Predictor to match players that are close on skill based on their playing history andrank players based on the players they have played with in the context of\n",
      "12917368283846924199 skills 505 534 TrueSkill Predictor to match players that are close on skill based on their playing history andrank players based on the players they have played with in the context of\n",
      "12917368283846924199 skills 506 534 Predictor to match players that are close on skill based on their playing history andrank players based on the players they have played with in the context of\n",
      "12917368283846924199 skills 507 534 to match players that are close on skill based on their playing history andrank players based on the players they have played with in the context of\n",
      "12917368283846924199 skills 508 534 match players that are close on skill based on their playing history andrank players based on the players they have played with in the context of\n",
      "12917368283846924199 skills 509 534 players that are close on skill based on their playing history andrank players based on the players they have played with in the context of\n",
      "12917368283846924199 skills 510 534 that are close on skill based on their playing history andrank players based on the players they have played with in the context of\n",
      "12917368283846924199 skills 511 534 are close on skill based on their playing history andrank players based on the players they have played with in the context of\n",
      "12917368283846924199 skills 512 534 close on skill based on their playing history andrank players based on the players they have played with in the context of\n",
      "12917368283846924199 skills 513 534 on skill based on their playing history andrank players based on the players they have played with in the context of\n",
      "12917368283846924199 skills 514 534 skill based on their playing history andrank players based on the players they have played with in the context of\n",
      "12917368283846924199 skills 503 535 Gaming algorithm TrueSkill Predictor to match players that are close on skill based on their playing history andrank players based on the players they have played with in the context of CTR\n",
      "12917368283846924199 skills 504 535 algorithm TrueSkill Predictor to match players that are close on skill based on their playing history andrank players based on the players they have played with in the context of CTR\n",
      "12917368283846924199 skills 505 535 TrueSkill Predictor to match players that are close on skill based on their playing history andrank players based on the players they have played with in the context of CTR\n",
      "12917368283846924199 skills 506 535 Predictor to match players that are close on skill based on their playing history andrank players based on the players they have played with in the context of CTR\n",
      "12917368283846924199 skills 507 535 to match players that are close on skill based on their playing history andrank players based on the players they have played with in the context of CTR\n",
      "12917368283846924199 skills 508 535 match players that are close on skill based on their playing history andrank players based on the players they have played with in the context of CTR\n",
      "12917368283846924199 skills 509 535 players that are close on skill based on their playing history andrank players based on the players they have played with in the context of CTR\n",
      "12917368283846924199 skills 510 535 that are close on skill based on their playing history andrank players based on the players they have played with in the context of CTR\n",
      "12917368283846924199 skills 511 535 are close on skill based on their playing history andrank players based on the players they have played with in the context of CTR\n",
      "12917368283846924199 skills 512 535 close on skill based on their playing history andrank players based on the players they have played with in the context of CTR\n",
      "12917368283846924199 skills 513 535 on skill based on their playing history andrank players based on the players they have played with in the context of CTR\n",
      "12917368283846924199 skills 514 535 skill based on their playing history andrank players based on the players they have played with in the context of CTR\n",
      "12917368283846924199 skills 503 536 Gaming algorithm TrueSkill Predictor to match players that are close on skill based on their playing history andrank players based on the players they have played with in the context of CTR prediction\n",
      "12917368283846924199 skills 504 536 algorithm TrueSkill Predictor to match players that are close on skill based on their playing history andrank players based on the players they have played with in the context of CTR prediction\n",
      "12917368283846924199 skills 505 536 TrueSkill Predictor to match players that are close on skill based on their playing history andrank players based on the players they have played with in the context of CTR prediction\n",
      "12917368283846924199 skills 506 536 Predictor to match players that are close on skill based on their playing history andrank players based on the players they have played with in the context of CTR prediction\n",
      "12917368283846924199 skills 507 536 to match players that are close on skill based on their playing history andrank players based on the players they have played with in the context of CTR prediction\n",
      "12917368283846924199 skills 508 536 match players that are close on skill based on their playing history andrank players based on the players they have played with in the context of CTR prediction\n",
      "12917368283846924199 skills 509 536 players that are close on skill based on their playing history andrank players based on the players they have played with in the context of CTR prediction\n",
      "12917368283846924199 skills 510 536 that are close on skill based on their playing history andrank players based on the players they have played with in the context of CTR prediction\n",
      "12917368283846924199 skills 511 536 are close on skill based on their playing history andrank players based on the players they have played with in the context of CTR prediction\n",
      "12917368283846924199 skills 512 536 close on skill based on their playing history andrank players based on the players they have played with in the context of CTR prediction\n",
      "12917368283846924199 skills 513 536 on skill based on their playing history andrank players based on the players they have played with in the context of CTR prediction\n",
      "12917368283846924199 skills 514 536 skill based on their playing history andrank players based on the players they have played with in the context of CTR prediction\n",
      "12917368283846924199 skills 503 537 Gaming algorithm TrueSkill Predictor to match players that are close on skill based on their playing history andrank players based on the players they have played with in the context of CTR prediction in\n",
      "12917368283846924199 skills 504 537 algorithm TrueSkill Predictor to match players that are close on skill based on their playing history andrank players based on the players they have played with in the context of CTR prediction in\n",
      "12917368283846924199 skills 505 537 TrueSkill Predictor to match players that are close on skill based on their playing history andrank players based on the players they have played with in the context of CTR prediction in\n",
      "12917368283846924199 skills 506 537 Predictor to match players that are close on skill based on their playing history andrank players based on the players they have played with in the context of CTR prediction in\n",
      "12917368283846924199 skills 507 537 to match players that are close on skill based on their playing history andrank players based on the players they have played with in the context of CTR prediction in\n",
      "12917368283846924199 skills 508 537 match players that are close on skill based on their playing history andrank players based on the players they have played with in the context of CTR prediction in\n",
      "12917368283846924199 skills 509 537 players that are close on skill based on their playing history andrank players based on the players they have played with in the context of CTR prediction in\n",
      "12917368283846924199 skills 510 537 that are close on skill based on their playing history andrank players based on the players they have played with in the context of CTR prediction in\n",
      "12917368283846924199 skills 511 537 are close on skill based on their playing history andrank players based on the players they have played with in the context of CTR prediction in\n",
      "12917368283846924199 skills 512 537 close on skill based on their playing history andrank players based on the players they have played with in the context of CTR prediction in\n",
      "12917368283846924199 skills 513 537 on skill based on their playing history andrank players based on the players they have played with in the context of CTR prediction in\n",
      "12917368283846924199 skills 514 537 skill based on their playing history andrank players based on the players they have played with in the context of CTR prediction in\n",
      "12917368283846924199 skills 503 538 Gaming algorithm TrueSkill Predictor to match players that are close on skill based on their playing history andrank players based on the players they have played with in the context of CTR prediction in advertising\n",
      "12917368283846924199 skills 504 538 algorithm TrueSkill Predictor to match players that are close on skill based on their playing history andrank players based on the players they have played with in the context of CTR prediction in advertising\n",
      "12917368283846924199 skills 505 538 TrueSkill Predictor to match players that are close on skill based on their playing history andrank players based on the players they have played with in the context of CTR prediction in advertising\n",
      "12917368283846924199 skills 506 538 Predictor to match players that are close on skill based on their playing history andrank players based on the players they have played with in the context of CTR prediction in advertising\n",
      "12917368283846924199 skills 507 538 to match players that are close on skill based on their playing history andrank players based on the players they have played with in the context of CTR prediction in advertising\n",
      "12917368283846924199 skills 508 538 match players that are close on skill based on their playing history andrank players based on the players they have played with in the context of CTR prediction in advertising\n",
      "12917368283846924199 skills 509 538 players that are close on skill based on their playing history andrank players based on the players they have played with in the context of CTR prediction in advertising\n",
      "12917368283846924199 skills 510 538 that are close on skill based on their playing history andrank players based on the players they have played with in the context of CTR prediction in advertising\n",
      "12917368283846924199 skills 511 538 are close on skill based on their playing history andrank players based on the players they have played with in the context of CTR prediction in advertising\n",
      "12917368283846924199 skills 512 538 close on skill based on their playing history andrank players based on the players they have played with in the context of CTR prediction in advertising\n",
      "12917368283846924199 skills 513 538 on skill based on their playing history andrank players based on the players they have played with in the context of CTR prediction in advertising\n",
      "12917368283846924199 skills 514 538 skill based on their playing history andrank players based on the players they have played with in the context of CTR prediction in advertising\n",
      "12917368283846924199 skills 503 539 Gaming algorithm TrueSkill Predictor to match players that are close on skill based on their playing history andrank players based on the players they have played with in the context of CTR prediction in advertising domain\n",
      "12917368283846924199 skills 504 539 algorithm TrueSkill Predictor to match players that are close on skill based on their playing history andrank players based on the players they have played with in the context of CTR prediction in advertising domain\n",
      "12917368283846924199 skills 505 539 TrueSkill Predictor to match players that are close on skill based on their playing history andrank players based on the players they have played with in the context of CTR prediction in advertising domain\n",
      "12917368283846924199 skills 506 539 Predictor to match players that are close on skill based on their playing history andrank players based on the players they have played with in the context of CTR prediction in advertising domain\n",
      "12917368283846924199 skills 507 539 to match players that are close on skill based on their playing history andrank players based on the players they have played with in the context of CTR prediction in advertising domain\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12917368283846924199 skills 508 539 match players that are close on skill based on their playing history andrank players based on the players they have played with in the context of CTR prediction in advertising domain\n",
      "12917368283846924199 skills 509 539 players that are close on skill based on their playing history andrank players based on the players they have played with in the context of CTR prediction in advertising domain\n",
      "12917368283846924199 skills 510 539 that are close on skill based on their playing history andrank players based on the players they have played with in the context of CTR prediction in advertising domain\n",
      "12917368283846924199 skills 511 539 are close on skill based on their playing history andrank players based on the players they have played with in the context of CTR prediction in advertising domain\n",
      "12917368283846924199 skills 512 539 close on skill based on their playing history andrank players based on the players they have played with in the context of CTR prediction in advertising domain\n",
      "12917368283846924199 skills 513 539 on skill based on their playing history andrank players based on the players they have played with in the context of CTR prediction in advertising domain\n",
      "12917368283846924199 skills 514 539 skill based on their playing history andrank players based on the players they have played with in the context of CTR prediction in advertising domain\n",
      "12917368283846924199 skills 1037 1043 Healthcare Professional or not) SKILLS\n",
      "12917368283846924199 skills 1038 1043 Professional or not) SKILLS\n",
      "12917368283846924199 skills 1039 1043 or not) SKILLS\n",
      "12917368283846924199 skills 1040 1043 not) SKILLS\n",
      "12917368283846924199 skills 1041 1043 ) SKILLS\n",
      "12917368283846924199 skills 1042 1043 SKILLS\n",
      "12917368283846924199 skills 246 249 hour) Skills\n",
      "12917368283846924199 skills 247 249 ) Skills\n",
      "12917368283846924199 skills 248 249 Skills\n",
      "12917368283846924199 skills 61 78 Data Science at theDepartment of Applied Mathematics and Computational Sciences at PSG College of Technology.SKILL\n",
      "12917368283846924199 skills 62 78 Science at theDepartment of Applied Mathematics and Computational Sciences at PSG College of Technology.SKILL\n",
      "12917368283846924199 skills 63 78 at theDepartment of Applied Mathematics and Computational Sciences at PSG College of Technology.SKILL\n",
      "12917368283846924199 skills 64 78 theDepartment of Applied Mathematics and Computational Sciences at PSG College of Technology.SKILL\n",
      "12917368283846924199 skills 65 78 of Applied Mathematics and Computational Sciences at PSG College of Technology.SKILL\n",
      "12917368283846924199 skills 66 78 Applied Mathematics and Computational Sciences at PSG College of Technology.SKILL\n",
      "12917368283846924199 skills 67 78 Mathematics and Computational Sciences at PSG College of Technology.SKILL\n",
      "12917368283846924199 skills 68 78 and Computational Sciences at PSG College of Technology.SKILL\n",
      "12917368283846924199 skills 69 78 Computational Sciences at PSG College of Technology.SKILL\n",
      "12917368283846924199 skills 70 78 Sciences at PSG College of Technology.SKILL\n",
      "12917368283846924199 skills 71 78 at PSG College of Technology.SKILL\n",
      "12917368283846924199 skills 72 78 PSG College of Technology.SKILL\n",
      "12917368283846924199 skills 73 78 College of Technology.SKILL\n",
      "12917368283846924199 skills 74 78 of Technology.SKILL\n",
      "12917368283846924199 skills 75 78 Technology.SKILL\n",
      "12917368283846924199 skills 76 78 .SKILL\n",
      "12917368283846924199 skills 77 78 SKILL\n",
      "12917368283846924199 skills 61 79 Data Science at theDepartment of Applied Mathematics and Computational Sciences at PSG College of Technology.SKILL SETLanguagesPython\n",
      "12917368283846924199 skills 62 79 Science at theDepartment of Applied Mathematics and Computational Sciences at PSG College of Technology.SKILL SETLanguagesPython\n",
      "12917368283846924199 skills 63 79 at theDepartment of Applied Mathematics and Computational Sciences at PSG College of Technology.SKILL SETLanguagesPython\n",
      "12917368283846924199 skills 64 79 theDepartment of Applied Mathematics and Computational Sciences at PSG College of Technology.SKILL SETLanguagesPython\n",
      "12917368283846924199 skills 65 79 of Applied Mathematics and Computational Sciences at PSG College of Technology.SKILL SETLanguagesPython\n",
      "12917368283846924199 skills 66 79 Applied Mathematics and Computational Sciences at PSG College of Technology.SKILL SETLanguagesPython\n",
      "12917368283846924199 skills 67 79 Mathematics and Computational Sciences at PSG College of Technology.SKILL SETLanguagesPython\n",
      "12917368283846924199 skills 68 79 and Computational Sciences at PSG College of Technology.SKILL SETLanguagesPython\n",
      "12917368283846924199 skills 69 79 Computational Sciences at PSG College of Technology.SKILL SETLanguagesPython\n",
      "12917368283846924199 skills 70 79 Sciences at PSG College of Technology.SKILL SETLanguagesPython\n",
      "12917368283846924199 skills 71 79 at PSG College of Technology.SKILL SETLanguagesPython\n",
      "12917368283846924199 skills 72 79 PSG College of Technology.SKILL SETLanguagesPython\n",
      "12917368283846924199 skills 73 79 College of Technology.SKILL SETLanguagesPython\n",
      "12917368283846924199 skills 74 79 of Technology.SKILL SETLanguagesPython\n",
      "12917368283846924199 skills 75 79 Technology.SKILL SETLanguagesPython\n",
      "12917368283846924199 skills 76 79 .SKILL SETLanguagesPython\n",
      "12917368283846924199 skills 77 79 SKILL SETLanguagesPython\n",
      "12917368283846924199 skills 23 47 To pursue a challenging career and be a part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills\n",
      "12917368283846924199 skills 24 47 pursue a challenging career and be a part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills\n",
      "12917368283846924199 skills 25 47 a challenging career and be a part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills\n",
      "12917368283846924199 skills 26 47 challenging career and be a part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills\n",
      "12917368283846924199 skills 27 47 career and be a part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills\n",
      "12917368283846924199 skills 28 47 and be a part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills\n",
      "12917368283846924199 skills 29 47 be a part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills\n",
      "12917368283846924199 skills 30 47 a part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills\n",
      "12917368283846924199 skills 31 47 part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills\n",
      "12917368283846924199 skills 32 47 of progressive organization that gives a scope to enhance my knowledge and utilizing my skills\n",
      "12917368283846924199 skills 33 47 progressive organization that gives a scope to enhance my knowledge and utilizing my skills\n",
      "12917368283846924199 skills 34 47 organization that gives a scope to enhance my knowledge and utilizing my skills\n",
      "12917368283846924199 skills 35 47 that gives a scope to enhance my knowledge and utilizing my skills\n",
      "12917368283846924199 skills 36 47 gives a scope to enhance my knowledge and utilizing my skills\n",
      "12917368283846924199 skills 37 47 a scope to enhance my knowledge and utilizing my skills\n",
      "12917368283846924199 skills 38 47 scope to enhance my knowledge and utilizing my skills\n",
      "12917368283846924199 skills 39 47 to enhance my knowledge and utilizing my skills\n",
      "12917368283846924199 skills 40 47 enhance my knowledge and utilizing my skills\n",
      "12917368283846924199 skills 41 47 my knowledge and utilizing my skills\n",
      "12917368283846924199 skills 42 47 knowledge and utilizing my skills\n",
      "12917368283846924199 skills 43 47 and utilizing my skills\n",
      "12917368283846924199 skills 44 47 utilizing my skills\n",
      "12917368283846924199 skills 45 47 my skills\n",
      "12917368283846924199 skills 46 47 skills\n",
      "12917368283846924199 skills 23 48 To pursue a challenging career and be a part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards\n",
      "12917368283846924199 skills 24 48 pursue a challenging career and be a part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards\n",
      "12917368283846924199 skills 25 48 a challenging career and be a part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards\n",
      "12917368283846924199 skills 26 48 challenging career and be a part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards\n",
      "12917368283846924199 skills 27 48 career and be a part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards\n",
      "12917368283846924199 skills 28 48 and be a part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards\n",
      "12917368283846924199 skills 29 48 be a part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards\n",
      "12917368283846924199 skills 30 48 a part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards\n",
      "12917368283846924199 skills 31 48 part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards\n",
      "12917368283846924199 skills 32 48 of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards\n",
      "12917368283846924199 skills 33 48 progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards\n",
      "12917368283846924199 skills 34 48 organization that gives a scope to enhance my knowledge and utilizing my skills towards\n",
      "12917368283846924199 skills 35 48 that gives a scope to enhance my knowledge and utilizing my skills towards\n",
      "12917368283846924199 skills 36 48 gives a scope to enhance my knowledge and utilizing my skills towards\n",
      "12917368283846924199 skills 37 48 a scope to enhance my knowledge and utilizing my skills towards\n",
      "12917368283846924199 skills 38 48 scope to enhance my knowledge and utilizing my skills towards\n",
      "12917368283846924199 skills 39 48 to enhance my knowledge and utilizing my skills towards\n",
      "12917368283846924199 skills 40 48 enhance my knowledge and utilizing my skills towards\n",
      "12917368283846924199 skills 41 48 my knowledge and utilizing my skills towards\n",
      "12917368283846924199 skills 42 48 knowledge and utilizing my skills towards\n",
      "12917368283846924199 skills 43 48 and utilizing my skills towards\n",
      "12917368283846924199 skills 44 48 utilizing my skills towards\n",
      "12917368283846924199 skills 45 48 my skills towards\n",
      "12917368283846924199 skills 46 48 skills towards\n",
      "12917368283846924199 skills 23 49 To pursue a challenging career and be a part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards the\n",
      "12917368283846924199 skills 24 49 pursue a challenging career and be a part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards the\n",
      "12917368283846924199 skills 25 49 a challenging career and be a part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards the\n",
      "12917368283846924199 skills 26 49 challenging career and be a part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards the\n",
      "12917368283846924199 skills 27 49 career and be a part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards the\n",
      "12917368283846924199 skills 28 49 and be a part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards the\n",
      "12917368283846924199 skills 29 49 be a part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards the\n",
      "12917368283846924199 skills 30 49 a part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards the\n",
      "12917368283846924199 skills 31 49 part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards the\n",
      "12917368283846924199 skills 32 49 of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards the\n",
      "12917368283846924199 skills 33 49 progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards the\n",
      "12917368283846924199 skills 34 49 organization that gives a scope to enhance my knowledge and utilizing my skills towards the\n",
      "12917368283846924199 skills 35 49 that gives a scope to enhance my knowledge and utilizing my skills towards the\n",
      "12917368283846924199 skills 36 49 gives a scope to enhance my knowledge and utilizing my skills towards the\n",
      "12917368283846924199 skills 37 49 a scope to enhance my knowledge and utilizing my skills towards the\n",
      "12917368283846924199 skills 38 49 scope to enhance my knowledge and utilizing my skills towards the\n",
      "12917368283846924199 skills 39 49 to enhance my knowledge and utilizing my skills towards the\n",
      "12917368283846924199 skills 40 49 enhance my knowledge and utilizing my skills towards the\n",
      "12917368283846924199 skills 41 49 my knowledge and utilizing my skills towards the\n",
      "12917368283846924199 skills 42 49 knowledge and utilizing my skills towards the\n",
      "12917368283846924199 skills 43 49 and utilizing my skills towards the\n",
      "12917368283846924199 skills 44 49 utilizing my skills towards the\n",
      "12917368283846924199 skills 45 49 my skills towards the\n",
      "12917368283846924199 skills 46 49 skills towards the\n",
      "12917368283846924199 skills 23 50 To pursue a challenging career and be a part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards the growth\n",
      "12917368283846924199 skills 24 50 pursue a challenging career and be a part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards the growth\n",
      "12917368283846924199 skills 25 50 a challenging career and be a part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards the growth\n",
      "12917368283846924199 skills 26 50 challenging career and be a part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards the growth\n",
      "12917368283846924199 skills 27 50 career and be a part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards the growth\n",
      "12917368283846924199 skills 28 50 and be a part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards the growth\n",
      "12917368283846924199 skills 29 50 be a part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards the growth\n",
      "12917368283846924199 skills 30 50 a part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards the growth\n",
      "12917368283846924199 skills 31 50 part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards the growth\n",
      "12917368283846924199 skills 32 50 of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards the growth\n",
      "12917368283846924199 skills 33 50 progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards the growth\n",
      "12917368283846924199 skills 34 50 organization that gives a scope to enhance my knowledge and utilizing my skills towards the growth\n",
      "12917368283846924199 skills 35 50 that gives a scope to enhance my knowledge and utilizing my skills towards the growth\n",
      "12917368283846924199 skills 36 50 gives a scope to enhance my knowledge and utilizing my skills towards the growth\n",
      "12917368283846924199 skills 37 50 a scope to enhance my knowledge and utilizing my skills towards the growth\n",
      "12917368283846924199 skills 38 50 scope to enhance my knowledge and utilizing my skills towards the growth\n",
      "12917368283846924199 skills 39 50 to enhance my knowledge and utilizing my skills towards the growth\n",
      "12917368283846924199 skills 40 50 enhance my knowledge and utilizing my skills towards the growth\n",
      "12917368283846924199 skills 41 50 my knowledge and utilizing my skills towards the growth\n",
      "12917368283846924199 skills 42 50 knowledge and utilizing my skills towards the growth\n",
      "12917368283846924199 skills 43 50 and utilizing my skills towards the growth\n",
      "12917368283846924199 skills 44 50 utilizing my skills towards the growth\n",
      "12917368283846924199 skills 45 50 my skills towards the growth\n",
      "12917368283846924199 skills 46 50 skills towards the growth\n",
      "12917368283846924199 skills 23 51 To pursue a challenging career and be a part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards the growth of\n",
      "12917368283846924199 skills 24 51 pursue a challenging career and be a part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards the growth of\n",
      "12917368283846924199 skills 25 51 a challenging career and be a part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards the growth of\n",
      "12917368283846924199 skills 26 51 challenging career and be a part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards the growth of\n",
      "12917368283846924199 skills 27 51 career and be a part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards the growth of\n",
      "12917368283846924199 skills 28 51 and be a part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards the growth of\n",
      "12917368283846924199 skills 29 51 be a part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards the growth of\n",
      "12917368283846924199 skills 30 51 a part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards the growth of\n",
      "12917368283846924199 skills 31 51 part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards the growth of\n",
      "12917368283846924199 skills 32 51 of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards the growth of\n",
      "12917368283846924199 skills 33 51 progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards the growth of\n",
      "12917368283846924199 skills 34 51 organization that gives a scope to enhance my knowledge and utilizing my skills towards the growth of\n",
      "12917368283846924199 skills 35 51 that gives a scope to enhance my knowledge and utilizing my skills towards the growth of\n",
      "12917368283846924199 skills 36 51 gives a scope to enhance my knowledge and utilizing my skills towards the growth of\n",
      "12917368283846924199 skills 37 51 a scope to enhance my knowledge and utilizing my skills towards the growth of\n",
      "12917368283846924199 skills 38 51 scope to enhance my knowledge and utilizing my skills towards the growth of\n",
      "12917368283846924199 skills 39 51 to enhance my knowledge and utilizing my skills towards the growth of\n",
      "12917368283846924199 skills 40 51 enhance my knowledge and utilizing my skills towards the growth of\n",
      "12917368283846924199 skills 41 51 my knowledge and utilizing my skills towards the growth of\n",
      "12917368283846924199 skills 42 51 knowledge and utilizing my skills towards the growth of\n",
      "12917368283846924199 skills 43 51 and utilizing my skills towards the growth of\n",
      "12917368283846924199 skills 44 51 utilizing my skills towards the growth of\n",
      "12917368283846924199 skills 45 51 my skills towards the growth of\n",
      "12917368283846924199 skills 46 51 skills towards the growth of\n",
      "12917368283846924199 skills 23 52 To pursue a challenging career and be a part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards the growth of the\n",
      "12917368283846924199 skills 24 52 pursue a challenging career and be a part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards the growth of the\n",
      "12917368283846924199 skills 25 52 a challenging career and be a part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards the growth of the\n",
      "12917368283846924199 skills 26 52 challenging career and be a part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards the growth of the\n",
      "12917368283846924199 skills 27 52 career and be a part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards the growth of the\n",
      "12917368283846924199 skills 28 52 and be a part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards the growth of the\n",
      "12917368283846924199 skills 29 52 be a part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards the growth of the\n",
      "12917368283846924199 skills 30 52 a part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards the growth of the\n",
      "12917368283846924199 skills 31 52 part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards the growth of the\n",
      "12917368283846924199 skills 32 52 of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards the growth of the\n",
      "12917368283846924199 skills 33 52 progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards the growth of the\n",
      "12917368283846924199 skills 34 52 organization that gives a scope to enhance my knowledge and utilizing my skills towards the growth of the\n",
      "12917368283846924199 skills 35 52 that gives a scope to enhance my knowledge and utilizing my skills towards the growth of the\n",
      "12917368283846924199 skills 36 52 gives a scope to enhance my knowledge and utilizing my skills towards the growth of the\n",
      "12917368283846924199 skills 37 52 a scope to enhance my knowledge and utilizing my skills towards the growth of the\n",
      "12917368283846924199 skills 38 52 scope to enhance my knowledge and utilizing my skills towards the growth of the\n",
      "12917368283846924199 skills 39 52 to enhance my knowledge and utilizing my skills towards the growth of the\n",
      "12917368283846924199 skills 40 52 enhance my knowledge and utilizing my skills towards the growth of the\n",
      "12917368283846924199 skills 41 52 my knowledge and utilizing my skills towards the growth of the\n",
      "12917368283846924199 skills 42 52 knowledge and utilizing my skills towards the growth of the\n",
      "12917368283846924199 skills 43 52 and utilizing my skills towards the growth of the\n",
      "12917368283846924199 skills 44 52 utilizing my skills towards the growth of the\n",
      "12917368283846924199 skills 45 52 my skills towards the growth of the\n",
      "12917368283846924199 skills 46 52 skills towards the growth of the\n",
      "12917368283846924199 skills 23 53 To pursue a challenging career and be a part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards the growth of the organization\n",
      "12917368283846924199 skills 24 53 pursue a challenging career and be a part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards the growth of the organization\n",
      "12917368283846924199 skills 25 53 a challenging career and be a part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards the growth of the organization\n",
      "12917368283846924199 skills 26 53 challenging career and be a part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards the growth of the organization\n",
      "12917368283846924199 skills 27 53 career and be a part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards the growth of the organization\n",
      "12917368283846924199 skills 28 53 and be a part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards the growth of the organization\n",
      "12917368283846924199 skills 29 53 be a part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards the growth of the organization\n",
      "12917368283846924199 skills 30 53 a part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards the growth of the organization\n",
      "12917368283846924199 skills 31 53 part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards the growth of the organization\n",
      "12917368283846924199 skills 32 53 of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards the growth of the organization\n",
      "12917368283846924199 skills 33 53 progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards the growth of the organization\n",
      "12917368283846924199 skills 34 53 organization that gives a scope to enhance my knowledge and utilizing my skills towards the growth of the organization\n",
      "12917368283846924199 skills 35 53 that gives a scope to enhance my knowledge and utilizing my skills towards the growth of the organization\n",
      "12917368283846924199 skills 36 53 gives a scope to enhance my knowledge and utilizing my skills towards the growth of the organization\n",
      "12917368283846924199 skills 37 53 a scope to enhance my knowledge and utilizing my skills towards the growth of the organization\n",
      "12917368283846924199 skills 38 53 scope to enhance my knowledge and utilizing my skills towards the growth of the organization\n",
      "12917368283846924199 skills 39 53 to enhance my knowledge and utilizing my skills towards the growth of the organization\n",
      "12917368283846924199 skills 40 53 enhance my knowledge and utilizing my skills towards the growth of the organization\n",
      "12917368283846924199 skills 41 53 my knowledge and utilizing my skills towards the growth of the organization\n",
      "12917368283846924199 skills 42 53 knowledge and utilizing my skills towards the growth of the organization\n",
      "12917368283846924199 skills 43 53 and utilizing my skills towards the growth of the organization\n",
      "12917368283846924199 skills 44 53 utilizing my skills towards the growth of the organization\n",
      "12917368283846924199 skills 45 53 my skills towards the growth of the organization\n",
      "12917368283846924199 skills 46 53 skills towards the growth of the organization\n",
      "12917368283846924199 skills 23 54 To pursue a challenging career and be a part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards the growth of the organization and\n",
      "12917368283846924199 skills 24 54 pursue a challenging career and be a part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards the growth of the organization and\n",
      "12917368283846924199 skills 25 54 a challenging career and be a part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards the growth of the organization and\n",
      "12917368283846924199 skills 26 54 challenging career and be a part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards the growth of the organization and\n",
      "12917368283846924199 skills 27 54 career and be a part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards the growth of the organization and\n",
      "12917368283846924199 skills 28 54 and be a part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards the growth of the organization and\n",
      "12917368283846924199 skills 29 54 be a part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards the growth of the organization and\n",
      "12917368283846924199 skills 30 54 a part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards the growth of the organization and\n",
      "12917368283846924199 skills 31 54 part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards the growth of the organization and\n",
      "12917368283846924199 skills 32 54 of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards the growth of the organization and\n",
      "12917368283846924199 skills 33 54 progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards the growth of the organization and\n",
      "12917368283846924199 skills 34 54 organization that gives a scope to enhance my knowledge and utilizing my skills towards the growth of the organization and\n",
      "12917368283846924199 skills 35 54 that gives a scope to enhance my knowledge and utilizing my skills towards the growth of the organization and\n",
      "12917368283846924199 skills 36 54 gives a scope to enhance my knowledge and utilizing my skills towards the growth of the organization and\n",
      "12917368283846924199 skills 37 54 a scope to enhance my knowledge and utilizing my skills towards the growth of the organization and\n",
      "12917368283846924199 skills 38 54 scope to enhance my knowledge and utilizing my skills towards the growth of the organization and\n",
      "12917368283846924199 skills 39 54 to enhance my knowledge and utilizing my skills towards the growth of the organization and\n",
      "12917368283846924199 skills 40 54 enhance my knowledge and utilizing my skills towards the growth of the organization and\n",
      "12917368283846924199 skills 41 54 my knowledge and utilizing my skills towards the growth of the organization and\n",
      "12917368283846924199 skills 42 54 knowledge and utilizing my skills towards the growth of the organization and\n",
      "12917368283846924199 skills 43 54 and utilizing my skills towards the growth of the organization and\n",
      "12917368283846924199 skills 44 54 utilizing my skills towards the growth of the organization and\n",
      "12917368283846924199 skills 45 54 my skills towards the growth of the organization and\n",
      "12917368283846924199 skills 46 54 skills towards the growth of the organization and\n",
      "12917368283846924199 skills 23 55 To pursue a challenging career and be a part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards the growth of the organization and self\n",
      "12917368283846924199 skills 24 55 pursue a challenging career and be a part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards the growth of the organization and self\n",
      "12917368283846924199 skills 25 55 a challenging career and be a part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards the growth of the organization and self\n",
      "12917368283846924199 skills 26 55 challenging career and be a part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards the growth of the organization and self\n",
      "12917368283846924199 skills 27 55 career and be a part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards the growth of the organization and self\n",
      "12917368283846924199 skills 28 55 and be a part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards the growth of the organization and self\n",
      "12917368283846924199 skills 29 55 be a part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards the growth of the organization and self\n",
      "12917368283846924199 skills 30 55 a part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards the growth of the organization and self\n",
      "12917368283846924199 skills 31 55 part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards the growth of the organization and self\n",
      "12917368283846924199 skills 32 55 of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards the growth of the organization and self\n",
      "12917368283846924199 skills 33 55 progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards the growth of the organization and self\n",
      "12917368283846924199 skills 34 55 organization that gives a scope to enhance my knowledge and utilizing my skills towards the growth of the organization and self\n",
      "12917368283846924199 skills 35 55 that gives a scope to enhance my knowledge and utilizing my skills towards the growth of the organization and self\n",
      "12917368283846924199 skills 36 55 gives a scope to enhance my knowledge and utilizing my skills towards the growth of the organization and self\n",
      "12917368283846924199 skills 37 55 a scope to enhance my knowledge and utilizing my skills towards the growth of the organization and self\n",
      "12917368283846924199 skills 38 55 scope to enhance my knowledge and utilizing my skills towards the growth of the organization and self\n",
      "12917368283846924199 skills 39 55 to enhance my knowledge and utilizing my skills towards the growth of the organization and self\n",
      "12917368283846924199 skills 40 55 enhance my knowledge and utilizing my skills towards the growth of the organization and self\n",
      "12917368283846924199 skills 41 55 my knowledge and utilizing my skills towards the growth of the organization and self\n",
      "12917368283846924199 skills 42 55 knowledge and utilizing my skills towards the growth of the organization and self\n",
      "12917368283846924199 skills 43 55 and utilizing my skills towards the growth of the organization and self\n",
      "12917368283846924199 skills 44 55 utilizing my skills towards the growth of the organization and self\n",
      "12917368283846924199 skills 45 55 my skills towards the growth of the organization and self\n",
      "12917368283846924199 skills 46 55 skills towards the growth of the organization and self\n",
      "12917368283846924199 skills 18 24 Data Scientist with hands on skills\n",
      "12917368283846924199 skills 19 24 Scientist with hands on skills\n",
      "12917368283846924199 skills 20 24 with hands on skills\n",
      "12917368283846924199 skills 21 24 hands on skills\n",
      "12917368283846924199 skills 22 24 on skills\n",
      "12917368283846924199 skills 23 24 skills\n",
      "12917368283846924199 skills 18 25 Data Scientist with hands on skills in\n",
      "12917368283846924199 skills 19 25 Scientist with hands on skills in\n",
      "12917368283846924199 skills 20 25 with hands on skills in\n",
      "12917368283846924199 skills 21 25 hands on skills in\n",
      "12917368283846924199 skills 22 25 on skills in\n",
      "12917368283846924199 skills 23 25 skills in\n",
      "12917368283846924199 skills 18 26 Data Scientist with hands on skills in Python\n",
      "12917368283846924199 skills 19 26 Scientist with hands on skills in Python\n",
      "12917368283846924199 skills 20 26 with hands on skills in Python\n",
      "12917368283846924199 skills 21 26 hands on skills in Python\n",
      "12917368283846924199 skills 22 26 on skills in Python\n",
      "12917368283846924199 skills 23 26 skills in Python\n",
      "12917368283846924199 skills 38 45 Model Building with goodvisualization and presentation skills\n",
      "12917368283846924199 skills 39 45 Building with goodvisualization and presentation skills\n",
      "12917368283846924199 skills 40 45 with goodvisualization and presentation skills\n",
      "12917368283846924199 skills 41 45 goodvisualization and presentation skills\n",
      "12917368283846924199 skills 42 45 and presentation skills\n",
      "12917368283846924199 skills 43 45 presentation skills\n",
      "12917368283846924199 skills 44 45 skills\n",
      "12917368283846924199 skills 310 312 PROGRAMMING SKILLS\n",
      "12917368283846924199 skills 311 312 SKILLS\n",
      "12917368283846924199 skills 27 29 PROFESSIONAL SKILLS\n",
      "12917368283846924199 skills 28 29 SKILLS\n",
      "12917368283846924199 skills 27 30 PROFESSIONAL SKILLS Machine\n",
      "12917368283846924199 skills 28 30 SKILLS Machine\n",
      "12917368283846924199 skills 27 31 PROFESSIONAL SKILLS Machine Learning\n",
      "12917368283846924199 skills 28 31 SKILLS Machine Learning\n",
      "12917368283846924199 skills 27 32 PROFESSIONAL SKILLS Machine Learning Algorithms\n",
      "12917368283846924199 skills 28 32 SKILLS Machine Learning Algorithms\n",
      "12917368283846924199 skills 27 33 PROFESSIONAL SKILLS Machine Learning Algorithms Statistical\n",
      "12917368283846924199 skills 28 33 SKILLS Machine Learning Algorithms Statistical\n",
      "12917368283846924199 skills 27 34 PROFESSIONAL SKILLS Machine Learning Algorithms Statistical Modelling\n",
      "12917368283846924199 skills 28 34 SKILLS Machine Learning Algorithms Statistical Modelling\n",
      "12917368283846924199 skills 27 35 PROFESSIONAL SKILLS Machine Learning Algorithms Statistical Modelling Python\n",
      "12917368283846924199 skills 28 35 SKILLS Machine Learning Algorithms Statistical Modelling Python\n",
      "12917368283846924199 skills 27 36 PROFESSIONAL SKILLS Machine Learning Algorithms Statistical Modelling Python Hive\n",
      "12917368283846924199 skills 28 36 SKILLS Machine Learning Algorithms Statistical Modelling Python Hive\n",
      "12917368283846924199 skills 27 37 PROFESSIONAL SKILLS Machine Learning Algorithms Statistical Modelling Python Hive PostgreSQL\n",
      "12917368283846924199 skills 28 37 SKILLS Machine Learning Algorithms Statistical Modelling Python Hive PostgreSQL\n",
      "12917368283846924199 skills 27 38 PROFESSIONAL SKILLS Machine Learning Algorithms Statistical Modelling Python Hive PostgreSQL Azure\n",
      "12917368283846924199 skills 28 38 SKILLS Machine Learning Algorithms Statistical Modelling Python Hive PostgreSQL Azure\n",
      "12917368283846924199 skills 27 39 PROFESSIONAL SKILLS Machine Learning Algorithms Statistical Modelling Python Hive PostgreSQL Azure DataBricks\n",
      "12917368283846924199 skills 28 39 SKILLS Machine Learning Algorithms Statistical Modelling Python Hive PostgreSQL Azure DataBricks\n",
      "12917368283846924199 skills 27 40 PROFESSIONAL SKILLS Machine Learning Algorithms Statistical Modelling Python Hive PostgreSQL Azure DataBricks Program\n",
      "12917368283846924199 skills 28 40 SKILLS Machine Learning Algorithms Statistical Modelling Python Hive PostgreSQL Azure DataBricks Program\n",
      "12917368283846924199 skills 27 41 PROFESSIONAL SKILLS Machine Learning Algorithms Statistical Modelling Python Hive PostgreSQL Azure DataBricks Program Management\n",
      "12917368283846924199 skills 28 41 SKILLS Machine Learning Algorithms Statistical Modelling Python Hive PostgreSQL Azure DataBricks Program Management\n",
      "12917368283846924199 skills 27 42 PROFESSIONAL SKILLS Machine Learning Algorithms Statistical Modelling Python Hive PostgreSQL Azure DataBricks Program Management Stakeholder\n",
      "12917368283846924199 skills 28 42 SKILLS Machine Learning Algorithms Statistical Modelling Python Hive PostgreSQL Azure DataBricks Program Management Stakeholder\n",
      "12917368283846924199 skills 27 43 PROFESSIONAL SKILLS Machine Learning Algorithms Statistical Modelling Python Hive PostgreSQL Azure DataBricks Program Management Stakeholder Management\n",
      "12917368283846924199 skills 28 43 SKILLS Machine Learning Algorithms Statistical Modelling Python Hive PostgreSQL Azure DataBricks Program Management Stakeholder Management\n",
      "12917368283846924199 skills 27 44 PROFESSIONAL SKILLS Machine Learning Algorithms Statistical Modelling Python Hive PostgreSQL Azure DataBricks Program Management Stakeholder Management EDUCATION\n",
      "12917368283846924199 skills 28 44 SKILLS Machine Learning Algorithms Statistical Modelling Python Hive PostgreSQL Azure DataBricks Program Management Stakeholder Management EDUCATION\n",
      "12917368283846924199 skills 27 45 PROFESSIONAL SKILLS Machine Learning Algorithms Statistical Modelling Python Hive PostgreSQL Azure DataBricks Program Management Stakeholder Management EDUCATION Experienced\n",
      "12917368283846924199 skills 28 45 SKILLS Machine Learning Algorithms Statistical Modelling Python Hive PostgreSQL Azure DataBricks Program Management Stakeholder Management EDUCATION Experienced\n",
      "12917368283846924199 skills 27 46 PROFESSIONAL SKILLS Machine Learning Algorithms Statistical Modelling Python Hive PostgreSQL Azure DataBricks Program Management Stakeholder Management EDUCATION Experienced and\n",
      "12917368283846924199 skills 28 46 SKILLS Machine Learning Algorithms Statistical Modelling Python Hive PostgreSQL Azure DataBricks Program Management Stakeholder Management EDUCATION Experienced and\n",
      "12917368283846924199 skills 27 47 PROFESSIONAL SKILLS Machine Learning Algorithms Statistical Modelling Python Hive PostgreSQL Azure DataBricks Program Management Stakeholder Management EDUCATION Experienced and Data\n",
      "12917368283846924199 skills 28 47 SKILLS Machine Learning Algorithms Statistical Modelling Python Hive PostgreSQL Azure DataBricks Program Management Stakeholder Management EDUCATION Experienced and Data\n",
      "12917368283846924199 skills 27 48 PROFESSIONAL SKILLS Machine Learning Algorithms Statistical Modelling Python Hive PostgreSQL Azure DataBricks Program Management Stakeholder Management EDUCATION Experienced and Data Driven\n",
      "12917368283846924199 skills 28 48 SKILLS Machine Learning Algorithms Statistical Modelling Python Hive PostgreSQL Azure DataBricks Program Management Stakeholder Management EDUCATION Experienced and Data Driven\n",
      "12917368283846924199 skills 27 49 PROFESSIONAL SKILLS Machine Learning Algorithms Statistical Modelling Python Hive PostgreSQL Azure DataBricks Program Management Stakeholder Management EDUCATION Experienced and Data Driven Data\n",
      "12917368283846924199 skills 28 49 SKILLS Machine Learning Algorithms Statistical Modelling Python Hive PostgreSQL Azure DataBricks Program Management Stakeholder Management EDUCATION Experienced and Data Driven Data\n",
      "12917368283846924199 skills 27 50 PROFESSIONAL SKILLS Machine Learning Algorithms Statistical Modelling Python Hive PostgreSQL Azure DataBricks Program Management Stakeholder Management EDUCATION Experienced and Data Driven Data Scientist\n",
      "12917368283846924199 skills 28 50 SKILLS Machine Learning Algorithms Statistical Modelling Python Hive PostgreSQL Azure DataBricks Program Management Stakeholder Management EDUCATION Experienced and Data Driven Data Scientist\n",
      "12917368283846924199 skills 27 51 PROFESSIONAL SKILLS Machine Learning Algorithms Statistical Modelling Python Hive PostgreSQL Azure DataBricks Program Management Stakeholder Management EDUCATION Experienced and Data Driven Data Scientist with\n",
      "12917368283846924199 skills 28 51 SKILLS Machine Learning Algorithms Statistical Modelling Python Hive PostgreSQL Azure DataBricks Program Management Stakeholder Management EDUCATION Experienced and Data Driven Data Scientist with\n",
      "12917368283846924199 skills 27 52 PROFESSIONAL SKILLS Machine Learning Algorithms Statistical Modelling Python Hive PostgreSQL Azure DataBricks Program Management Stakeholder Management EDUCATION Experienced and Data Driven Data Scientist with approx\n",
      "12917368283846924199 skills 28 52 SKILLS Machine Learning Algorithms Statistical Modelling Python Hive PostgreSQL Azure DataBricks Program Management Stakeholder Management EDUCATION Experienced and Data Driven Data Scientist with approx\n",
      "12917368283846924199 skills 978 984 Introduction to Probability and Data SKILLS\n",
      "12917368283846924199 skills 979 984 to Probability and Data SKILLS\n",
      "12917368283846924199 skills 980 984 Probability and Data SKILLS\n",
      "12917368283846924199 skills 981 984 and Data SKILLS\n",
      "12917368283846924199 skills 982 984 Data SKILLS\n",
      "12917368283846924199 skills 983 984 SKILLS\n",
      "12917368283846924199 skills 978 985 Introduction to Probability and Data SKILLS AND\n",
      "12917368283846924199 skills 979 985 to Probability and Data SKILLS AND\n",
      "12917368283846924199 skills 980 985 Probability and Data SKILLS AND\n",
      "12917368283846924199 skills 981 985 and Data SKILLS AND\n",
      "12917368283846924199 skills 982 985 Data SKILLS AND\n",
      "12917368283846924199 skills 983 985 SKILLS AND\n",
      "12917368283846924199 skills 978 986 Introduction to Probability and Data SKILLS AND EXPERTISE\n",
      "12917368283846924199 skills 979 986 to Probability and Data SKILLS AND EXPERTISE\n",
      "12917368283846924199 skills 980 986 Probability and Data SKILLS AND EXPERTISE\n",
      "12917368283846924199 skills 981 986 and Data SKILLS AND EXPERTISE\n",
      "12917368283846924199 skills 982 986 Data SKILLS AND EXPERTISE\n",
      "12917368283846924199 skills 983 986 SKILLS AND EXPERTISE\n",
      "12917368283846924199 skills 328 330 Interpersonal skills\n",
      "12917368283846924199 skills 329 330 skills\n",
      "12917368283846924199 skills 332 334 Strong skills\n",
      "12917368283846924199 skills 333 334 skills\n",
      "12917368283846924199 skills 332 335 Strong skills in\n",
      "12917368283846924199 skills 333 335 skills in\n",
      "12917368283846924199 skills 332 336 Strong skills in building\n",
      "12917368283846924199 skills 333 336 skills in building\n",
      "12917368283846924199 skills 332 337 Strong skills in building client\n",
      "12917368283846924199 skills 333 337 skills in building client\n",
      "12917368283846924199 skills 332 338 Strong skills in building client relationship\n",
      "12917368283846924199 skills 333 338 skills in building client relationship\n",
      "12917368283846924199 skills 349 353 interpersonal and leadership skills\n",
      "12917368283846924199 skills 350 353 and leadership skills\n",
      "12917368283846924199 skills 351 353 leadership skills\n",
      "12917368283846924199 skills 352 353 skills\n",
      "12917368283846924199 skills 460 463 Belgaum Key Skills\n",
      "12917368283846924199 skills 461 463 Key Skills\n",
      "12917368283846924199 skills 462 463 Skills\n",
      "12917368283846924199 skills 57 74 Data Science at theDepartment of Applied Mathematics and Computational Sciences at PSG College of Technology.SKILL\n",
      "12917368283846924199 skills 58 74 Science at theDepartment of Applied Mathematics and Computational Sciences at PSG College of Technology.SKILL\n",
      "12917368283846924199 skills 59 74 at theDepartment of Applied Mathematics and Computational Sciences at PSG College of Technology.SKILL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12917368283846924199 skills 60 74 theDepartment of Applied Mathematics and Computational Sciences at PSG College of Technology.SKILL\n",
      "12917368283846924199 skills 61 74 of Applied Mathematics and Computational Sciences at PSG College of Technology.SKILL\n",
      "12917368283846924199 skills 62 74 Applied Mathematics and Computational Sciences at PSG College of Technology.SKILL\n",
      "12917368283846924199 skills 63 74 Mathematics and Computational Sciences at PSG College of Technology.SKILL\n",
      "12917368283846924199 skills 64 74 and Computational Sciences at PSG College of Technology.SKILL\n",
      "12917368283846924199 skills 65 74 Computational Sciences at PSG College of Technology.SKILL\n",
      "12917368283846924199 skills 66 74 Sciences at PSG College of Technology.SKILL\n",
      "12917368283846924199 skills 67 74 at PSG College of Technology.SKILL\n",
      "12917368283846924199 skills 68 74 PSG College of Technology.SKILL\n",
      "12917368283846924199 skills 69 74 College of Technology.SKILL\n",
      "12917368283846924199 skills 70 74 of Technology.SKILL\n",
      "12917368283846924199 skills 71 74 Technology.SKILL\n",
      "12917368283846924199 skills 72 74 .SKILL\n",
      "12917368283846924199 skills 73 74 SKILL\n",
      "12917368283846924199 skills 32 38 Extremely motivated to constantlydevelop my skills\n",
      "12917368283846924199 skills 33 38 motivated to constantlydevelop my skills\n",
      "12917368283846924199 skills 34 38 to constantlydevelop my skills\n",
      "12917368283846924199 skills 35 38 constantlydevelop my skills\n",
      "12917368283846924199 skills 36 38 my skills\n",
      "12917368283846924199 skills 37 38 skills\n",
      "12917368283846924199 skills 154 161 Excellent interpersonal and customer relationship management skills\n",
      "12917368283846924199 skills 155 161 interpersonal and customer relationship management skills\n",
      "12917368283846924199 skills 156 161 and customer relationship management skills\n",
      "12917368283846924199 skills 157 161 customer relationship management skills\n",
      "12917368283846924199 skills 158 161 relationship management skills\n",
      "12917368283846924199 skills 159 161 management skills\n",
      "12917368283846924199 skills 160 161 skills\n",
      "12917368283846924199 skills 210 214 Possess good analytical skills\n",
      "12917368283846924199 skills 211 214 good analytical skills\n",
      "12917368283846924199 skills 212 214 analytical skills\n",
      "12917368283846924199 skills 213 214 skills\n",
      "12917368283846924199 skills 210 215 Possess good analytical skills and\n",
      "12917368283846924199 skills 211 215 good analytical skills and\n",
      "12917368283846924199 skills 212 215 analytical skills and\n",
      "12917368283846924199 skills 213 215 skills and\n",
      "12917368283846924199 skills 210 216 Possess good analytical skills and problem\n",
      "12917368283846924199 skills 211 216 good analytical skills and problem\n",
      "12917368283846924199 skills 212 216 analytical skills and problem\n",
      "12917368283846924199 skills 213 216 skills and problem\n",
      "12917368283846924199 skills 210 217 Possess good analytical skills and problem solving\n",
      "12917368283846924199 skills 211 217 good analytical skills and problem solving\n",
      "12917368283846924199 skills 212 217 analytical skills and problem solving\n",
      "12917368283846924199 skills 213 217 skills and problem solving\n",
      "12917368283846924199 skills 210 218 Possess good analytical skills and problem solving capabilities\n",
      "12917368283846924199 skills 211 218 good analytical skills and problem solving capabilities\n",
      "12917368283846924199 skills 212 218 analytical skills and problem solving capabilities\n",
      "12917368283846924199 skills 213 218 skills and problem solving capabilities\n",
      "12917368283846924199 skills 210 219 Possess good analytical skills and problem solving capabilities along\n",
      "12917368283846924199 skills 211 219 good analytical skills and problem solving capabilities along\n",
      "12917368283846924199 skills 212 219 analytical skills and problem solving capabilities along\n",
      "12917368283846924199 skills 213 219 skills and problem solving capabilities along\n",
      "12917368283846924199 skills 210 220 Possess good analytical skills and problem solving capabilities along with\n",
      "12917368283846924199 skills 211 220 good analytical skills and problem solving capabilities along with\n",
      "12917368283846924199 skills 212 220 analytical skills and problem solving capabilities along with\n",
      "12917368283846924199 skills 213 220 skills and problem solving capabilities along with\n",
      "12917368283846924199 skills 210 221 Possess good analytical skills and problem solving capabilities along with good\n",
      "12917368283846924199 skills 211 221 good analytical skills and problem solving capabilities along with good\n",
      "12917368283846924199 skills 212 221 analytical skills and problem solving capabilities along with good\n",
      "12917368283846924199 skills 213 221 skills and problem solving capabilities along with good\n",
      "12917368283846924199 skills 210 222 Possess good analytical skills and problem solving capabilities along with good communication\n",
      "12917368283846924199 skills 211 222 good analytical skills and problem solving capabilities along with good communication\n",
      "12917368283846924199 skills 212 222 analytical skills and problem solving capabilities along with good communication\n",
      "12917368283846924199 skills 213 222 skills and problem solving capabilities along with good communication\n",
      "12917368283846924199 skills 210 223 Possess good analytical skills and problem solving capabilities along with good communication skills\n",
      "12917368283846924199 skills 211 223 good analytical skills and problem solving capabilities along with good communication skills\n",
      "12917368283846924199 skills 212 223 analytical skills and problem solving capabilities along with good communication skills\n",
      "12917368283846924199 skills 213 223 skills and problem solving capabilities along with good communication skills\n",
      "12917368283846924199 skills 214 223 and problem solving capabilities along with good communication skills\n",
      "12917368283846924199 skills 215 223 problem solving capabilities along with good communication skills\n",
      "12917368283846924199 skills 216 223 solving capabilities along with good communication skills\n",
      "12917368283846924199 skills 217 223 capabilities along with good communication skills\n",
      "12917368283846924199 skills 218 223 along with good communication skills\n",
      "12917368283846924199 skills 219 223 with good communication skills\n",
      "12917368283846924199 skills 220 223 good communication skills\n",
      "12917368283846924199 skills 221 223 communication skills\n",
      "12917368283846924199 skills 222 223 skills\n",
      "12917368283846924199 skills 224 226 TECHNICAL SKILLS\n",
      "12917368283846924199 skills 225 226 SKILLS\n",
      "12917368283846924199 skills 224 227 TECHNICAL SKILLS PROGRAMMING\n",
      "12917368283846924199 skills 225 227 SKILLS PROGRAMMING\n",
      "12917368283846924199 skills 224 228 TECHNICAL SKILLS PROGRAMMING LANGUAGES\n",
      "12917368283846924199 skills 225 228 SKILLS PROGRAMMING LANGUAGES\n",
      "12917368283846924199 skills 224 229 TECHNICAL SKILLS PROGRAMMING LANGUAGES Python\n",
      "12917368283846924199 skills 225 229 SKILLS PROGRAMMING LANGUAGES Python\n",
      "12917368283846924199 skills 700 701 SKILLS\n",
      "12917368283846924199 skills 700 702 SKILLS Technical\n",
      "12917368283846924199 skills 715 719 Deep Learning Soft Skills\n",
      "12917368283846924199 skills 716 719 Learning Soft Skills\n",
      "12917368283846924199 skills 717 719 Soft Skills\n",
      "12917368283846924199 skills 718 719 Skills\n",
      "12917368283846924199 skills 741 761 Data Science for juniors to introduce them with these new technologies and how they can learn and enhance their skills\n",
      "12917368283846924199 skills 742 761 Science for juniors to introduce them with these new technologies and how they can learn and enhance their skills\n",
      "12917368283846924199 skills 743 761 for juniors to introduce them with these new technologies and how they can learn and enhance their skills\n",
      "12917368283846924199 skills 744 761 juniors to introduce them with these new technologies and how they can learn and enhance their skills\n",
      "12917368283846924199 skills 745 761 to introduce them with these new technologies and how they can learn and enhance their skills\n",
      "12917368283846924199 skills 746 761 introduce them with these new technologies and how they can learn and enhance their skills\n",
      "12917368283846924199 skills 747 761 them with these new technologies and how they can learn and enhance their skills\n",
      "12917368283846924199 skills 748 761 with these new technologies and how they can learn and enhance their skills\n",
      "12917368283846924199 skills 749 761 these new technologies and how they can learn and enhance their skills\n",
      "12917368283846924199 skills 750 761 new technologies and how they can learn and enhance their skills\n",
      "12917368283846924199 skills 751 761 technologies and how they can learn and enhance their skills\n",
      "12917368283846924199 skills 752 761 and how they can learn and enhance their skills\n",
      "12917368283846924199 skills 753 761 how they can learn and enhance their skills\n",
      "12917368283846924199 skills 754 761 they can learn and enhance their skills\n",
      "12917368283846924199 skills 755 761 can learn and enhance their skills\n",
      "12917368283846924199 skills 756 761 learn and enhance their skills\n",
      "12917368283846924199 skills 757 761 and enhance their skills\n",
      "12917368283846924199 skills 758 761 enhance their skills\n",
      "12917368283846924199 skills 759 761 their skills\n",
      "12917368283846924199 skills 760 761 skills\n",
      "12917368283846924199 skills 741 762 Data Science for juniors to introduce them with these new technologies and how they can learn and enhance their skills in\n",
      "12917368283846924199 skills 742 762 Science for juniors to introduce them with these new technologies and how they can learn and enhance their skills in\n",
      "12917368283846924199 skills 743 762 for juniors to introduce them with these new technologies and how they can learn and enhance their skills in\n",
      "12917368283846924199 skills 744 762 juniors to introduce them with these new technologies and how they can learn and enhance their skills in\n",
      "12917368283846924199 skills 745 762 to introduce them with these new technologies and how they can learn and enhance their skills in\n",
      "12917368283846924199 skills 746 762 introduce them with these new technologies and how they can learn and enhance their skills in\n",
      "12917368283846924199 skills 747 762 them with these new technologies and how they can learn and enhance their skills in\n",
      "12917368283846924199 skills 748 762 with these new technologies and how they can learn and enhance their skills in\n",
      "12917368283846924199 skills 749 762 these new technologies and how they can learn and enhance their skills in\n",
      "12917368283846924199 skills 750 762 new technologies and how they can learn and enhance their skills in\n",
      "12917368283846924199 skills 751 762 technologies and how they can learn and enhance their skills in\n",
      "12917368283846924199 skills 752 762 and how they can learn and enhance their skills in\n",
      "12917368283846924199 skills 753 762 how they can learn and enhance their skills in\n",
      "12917368283846924199 skills 754 762 they can learn and enhance their skills in\n",
      "12917368283846924199 skills 755 762 can learn and enhance their skills in\n",
      "12917368283846924199 skills 756 762 learn and enhance their skills in\n",
      "12917368283846924199 skills 757 762 and enhance their skills in\n",
      "12917368283846924199 skills 758 762 enhance their skills in\n",
      "12917368283846924199 skills 759 762 their skills in\n",
      "12917368283846924199 skills 760 762 skills in\n",
      "12917368283846924199 skills 741 763 Data Science for juniors to introduce them with these new technologies and how they can learn and enhance their skills in machine\n",
      "12917368283846924199 skills 742 763 Science for juniors to introduce them with these new technologies and how they can learn and enhance their skills in machine\n",
      "12917368283846924199 skills 743 763 for juniors to introduce them with these new technologies and how they can learn and enhance their skills in machine\n",
      "12917368283846924199 skills 744 763 juniors to introduce them with these new technologies and how they can learn and enhance their skills in machine\n",
      "12917368283846924199 skills 745 763 to introduce them with these new technologies and how they can learn and enhance their skills in machine\n",
      "12917368283846924199 skills 746 763 introduce them with these new technologies and how they can learn and enhance their skills in machine\n",
      "12917368283846924199 skills 747 763 them with these new technologies and how they can learn and enhance their skills in machine\n",
      "12917368283846924199 skills 748 763 with these new technologies and how they can learn and enhance their skills in machine\n",
      "12917368283846924199 skills 749 763 these new technologies and how they can learn and enhance their skills in machine\n",
      "12917368283846924199 skills 750 763 new technologies and how they can learn and enhance their skills in machine\n",
      "12917368283846924199 skills 751 763 technologies and how they can learn and enhance their skills in machine\n",
      "12917368283846924199 skills 752 763 and how they can learn and enhance their skills in machine\n",
      "12917368283846924199 skills 753 763 how they can learn and enhance their skills in machine\n",
      "12917368283846924199 skills 754 763 they can learn and enhance their skills in machine\n",
      "12917368283846924199 skills 755 763 can learn and enhance their skills in machine\n",
      "12917368283846924199 skills 756 763 learn and enhance their skills in machine\n",
      "12917368283846924199 skills 757 763 and enhance their skills in machine\n",
      "12917368283846924199 skills 758 763 enhance their skills in machine\n",
      "12917368283846924199 skills 759 763 their skills in machine\n",
      "12917368283846924199 skills 760 763 skills in machine\n",
      "12917368283846924199 skills 741 764 Data Science for juniors to introduce them with these new technologies and how they can learn and enhance their skills in machine learning\n",
      "12917368283846924199 skills 742 764 Science for juniors to introduce them with these new technologies and how they can learn and enhance their skills in machine learning\n",
      "12917368283846924199 skills 743 764 for juniors to introduce them with these new technologies and how they can learn and enhance their skills in machine learning\n",
      "12917368283846924199 skills 744 764 juniors to introduce them with these new technologies and how they can learn and enhance their skills in machine learning\n",
      "12917368283846924199 skills 745 764 to introduce them with these new technologies and how they can learn and enhance their skills in machine learning\n",
      "12917368283846924199 skills 746 764 introduce them with these new technologies and how they can learn and enhance their skills in machine learning\n",
      "12917368283846924199 skills 747 764 them with these new technologies and how they can learn and enhance their skills in machine learning\n",
      "12917368283846924199 skills 748 764 with these new technologies and how they can learn and enhance their skills in machine learning\n",
      "12917368283846924199 skills 749 764 these new technologies and how they can learn and enhance their skills in machine learning\n",
      "12917368283846924199 skills 750 764 new technologies and how they can learn and enhance their skills in machine learning\n",
      "12917368283846924199 skills 751 764 technologies and how they can learn and enhance their skills in machine learning\n",
      "12917368283846924199 skills 752 764 and how they can learn and enhance their skills in machine learning\n",
      "12917368283846924199 skills 753 764 how they can learn and enhance their skills in machine learning\n",
      "12917368283846924199 skills 754 764 they can learn and enhance their skills in machine learning\n",
      "12917368283846924199 skills 755 764 can learn and enhance their skills in machine learning\n",
      "12917368283846924199 skills 756 764 learn and enhance their skills in machine learning\n",
      "12917368283846924199 skills 757 764 and enhance their skills in machine learning\n",
      "12917368283846924199 skills 758 764 enhance their skills in machine learning\n",
      "12917368283846924199 skills 759 764 their skills in machine learning\n",
      "12917368283846924199 skills 760 764 skills in machine learning\n",
      "12917368283846924199 skills 19 41 PROFILE Organized and ambitious budding professional with experience in data science and business analysis with a thirst for learning top notch skills\n",
      "12917368283846924199 skills 20 41 Organized and ambitious budding professional with experience in data science and business analysis with a thirst for learning top notch skills\n",
      "12917368283846924199 skills 21 41 and ambitious budding professional with experience in data science and business analysis with a thirst for learning top notch skills\n",
      "12917368283846924199 skills 22 41 ambitious budding professional with experience in data science and business analysis with a thirst for learning top notch skills\n",
      "12917368283846924199 skills 23 41 budding professional with experience in data science and business analysis with a thirst for learning top notch skills\n",
      "12917368283846924199 skills 24 41 professional with experience in data science and business analysis with a thirst for learning top notch skills\n",
      "12917368283846924199 skills 25 41 with experience in data science and business analysis with a thirst for learning top notch skills\n",
      "12917368283846924199 skills 26 41 experience in data science and business analysis with a thirst for learning top notch skills\n",
      "12917368283846924199 skills 27 41 in data science and business analysis with a thirst for learning top notch skills\n",
      "12917368283846924199 skills 28 41 data science and business analysis with a thirst for learning top notch skills\n",
      "12917368283846924199 skills 29 41 science and business analysis with a thirst for learning top notch skills\n",
      "12917368283846924199 skills 30 41 and business analysis with a thirst for learning top notch skills\n",
      "12917368283846924199 skills 31 41 business analysis with a thirst for learning top notch skills\n",
      "12917368283846924199 skills 32 41 analysis with a thirst for learning top notch skills\n",
      "12917368283846924199 skills 33 41 with a thirst for learning top notch skills\n",
      "12917368283846924199 skills 34 41 a thirst for learning top notch skills\n",
      "12917368283846924199 skills 35 41 thirst for learning top notch skills\n",
      "12917368283846924199 skills 36 41 for learning top notch skills\n",
      "12917368283846924199 skills 37 41 learning top notch skills\n",
      "12917368283846924199 skills 38 41 top notch skills\n",
      "12917368283846924199 skills 39 41 notch skills\n",
      "12917368283846924199 skills 40 41 skills\n",
      "12917368283846924199 skills 47 54 sharing new ideas and leveraging my skills\n",
      "12917368283846924199 skills 48 54 new ideas and leveraging my skills\n",
      "12917368283846924199 skills 49 54 ideas and leveraging my skills\n",
      "12917368283846924199 skills 50 54 and leveraging my skills\n",
      "12917368283846924199 skills 51 54 leveraging my skills\n",
      "12917368283846924199 skills 52 54 my skills\n",
      "12917368283846924199 skills 53 54 skills\n",
      "12917368283846924199 skills 47 55 sharing new ideas and leveraging my skills for\n",
      "12917368283846924199 skills 48 55 new ideas and leveraging my skills for\n",
      "12917368283846924199 skills 49 55 ideas and leveraging my skills for\n",
      "12917368283846924199 skills 50 55 and leveraging my skills for\n",
      "12917368283846924199 skills 51 55 leveraging my skills for\n",
      "12917368283846924199 skills 52 55 my skills for\n",
      "12917368283846924199 skills 53 55 skills for\n",
      "12917368283846924199 skills 47 56 sharing new ideas and leveraging my skills for the\n",
      "12917368283846924199 skills 48 56 new ideas and leveraging my skills for the\n",
      "12917368283846924199 skills 49 56 ideas and leveraging my skills for the\n",
      "12917368283846924199 skills 50 56 and leveraging my skills for the\n",
      "12917368283846924199 skills 51 56 leveraging my skills for the\n",
      "12917368283846924199 skills 52 56 my skills for the\n",
      "12917368283846924199 skills 53 56 skills for the\n",
      "12917368283846924199 skills 47 57 sharing new ideas and leveraging my skills for the growth\n",
      "12917368283846924199 skills 48 57 new ideas and leveraging my skills for the growth\n",
      "12917368283846924199 skills 49 57 ideas and leveraging my skills for the growth\n",
      "12917368283846924199 skills 50 57 and leveraging my skills for the growth\n",
      "12917368283846924199 skills 51 57 leveraging my skills for the growth\n",
      "12917368283846924199 skills 52 57 my skills for the growth\n",
      "12917368283846924199 skills 53 57 skills for the growth\n",
      "12917368283846924199 skills 47 58 sharing new ideas and leveraging my skills for the growth of\n",
      "12917368283846924199 skills 48 58 new ideas and leveraging my skills for the growth of\n",
      "12917368283846924199 skills 49 58 ideas and leveraging my skills for the growth of\n",
      "12917368283846924199 skills 50 58 and leveraging my skills for the growth of\n",
      "12917368283846924199 skills 51 58 leveraging my skills for the growth of\n",
      "12917368283846924199 skills 52 58 my skills for the growth of\n",
      "12917368283846924199 skills 53 58 skills for the growth of\n",
      "12917368283846924199 skills 47 59 sharing new ideas and leveraging my skills for the growth of the\n",
      "12917368283846924199 skills 48 59 new ideas and leveraging my skills for the growth of the\n",
      "12917368283846924199 skills 49 59 ideas and leveraging my skills for the growth of the\n",
      "12917368283846924199 skills 50 59 and leveraging my skills for the growth of the\n",
      "12917368283846924199 skills 51 59 leveraging my skills for the growth of the\n",
      "12917368283846924199 skills 52 59 my skills for the growth of the\n",
      "12917368283846924199 skills 53 59 skills for the growth of the\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12917368283846924199 skills 47 60 sharing new ideas and leveraging my skills for the growth of the company\n",
      "12917368283846924199 skills 48 60 new ideas and leveraging my skills for the growth of the company\n",
      "12917368283846924199 skills 49 60 ideas and leveraging my skills for the growth of the company\n",
      "12917368283846924199 skills 50 60 and leveraging my skills for the growth of the company\n",
      "12917368283846924199 skills 51 60 leveraging my skills for the growth of the company\n",
      "12917368283846924199 skills 52 60 my skills for the growth of the company\n",
      "12917368283846924199 skills 53 60 skills for the growth of the company\n",
      "12917368283846924199 skills 47 62 sharing new ideas and leveraging my skills for the growth of the company. Skills\n",
      "12917368283846924199 skills 48 62 new ideas and leveraging my skills for the growth of the company. Skills\n",
      "12917368283846924199 skills 49 62 ideas and leveraging my skills for the growth of the company. Skills\n",
      "12917368283846924199 skills 50 62 and leveraging my skills for the growth of the company. Skills\n",
      "12917368283846924199 skills 51 62 leveraging my skills for the growth of the company. Skills\n",
      "12917368283846924199 skills 52 62 my skills for the growth of the company. Skills\n",
      "12917368283846924199 skills 53 62 skills for the growth of the company. Skills\n",
      "12917368283846924199 skills 54 62 for the growth of the company. Skills\n",
      "12917368283846924199 skills 55 62 the growth of the company. Skills\n",
      "12917368283846924199 skills 56 62 growth of the company. Skills\n",
      "12917368283846924199 skills 57 62 of the company. Skills\n",
      "12917368283846924199 skills 58 62 the company. Skills\n",
      "12917368283846924199 skills 59 62 company. Skills\n",
      "12917368283846924199 skills 60 62 . Skills\n",
      "12917368283846924199 skills 61 62 Skills\n",
      "12917368283846924199 skills 47 63 sharing new ideas and leveraging my skills for the growth of the company. Skills Summary\n",
      "12917368283846924199 skills 48 63 new ideas and leveraging my skills for the growth of the company. Skills Summary\n",
      "12917368283846924199 skills 49 63 ideas and leveraging my skills for the growth of the company. Skills Summary\n",
      "12917368283846924199 skills 50 63 and leveraging my skills for the growth of the company. Skills Summary\n",
      "12917368283846924199 skills 51 63 leveraging my skills for the growth of the company. Skills Summary\n",
      "12917368283846924199 skills 52 63 my skills for the growth of the company. Skills Summary\n",
      "12917368283846924199 skills 53 63 skills for the growth of the company. Skills Summary\n",
      "12917368283846924199 skills 54 63 for the growth of the company. Skills Summary\n",
      "12917368283846924199 skills 55 63 the growth of the company. Skills Summary\n",
      "12917368283846924199 skills 56 63 growth of the company. Skills Summary\n",
      "12917368283846924199 skills 57 63 of the company. Skills Summary\n",
      "12917368283846924199 skills 58 63 the company. Skills Summary\n",
      "12917368283846924199 skills 59 63 company. Skills Summary\n",
      "12917368283846924199 skills 60 63 . Skills Summary\n",
      "12917368283846924199 skills 61 63 Skills Summary\n",
      "12917368283846924199 skills 47 64 sharing new ideas and leveraging my skills for the growth of the company. Skills Summary Data\n",
      "12917368283846924199 skills 48 64 new ideas and leveraging my skills for the growth of the company. Skills Summary Data\n",
      "12917368283846924199 skills 49 64 ideas and leveraging my skills for the growth of the company. Skills Summary Data\n",
      "12917368283846924199 skills 50 64 and leveraging my skills for the growth of the company. Skills Summary Data\n",
      "12917368283846924199 skills 51 64 leveraging my skills for the growth of the company. Skills Summary Data\n",
      "12917368283846924199 skills 52 64 my skills for the growth of the company. Skills Summary Data\n",
      "12917368283846924199 skills 53 64 skills for the growth of the company. Skills Summary Data\n",
      "12917368283846924199 skills 54 64 for the growth of the company. Skills Summary Data\n",
      "12917368283846924199 skills 55 64 the growth of the company. Skills Summary Data\n",
      "12917368283846924199 skills 56 64 growth of the company. Skills Summary Data\n",
      "12917368283846924199 skills 57 64 of the company. Skills Summary Data\n",
      "12917368283846924199 skills 58 64 the company. Skills Summary Data\n",
      "12917368283846924199 skills 59 64 company. Skills Summary Data\n",
      "12917368283846924199 skills 60 64 . Skills Summary Data\n",
      "12917368283846924199 skills 61 64 Skills Summary Data\n",
      "12917368283846924199 skills 47 65 sharing new ideas and leveraging my skills for the growth of the company. Skills Summary Data Science\n",
      "12917368283846924199 skills 48 65 new ideas and leveraging my skills for the growth of the company. Skills Summary Data Science\n",
      "12917368283846924199 skills 49 65 ideas and leveraging my skills for the growth of the company. Skills Summary Data Science\n",
      "12917368283846924199 skills 50 65 and leveraging my skills for the growth of the company. Skills Summary Data Science\n",
      "12917368283846924199 skills 51 65 leveraging my skills for the growth of the company. Skills Summary Data Science\n",
      "12917368283846924199 skills 52 65 my skills for the growth of the company. Skills Summary Data Science\n",
      "12917368283846924199 skills 53 65 skills for the growth of the company. Skills Summary Data Science\n",
      "12917368283846924199 skills 54 65 for the growth of the company. Skills Summary Data Science\n",
      "12917368283846924199 skills 55 65 the growth of the company. Skills Summary Data Science\n",
      "12917368283846924199 skills 56 65 growth of the company. Skills Summary Data Science\n",
      "12917368283846924199 skills 57 65 of the company. Skills Summary Data Science\n",
      "12917368283846924199 skills 58 65 the company. Skills Summary Data Science\n",
      "12917368283846924199 skills 59 65 company. Skills Summary Data Science\n",
      "12917368283846924199 skills 60 65 . Skills Summary Data Science\n",
      "12917368283846924199 skills 61 65 Skills Summary Data Science\n",
      "12917368283846924199 skills 47 66 sharing new ideas and leveraging my skills for the growth of the company. Skills Summary Data Science Machine\n",
      "12917368283846924199 skills 48 66 new ideas and leveraging my skills for the growth of the company. Skills Summary Data Science Machine\n",
      "12917368283846924199 skills 49 66 ideas and leveraging my skills for the growth of the company. Skills Summary Data Science Machine\n",
      "12917368283846924199 skills 50 66 and leveraging my skills for the growth of the company. Skills Summary Data Science Machine\n",
      "12917368283846924199 skills 51 66 leveraging my skills for the growth of the company. Skills Summary Data Science Machine\n",
      "12917368283846924199 skills 52 66 my skills for the growth of the company. Skills Summary Data Science Machine\n",
      "12917368283846924199 skills 53 66 skills for the growth of the company. Skills Summary Data Science Machine\n",
      "12917368283846924199 skills 54 66 for the growth of the company. Skills Summary Data Science Machine\n",
      "12917368283846924199 skills 55 66 the growth of the company. Skills Summary Data Science Machine\n",
      "12917368283846924199 skills 56 66 growth of the company. Skills Summary Data Science Machine\n",
      "12917368283846924199 skills 57 66 of the company. Skills Summary Data Science Machine\n",
      "12917368283846924199 skills 58 66 the company. Skills Summary Data Science Machine\n",
      "12917368283846924199 skills 59 66 company. Skills Summary Data Science Machine\n",
      "12917368283846924199 skills 60 66 . Skills Summary Data Science Machine\n",
      "12917368283846924199 skills 61 66 Skills Summary Data Science Machine\n",
      "12917368283846924199 skills 47 67 sharing new ideas and leveraging my skills for the growth of the company. Skills Summary Data Science Machine Learning\n",
      "12917368283846924199 skills 48 67 new ideas and leveraging my skills for the growth of the company. Skills Summary Data Science Machine Learning\n",
      "12917368283846924199 skills 49 67 ideas and leveraging my skills for the growth of the company. Skills Summary Data Science Machine Learning\n",
      "12917368283846924199 skills 50 67 and leveraging my skills for the growth of the company. Skills Summary Data Science Machine Learning\n",
      "12917368283846924199 skills 51 67 leveraging my skills for the growth of the company. Skills Summary Data Science Machine Learning\n",
      "12917368283846924199 skills 52 67 my skills for the growth of the company. Skills Summary Data Science Machine Learning\n",
      "12917368283846924199 skills 53 67 skills for the growth of the company. Skills Summary Data Science Machine Learning\n",
      "12917368283846924199 skills 54 67 for the growth of the company. Skills Summary Data Science Machine Learning\n",
      "12917368283846924199 skills 55 67 the growth of the company. Skills Summary Data Science Machine Learning\n",
      "12917368283846924199 skills 56 67 growth of the company. Skills Summary Data Science Machine Learning\n",
      "12917368283846924199 skills 57 67 of the company. Skills Summary Data Science Machine Learning\n",
      "12917368283846924199 skills 58 67 the company. Skills Summary Data Science Machine Learning\n",
      "12917368283846924199 skills 59 67 company. Skills Summary Data Science Machine Learning\n",
      "12917368283846924199 skills 60 67 . Skills Summary Data Science Machine Learning\n",
      "12917368283846924199 skills 61 67 Skills Summary Data Science Machine Learning\n",
      "12917368283846924199 skills 396 420 random forest and XGBoost to predict the bad loan applications Performed data visualization for analysis and feature engineering to generate new features KEY SKILLS\n",
      "12917368283846924199 skills 397 420 forest and XGBoost to predict the bad loan applications Performed data visualization for analysis and feature engineering to generate new features KEY SKILLS\n",
      "12917368283846924199 skills 398 420 and XGBoost to predict the bad loan applications Performed data visualization for analysis and feature engineering to generate new features KEY SKILLS\n",
      "12917368283846924199 skills 399 420 XGBoost to predict the bad loan applications Performed data visualization for analysis and feature engineering to generate new features KEY SKILLS\n",
      "12917368283846924199 skills 400 420 to predict the bad loan applications Performed data visualization for analysis and feature engineering to generate new features KEY SKILLS\n",
      "12917368283846924199 skills 401 420 predict the bad loan applications Performed data visualization for analysis and feature engineering to generate new features KEY SKILLS\n",
      "12917368283846924199 skills 402 420 the bad loan applications Performed data visualization for analysis and feature engineering to generate new features KEY SKILLS\n",
      "12917368283846924199 skills 403 420 bad loan applications Performed data visualization for analysis and feature engineering to generate new features KEY SKILLS\n",
      "12917368283846924199 skills 404 420 loan applications Performed data visualization for analysis and feature engineering to generate new features KEY SKILLS\n",
      "12917368283846924199 skills 405 420 applications Performed data visualization for analysis and feature engineering to generate new features KEY SKILLS\n",
      "12917368283846924199 skills 406 420 Performed data visualization for analysis and feature engineering to generate new features KEY SKILLS\n",
      "12917368283846924199 skills 407 420 data visualization for analysis and feature engineering to generate new features KEY SKILLS\n",
      "12917368283846924199 skills 408 420 visualization for analysis and feature engineering to generate new features KEY SKILLS\n",
      "12917368283846924199 skills 409 420 for analysis and feature engineering to generate new features KEY SKILLS\n",
      "12917368283846924199 skills 410 420 analysis and feature engineering to generate new features KEY SKILLS\n",
      "12917368283846924199 skills 411 420 and feature engineering to generate new features KEY SKILLS\n",
      "12917368283846924199 skills 412 420 feature engineering to generate new features KEY SKILLS\n",
      "12917368283846924199 skills 413 420 engineering to generate new features KEY SKILLS\n",
      "12917368283846924199 skills 414 420 to generate new features KEY SKILLS\n",
      "12917368283846924199 skills 415 420 generate new features KEY SKILLS\n",
      "12917368283846924199 skills 416 420 new features KEY SKILLS\n",
      "12917368283846924199 skills 417 420 features KEY SKILLS\n",
      "12917368283846924199 skills 418 420 KEY SKILLS\n",
      "12917368283846924199 skills 419 420 SKILLS\n",
      "12917368283846924199 skills 396 421 random forest and XGBoost to predict the bad loan applications Performed data visualization for analysis and feature engineering to generate new features KEY SKILLS AND\n",
      "12917368283846924199 skills 397 421 forest and XGBoost to predict the bad loan applications Performed data visualization for analysis and feature engineering to generate new features KEY SKILLS AND\n",
      "12917368283846924199 skills 398 421 and XGBoost to predict the bad loan applications Performed data visualization for analysis and feature engineering to generate new features KEY SKILLS AND\n",
      "12917368283846924199 skills 399 421 XGBoost to predict the bad loan applications Performed data visualization for analysis and feature engineering to generate new features KEY SKILLS AND\n",
      "12917368283846924199 skills 400 421 to predict the bad loan applications Performed data visualization for analysis and feature engineering to generate new features KEY SKILLS AND\n",
      "12917368283846924199 skills 401 421 predict the bad loan applications Performed data visualization for analysis and feature engineering to generate new features KEY SKILLS AND\n",
      "12917368283846924199 skills 402 421 the bad loan applications Performed data visualization for analysis and feature engineering to generate new features KEY SKILLS AND\n",
      "12917368283846924199 skills 403 421 bad loan applications Performed data visualization for analysis and feature engineering to generate new features KEY SKILLS AND\n",
      "12917368283846924199 skills 404 421 loan applications Performed data visualization for analysis and feature engineering to generate new features KEY SKILLS AND\n",
      "12917368283846924199 skills 405 421 applications Performed data visualization for analysis and feature engineering to generate new features KEY SKILLS AND\n",
      "12917368283846924199 skills 406 421 Performed data visualization for analysis and feature engineering to generate new features KEY SKILLS AND\n",
      "12917368283846924199 skills 407 421 data visualization for analysis and feature engineering to generate new features KEY SKILLS AND\n",
      "12917368283846924199 skills 408 421 visualization for analysis and feature engineering to generate new features KEY SKILLS AND\n",
      "12917368283846924199 skills 409 421 for analysis and feature engineering to generate new features KEY SKILLS AND\n",
      "12917368283846924199 skills 410 421 analysis and feature engineering to generate new features KEY SKILLS AND\n",
      "12917368283846924199 skills 411 421 and feature engineering to generate new features KEY SKILLS AND\n",
      "12917368283846924199 skills 412 421 feature engineering to generate new features KEY SKILLS AND\n",
      "12917368283846924199 skills 413 421 engineering to generate new features KEY SKILLS AND\n",
      "12917368283846924199 skills 414 421 to generate new features KEY SKILLS AND\n",
      "12917368283846924199 skills 415 421 generate new features KEY SKILLS AND\n",
      "12917368283846924199 skills 416 421 new features KEY SKILLS AND\n",
      "12917368283846924199 skills 417 421 features KEY SKILLS AND\n",
      "12917368283846924199 skills 418 421 KEY SKILLS AND\n",
      "12917368283846924199 skills 419 421 SKILLS AND\n",
      "12917368283846924199 skills 396 422 random forest and XGBoost to predict the bad loan applications Performed data visualization for analysis and feature engineering to generate new features KEY SKILLS AND CHARACTERISTICS\n",
      "12917368283846924199 skills 397 422 forest and XGBoost to predict the bad loan applications Performed data visualization for analysis and feature engineering to generate new features KEY SKILLS AND CHARACTERISTICS\n",
      "12917368283846924199 skills 398 422 and XGBoost to predict the bad loan applications Performed data visualization for analysis and feature engineering to generate new features KEY SKILLS AND CHARACTERISTICS\n",
      "12917368283846924199 skills 399 422 XGBoost to predict the bad loan applications Performed data visualization for analysis and feature engineering to generate new features KEY SKILLS AND CHARACTERISTICS\n",
      "12917368283846924199 skills 400 422 to predict the bad loan applications Performed data visualization for analysis and feature engineering to generate new features KEY SKILLS AND CHARACTERISTICS\n",
      "12917368283846924199 skills 401 422 predict the bad loan applications Performed data visualization for analysis and feature engineering to generate new features KEY SKILLS AND CHARACTERISTICS\n",
      "12917368283846924199 skills 402 422 the bad loan applications Performed data visualization for analysis and feature engineering to generate new features KEY SKILLS AND CHARACTERISTICS\n",
      "12917368283846924199 skills 403 422 bad loan applications Performed data visualization for analysis and feature engineering to generate new features KEY SKILLS AND CHARACTERISTICS\n",
      "12917368283846924199 skills 404 422 loan applications Performed data visualization for analysis and feature engineering to generate new features KEY SKILLS AND CHARACTERISTICS\n",
      "12917368283846924199 skills 405 422 applications Performed data visualization for analysis and feature engineering to generate new features KEY SKILLS AND CHARACTERISTICS\n",
      "12917368283846924199 skills 406 422 Performed data visualization for analysis and feature engineering to generate new features KEY SKILLS AND CHARACTERISTICS\n",
      "12917368283846924199 skills 407 422 data visualization for analysis and feature engineering to generate new features KEY SKILLS AND CHARACTERISTICS\n",
      "12917368283846924199 skills 408 422 visualization for analysis and feature engineering to generate new features KEY SKILLS AND CHARACTERISTICS\n",
      "12917368283846924199 skills 409 422 for analysis and feature engineering to generate new features KEY SKILLS AND CHARACTERISTICS\n",
      "12917368283846924199 skills 410 422 analysis and feature engineering to generate new features KEY SKILLS AND CHARACTERISTICS\n",
      "12917368283846924199 skills 411 422 and feature engineering to generate new features KEY SKILLS AND CHARACTERISTICS\n",
      "12917368283846924199 skills 412 422 feature engineering to generate new features KEY SKILLS AND CHARACTERISTICS\n",
      "12917368283846924199 skills 413 422 engineering to generate new features KEY SKILLS AND CHARACTERISTICS\n",
      "12917368283846924199 skills 414 422 to generate new features KEY SKILLS AND CHARACTERISTICS\n",
      "12917368283846924199 skills 415 422 generate new features KEY SKILLS AND CHARACTERISTICS\n",
      "12917368283846924199 skills 416 422 new features KEY SKILLS AND CHARACTERISTICS\n",
      "12917368283846924199 skills 417 422 features KEY SKILLS AND CHARACTERISTICS\n",
      "12917368283846924199 skills 418 422 KEY SKILLS AND CHARACTERISTICS\n",
      "12917368283846924199 skills 419 422 SKILLS AND CHARACTERISTICS\n",
      "12917368283846924199 skills 396 423 random forest and XGBoost to predict the bad loan applications Performed data visualization for analysis and feature engineering to generate new features KEY SKILLS AND CHARACTERISTICS Cornerstone\n",
      "12917368283846924199 skills 397 423 forest and XGBoost to predict the bad loan applications Performed data visualization for analysis and feature engineering to generate new features KEY SKILLS AND CHARACTERISTICS Cornerstone\n",
      "12917368283846924199 skills 398 423 and XGBoost to predict the bad loan applications Performed data visualization for analysis and feature engineering to generate new features KEY SKILLS AND CHARACTERISTICS Cornerstone\n",
      "12917368283846924199 skills 399 423 XGBoost to predict the bad loan applications Performed data visualization for analysis and feature engineering to generate new features KEY SKILLS AND CHARACTERISTICS Cornerstone\n",
      "12917368283846924199 skills 400 423 to predict the bad loan applications Performed data visualization for analysis and feature engineering to generate new features KEY SKILLS AND CHARACTERISTICS Cornerstone\n",
      "12917368283846924199 skills 401 423 predict the bad loan applications Performed data visualization for analysis and feature engineering to generate new features KEY SKILLS AND CHARACTERISTICS Cornerstone\n",
      "12917368283846924199 skills 402 423 the bad loan applications Performed data visualization for analysis and feature engineering to generate new features KEY SKILLS AND CHARACTERISTICS Cornerstone\n",
      "12917368283846924199 skills 403 423 bad loan applications Performed data visualization for analysis and feature engineering to generate new features KEY SKILLS AND CHARACTERISTICS Cornerstone\n",
      "12917368283846924199 skills 404 423 loan applications Performed data visualization for analysis and feature engineering to generate new features KEY SKILLS AND CHARACTERISTICS Cornerstone\n",
      "12917368283846924199 skills 405 423 applications Performed data visualization for analysis and feature engineering to generate new features KEY SKILLS AND CHARACTERISTICS Cornerstone\n",
      "12917368283846924199 skills 406 423 Performed data visualization for analysis and feature engineering to generate new features KEY SKILLS AND CHARACTERISTICS Cornerstone\n",
      "12917368283846924199 skills 407 423 data visualization for analysis and feature engineering to generate new features KEY SKILLS AND CHARACTERISTICS Cornerstone\n",
      "12917368283846924199 skills 408 423 visualization for analysis and feature engineering to generate new features KEY SKILLS AND CHARACTERISTICS Cornerstone\n",
      "12917368283846924199 skills 409 423 for analysis and feature engineering to generate new features KEY SKILLS AND CHARACTERISTICS Cornerstone\n",
      "12917368283846924199 skills 410 423 analysis and feature engineering to generate new features KEY SKILLS AND CHARACTERISTICS Cornerstone\n",
      "12917368283846924199 skills 411 423 and feature engineering to generate new features KEY SKILLS AND CHARACTERISTICS Cornerstone\n",
      "12917368283846924199 skills 412 423 feature engineering to generate new features KEY SKILLS AND CHARACTERISTICS Cornerstone\n",
      "12917368283846924199 skills 413 423 engineering to generate new features KEY SKILLS AND CHARACTERISTICS Cornerstone\n",
      "12917368283846924199 skills 414 423 to generate new features KEY SKILLS AND CHARACTERISTICS Cornerstone\n",
      "12917368283846924199 skills 415 423 generate new features KEY SKILLS AND CHARACTERISTICS Cornerstone\n",
      "12917368283846924199 skills 416 423 new features KEY SKILLS AND CHARACTERISTICS Cornerstone\n",
      "12917368283846924199 skills 417 423 features KEY SKILLS AND CHARACTERISTICS Cornerstone\n",
      "12917368283846924199 skills 418 423 KEY SKILLS AND CHARACTERISTICS Cornerstone\n",
      "12917368283846924199 skills 419 423 SKILLS AND CHARACTERISTICS Cornerstone\n",
      "12917368283846924199 skills 38 55 Data Science at Department of Applied Mathematics and Computational Sciences at PSG College of Technology. SKILL\n",
      "12917368283846924199 skills 39 55 Science at Department of Applied Mathematics and Computational Sciences at PSG College of Technology. SKILL\n",
      "12917368283846924199 skills 40 55 at Department of Applied Mathematics and Computational Sciences at PSG College of Technology. SKILL\n",
      "12917368283846924199 skills 41 55 Department of Applied Mathematics and Computational Sciences at PSG College of Technology. SKILL\n",
      "12917368283846924199 skills 42 55 of Applied Mathematics and Computational Sciences at PSG College of Technology. SKILL\n",
      "12917368283846924199 skills 43 55 Applied Mathematics and Computational Sciences at PSG College of Technology. SKILL\n",
      "12917368283846924199 skills 44 55 Mathematics and Computational Sciences at PSG College of Technology. SKILL\n",
      "12917368283846924199 skills 45 55 and Computational Sciences at PSG College of Technology. SKILL\n",
      "12917368283846924199 skills 46 55 Computational Sciences at PSG College of Technology. SKILL\n",
      "12917368283846924199 skills 47 55 Sciences at PSG College of Technology. SKILL\n",
      "12917368283846924199 skills 48 55 at PSG College of Technology. SKILL\n",
      "12917368283846924199 skills 49 55 PSG College of Technology. SKILL\n",
      "12917368283846924199 skills 50 55 College of Technology. SKILL\n",
      "12917368283846924199 skills 51 55 of Technology. SKILL\n",
      "12917368283846924199 skills 52 55 Technology. SKILL\n",
      "12917368283846924199 skills 53 55 . SKILL\n",
      "12917368283846924199 skills 54 55 SKILL\n",
      "12917368283846924199 skills 38 56 Data Science at Department of Applied Mathematics and Computational Sciences at PSG College of Technology. SKILL SET\n",
      "12917368283846924199 skills 39 56 Science at Department of Applied Mathematics and Computational Sciences at PSG College of Technology. SKILL SET\n",
      "12917368283846924199 skills 40 56 at Department of Applied Mathematics and Computational Sciences at PSG College of Technology. SKILL SET\n",
      "12917368283846924199 skills 41 56 Department of Applied Mathematics and Computational Sciences at PSG College of Technology. SKILL SET\n",
      "12917368283846924199 skills 42 56 of Applied Mathematics and Computational Sciences at PSG College of Technology. SKILL SET\n",
      "12917368283846924199 skills 43 56 Applied Mathematics and Computational Sciences at PSG College of Technology. SKILL SET\n",
      "12917368283846924199 skills 44 56 Mathematics and Computational Sciences at PSG College of Technology. SKILL SET\n",
      "12917368283846924199 skills 45 56 and Computational Sciences at PSG College of Technology. SKILL SET\n",
      "12917368283846924199 skills 46 56 Computational Sciences at PSG College of Technology. SKILL SET\n",
      "12917368283846924199 skills 47 56 Sciences at PSG College of Technology. SKILL SET\n",
      "12917368283846924199 skills 48 56 at PSG College of Technology. SKILL SET\n",
      "12917368283846924199 skills 49 56 PSG College of Technology. SKILL SET\n",
      "12917368283846924199 skills 50 56 College of Technology. SKILL SET\n",
      "12917368283846924199 skills 51 56 of Technology. SKILL SET\n",
      "12917368283846924199 skills 52 56 Technology. SKILL SET\n",
      "12917368283846924199 skills 53 56 . SKILL SET\n",
      "12917368283846924199 skills 54 56 SKILL SET\n",
      "12917368283846924199 skills 38 57 Data Science at Department of Applied Mathematics and Computational Sciences at PSG College of Technology. SKILL SET AREAS\n",
      "12917368283846924199 skills 39 57 Science at Department of Applied Mathematics and Computational Sciences at PSG College of Technology. SKILL SET AREAS\n",
      "12917368283846924199 skills 40 57 at Department of Applied Mathematics and Computational Sciences at PSG College of Technology. SKILL SET AREAS\n",
      "12917368283846924199 skills 41 57 Department of Applied Mathematics and Computational Sciences at PSG College of Technology. SKILL SET AREAS\n",
      "12917368283846924199 skills 42 57 of Applied Mathematics and Computational Sciences at PSG College of Technology. SKILL SET AREAS\n",
      "12917368283846924199 skills 43 57 Applied Mathematics and Computational Sciences at PSG College of Technology. SKILL SET AREAS\n",
      "12917368283846924199 skills 44 57 Mathematics and Computational Sciences at PSG College of Technology. SKILL SET AREAS\n",
      "12917368283846924199 skills 45 57 and Computational Sciences at PSG College of Technology. SKILL SET AREAS\n",
      "12917368283846924199 skills 46 57 Computational Sciences at PSG College of Technology. SKILL SET AREAS\n",
      "12917368283846924199 skills 47 57 Sciences at PSG College of Technology. SKILL SET AREAS\n",
      "12917368283846924199 skills 48 57 at PSG College of Technology. SKILL SET AREAS\n",
      "12917368283846924199 skills 49 57 PSG College of Technology. SKILL SET AREAS\n",
      "12917368283846924199 skills 50 57 College of Technology. SKILL SET AREAS\n",
      "12917368283846924199 skills 51 57 of Technology. SKILL SET AREAS\n",
      "12917368283846924199 skills 52 57 Technology. SKILL SET AREAS\n",
      "12917368283846924199 skills 53 57 . SKILL SET AREAS\n",
      "12917368283846924199 skills 54 57 SKILL SET AREAS\n",
      "12917368283846924199 skills 38 58 Data Science at Department of Applied Mathematics and Computational Sciences at PSG College of Technology. SKILL SET AREAS OF\n",
      "12917368283846924199 skills 39 58 Science at Department of Applied Mathematics and Computational Sciences at PSG College of Technology. SKILL SET AREAS OF\n",
      "12917368283846924199 skills 40 58 at Department of Applied Mathematics and Computational Sciences at PSG College of Technology. SKILL SET AREAS OF\n",
      "12917368283846924199 skills 41 58 Department of Applied Mathematics and Computational Sciences at PSG College of Technology. SKILL SET AREAS OF\n",
      "12917368283846924199 skills 42 58 of Applied Mathematics and Computational Sciences at PSG College of Technology. SKILL SET AREAS OF\n",
      "12917368283846924199 skills 43 58 Applied Mathematics and Computational Sciences at PSG College of Technology. SKILL SET AREAS OF\n",
      "12917368283846924199 skills 44 58 Mathematics and Computational Sciences at PSG College of Technology. SKILL SET AREAS OF\n",
      "12917368283846924199 skills 45 58 and Computational Sciences at PSG College of Technology. SKILL SET AREAS OF\n",
      "12917368283846924199 skills 46 58 Computational Sciences at PSG College of Technology. SKILL SET AREAS OF\n",
      "12917368283846924199 skills 47 58 Sciences at PSG College of Technology. SKILL SET AREAS OF\n",
      "12917368283846924199 skills 48 58 at PSG College of Technology. SKILL SET AREAS OF\n",
      "12917368283846924199 skills 49 58 PSG College of Technology. SKILL SET AREAS OF\n",
      "12917368283846924199 skills 50 58 College of Technology. SKILL SET AREAS OF\n",
      "12917368283846924199 skills 51 58 of Technology. SKILL SET AREAS OF\n",
      "12917368283846924199 skills 52 58 Technology. SKILL SET AREAS OF\n",
      "12917368283846924199 skills 53 58 . SKILL SET AREAS OF\n",
      "12917368283846924199 skills 54 58 SKILL SET AREAS OF\n",
      "12917368283846924199 skills 38 59 Data Science at Department of Applied Mathematics and Computational Sciences at PSG College of Technology. SKILL SET AREAS OF INTEREST\n",
      "12917368283846924199 skills 39 59 Science at Department of Applied Mathematics and Computational Sciences at PSG College of Technology. SKILL SET AREAS OF INTEREST\n",
      "12917368283846924199 skills 40 59 at Department of Applied Mathematics and Computational Sciences at PSG College of Technology. SKILL SET AREAS OF INTEREST\n",
      "12917368283846924199 skills 41 59 Department of Applied Mathematics and Computational Sciences at PSG College of Technology. SKILL SET AREAS OF INTEREST\n",
      "12917368283846924199 skills 42 59 of Applied Mathematics and Computational Sciences at PSG College of Technology. SKILL SET AREAS OF INTEREST\n",
      "12917368283846924199 skills 43 59 Applied Mathematics and Computational Sciences at PSG College of Technology. SKILL SET AREAS OF INTEREST\n",
      "12917368283846924199 skills 44 59 Mathematics and Computational Sciences at PSG College of Technology. SKILL SET AREAS OF INTEREST\n",
      "12917368283846924199 skills 45 59 and Computational Sciences at PSG College of Technology. SKILL SET AREAS OF INTEREST\n",
      "12917368283846924199 skills 46 59 Computational Sciences at PSG College of Technology. SKILL SET AREAS OF INTEREST\n",
      "12917368283846924199 skills 47 59 Sciences at PSG College of Technology. SKILL SET AREAS OF INTEREST\n",
      "12917368283846924199 skills 48 59 at PSG College of Technology. SKILL SET AREAS OF INTEREST\n",
      "12917368283846924199 skills 49 59 PSG College of Technology. SKILL SET AREAS OF INTEREST\n",
      "12917368283846924199 skills 50 59 College of Technology. SKILL SET AREAS OF INTEREST\n",
      "12917368283846924199 skills 51 59 of Technology. SKILL SET AREAS OF INTEREST\n",
      "12917368283846924199 skills 52 59 Technology. SKILL SET AREAS OF INTEREST\n",
      "12917368283846924199 skills 53 59 . SKILL SET AREAS OF INTEREST\n",
      "12917368283846924199 skills 54 59 SKILL SET AREAS OF INTEREST\n",
      "12917368283846924199 skills 38 60 Data Science at Department of Applied Mathematics and Computational Sciences at PSG College of Technology. SKILL SET AREAS OF INTEREST ACADEMIC\n",
      "12917368283846924199 skills 39 60 Science at Department of Applied Mathematics and Computational Sciences at PSG College of Technology. SKILL SET AREAS OF INTEREST ACADEMIC\n",
      "12917368283846924199 skills 40 60 at Department of Applied Mathematics and Computational Sciences at PSG College of Technology. SKILL SET AREAS OF INTEREST ACADEMIC\n",
      "12917368283846924199 skills 41 60 Department of Applied Mathematics and Computational Sciences at PSG College of Technology. SKILL SET AREAS OF INTEREST ACADEMIC\n",
      "12917368283846924199 skills 42 60 of Applied Mathematics and Computational Sciences at PSG College of Technology. SKILL SET AREAS OF INTEREST ACADEMIC\n",
      "12917368283846924199 skills 43 60 Applied Mathematics and Computational Sciences at PSG College of Technology. SKILL SET AREAS OF INTEREST ACADEMIC\n",
      "12917368283846924199 skills 44 60 Mathematics and Computational Sciences at PSG College of Technology. SKILL SET AREAS OF INTEREST ACADEMIC\n",
      "12917368283846924199 skills 45 60 and Computational Sciences at PSG College of Technology. SKILL SET AREAS OF INTEREST ACADEMIC\n",
      "12917368283846924199 skills 46 60 Computational Sciences at PSG College of Technology. SKILL SET AREAS OF INTEREST ACADEMIC\n",
      "12917368283846924199 skills 47 60 Sciences at PSG College of Technology. SKILL SET AREAS OF INTEREST ACADEMIC\n",
      "12917368283846924199 skills 48 60 at PSG College of Technology. SKILL SET AREAS OF INTEREST ACADEMIC\n",
      "12917368283846924199 skills 49 60 PSG College of Technology. SKILL SET AREAS OF INTEREST ACADEMIC\n",
      "12917368283846924199 skills 50 60 College of Technology. SKILL SET AREAS OF INTEREST ACADEMIC\n",
      "12917368283846924199 skills 51 60 of Technology. SKILL SET AREAS OF INTEREST ACADEMIC\n",
      "12917368283846924199 skills 52 60 Technology. SKILL SET AREAS OF INTEREST ACADEMIC\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12917368283846924199 skills 53 60 . SKILL SET AREAS OF INTEREST ACADEMIC\n",
      "12917368283846924199 skills 54 60 SKILL SET AREAS OF INTEREST ACADEMIC\n",
      "12917368283846924199 skills 38 61 Data Science at Department of Applied Mathematics and Computational Sciences at PSG College of Technology. SKILL SET AREAS OF INTEREST ACADEMIC RECORD\n",
      "12917368283846924199 skills 39 61 Science at Department of Applied Mathematics and Computational Sciences at PSG College of Technology. SKILL SET AREAS OF INTEREST ACADEMIC RECORD\n",
      "12917368283846924199 skills 40 61 at Department of Applied Mathematics and Computational Sciences at PSG College of Technology. SKILL SET AREAS OF INTEREST ACADEMIC RECORD\n",
      "12917368283846924199 skills 41 61 Department of Applied Mathematics and Computational Sciences at PSG College of Technology. SKILL SET AREAS OF INTEREST ACADEMIC RECORD\n",
      "12917368283846924199 skills 42 61 of Applied Mathematics and Computational Sciences at PSG College of Technology. SKILL SET AREAS OF INTEREST ACADEMIC RECORD\n",
      "12917368283846924199 skills 43 61 Applied Mathematics and Computational Sciences at PSG College of Technology. SKILL SET AREAS OF INTEREST ACADEMIC RECORD\n",
      "12917368283846924199 skills 44 61 Mathematics and Computational Sciences at PSG College of Technology. SKILL SET AREAS OF INTEREST ACADEMIC RECORD\n",
      "12917368283846924199 skills 45 61 and Computational Sciences at PSG College of Technology. SKILL SET AREAS OF INTEREST ACADEMIC RECORD\n",
      "12917368283846924199 skills 46 61 Computational Sciences at PSG College of Technology. SKILL SET AREAS OF INTEREST ACADEMIC RECORD\n",
      "12917368283846924199 skills 47 61 Sciences at PSG College of Technology. SKILL SET AREAS OF INTEREST ACADEMIC RECORD\n",
      "12917368283846924199 skills 48 61 at PSG College of Technology. SKILL SET AREAS OF INTEREST ACADEMIC RECORD\n",
      "12917368283846924199 skills 49 61 PSG College of Technology. SKILL SET AREAS OF INTEREST ACADEMIC RECORD\n",
      "12917368283846924199 skills 50 61 College of Technology. SKILL SET AREAS OF INTEREST ACADEMIC RECORD\n",
      "12917368283846924199 skills 51 61 of Technology. SKILL SET AREAS OF INTEREST ACADEMIC RECORD\n",
      "12917368283846924199 skills 52 61 Technology. SKILL SET AREAS OF INTEREST ACADEMIC RECORD\n",
      "12917368283846924199 skills 53 61 . SKILL SET AREAS OF INTEREST ACADEMIC RECORD\n",
      "12917368283846924199 skills 54 61 SKILL SET AREAS OF INTEREST ACADEMIC RECORD\n",
      "12917368283846924199 skills 38 62 Data Science at Department of Applied Mathematics and Computational Sciences at PSG College of Technology. SKILL SET AREAS OF INTEREST ACADEMIC RECORD Father\n",
      "12917368283846924199 skills 39 62 Science at Department of Applied Mathematics and Computational Sciences at PSG College of Technology. SKILL SET AREAS OF INTEREST ACADEMIC RECORD Father\n",
      "12917368283846924199 skills 40 62 at Department of Applied Mathematics and Computational Sciences at PSG College of Technology. SKILL SET AREAS OF INTEREST ACADEMIC RECORD Father\n",
      "12917368283846924199 skills 41 62 Department of Applied Mathematics and Computational Sciences at PSG College of Technology. SKILL SET AREAS OF INTEREST ACADEMIC RECORD Father\n",
      "12917368283846924199 skills 42 62 of Applied Mathematics and Computational Sciences at PSG College of Technology. SKILL SET AREAS OF INTEREST ACADEMIC RECORD Father\n",
      "12917368283846924199 skills 43 62 Applied Mathematics and Computational Sciences at PSG College of Technology. SKILL SET AREAS OF INTEREST ACADEMIC RECORD Father\n",
      "12917368283846924199 skills 44 62 Mathematics and Computational Sciences at PSG College of Technology. SKILL SET AREAS OF INTEREST ACADEMIC RECORD Father\n",
      "12917368283846924199 skills 45 62 and Computational Sciences at PSG College of Technology. SKILL SET AREAS OF INTEREST ACADEMIC RECORD Father\n",
      "12917368283846924199 skills 46 62 Computational Sciences at PSG College of Technology. SKILL SET AREAS OF INTEREST ACADEMIC RECORD Father\n",
      "12917368283846924199 skills 47 62 Sciences at PSG College of Technology. SKILL SET AREAS OF INTEREST ACADEMIC RECORD Father\n",
      "12917368283846924199 skills 48 62 at PSG College of Technology. SKILL SET AREAS OF INTEREST ACADEMIC RECORD Father\n",
      "12917368283846924199 skills 49 62 PSG College of Technology. SKILL SET AREAS OF INTEREST ACADEMIC RECORD Father\n",
      "12917368283846924199 skills 50 62 College of Technology. SKILL SET AREAS OF INTEREST ACADEMIC RECORD Father\n",
      "12917368283846924199 skills 51 62 of Technology. SKILL SET AREAS OF INTEREST ACADEMIC RECORD Father\n",
      "12917368283846924199 skills 52 62 Technology. SKILL SET AREAS OF INTEREST ACADEMIC RECORD Father\n",
      "12917368283846924199 skills 53 62 . SKILL SET AREAS OF INTEREST ACADEMIC RECORD Father\n",
      "12917368283846924199 skills 54 62 SKILL SET AREAS OF INTEREST ACADEMIC RECORD Father\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(found_matches)):\n",
    "    for match_id, start, end in found_matches[i]:\n",
    "            string_id = nlp.vocab.strings[match_id]  # get string representation\n",
    "            span = spacy_doc[i][start:end]                    # get the matched span\n",
    "            print(match_id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "e42d21a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 CHARU BANSAL 5th Year Undergraduate Mb: +91 7080024445 Department of Mathematics & Scientific Computing, with Minor in Machine Learning and Applications Email: charub@iitk.ac.in Indian Institute of Technology Kanpur Technical Skills Programming languages C/C++, HTML/CSS, R, Python, MATLAB Operating System Windows, Linux Educational Qualifications Year Degree/Board Institute/School CGPA/% 2018 2019 M.S. (MTH) IIT Kanpur 8.0* 2014 2018 B.S. (MTH) IIT Kanpur 6.9 2014 AISSCE (CBSE XIIth Board)\n",
      "1 None\n",
      "2 (Hons.), Department of Mechanical EngineeringThesis on Machine Learning applications in rural healthcareTechnical Skills○ Languages: Python, JavaScript, Java○ Machine Learning: Scikit learn, PyTorch, Tensorﬂow, Pandas, Numpy○ Web Stack: React JS, Spring Boot, MySQL, Docker○ AWS Services: Lambda, ECS, ECR, Batch, Step Functions, CloudWatchWork ExperienceMachine Learning Engineer BizReachTokyo, JapanRecommendation Engines | ML infrastructureOct 2019–\n",
      "3 None\n",
      "4 None\n",
      "5 None\n",
      "6 TECHNICAL SKILLS SUMMARY • Machine Learning: Classification, Regression, Clustering algorithms; Predictive and Statistical Modelling; Ensemble models; Advanced ML( LightGBM and Xgboost ), Hyperparameter Optimization; Natural Language Processing(NLP) using Spacy and Gensim; Word2Vec, Word Embeddings; Intent and Entity extraction using RASA NLU; Context based Search Engine(Semantic and Syntactic) using fastText; Timeseries Forecasting; Recommendation Engine, Collaborative Filtering; Deep Learning (CNN, LSTM, RNN), Keras, Pytorch, Transformers, BERT, NER, Question and Answering Model; SparkNLP; H2O AutoML, Anomaly Detection, Unsupervised Clustering(Kmeans and DBSCAN).\n",
      "7 CAREER OBJECTIVE: To work in pragmatic way in an organisation where I can show my talent and enhance my skills to meet company goals and objectives with full intensity.\n",
      "8  EDUCATION INTERNSHIP PROJECTS RELEVANT COURSEWORK SKILLS POSITIONS OF RESPONSIBILITY AND AWARDS WORK EXPERIENCE COMPETITION AYUSH PALIWAL AEROSPACE ENGINEER (B.Tech) ayushpaliwal2015@gmail.com|9800315999 Year Degree/Exam Institute CGPA/Marks 2019 B.TECH IIT Kharagpur 7.21/10 2014 Senior Secondary CBSE Tuli Public School, Nagpur 86.2% 2012 High School CBSE Center Point School, Nagpur 9.0/10 Reliance Jio Data Scientist July 2019 Present • Working on development and maintenance of machine learning models integrated to an automation pipeline of the project • Trained multiple classifiers on historical ambient condition data to forecast potential pest and disease (p&d) attack on the crop • Studied p&d lifecycle to identify key factors that facilitate or hinder their growth and use them to build features for the model • Responsible for updating models when new sensors are added in IOT device or new seasonal p&d training data is available Ben Gurion University of the Negev, Israel Deutsche Telekom Innovation Labs May 2019 • Worked on a novel method of unsupervised optimization of embedding dimension based on the stability of embedding • Created a custom metric, Node Pair Distance Correlation, to calculate embedding stability at varying dimensions • Implemented the method to successfully obtain optimum dimensions of Android Applications (APK apps) embedding • Received an appreciation for the work and was offered an opportunity to continue the research project as research assistant Deloitte Consulting: Information Management and Analytics May 2018 • Exploratory Data Analysis: Analyzed sales data by plotting sales against customer demographics, product and store details • Statistical Inference : Drew inference by performing parametric and nonparametric tests to review feature importance • Handling Missing Values: Developed a versatile imputing method which improved imputed missing value accuracy by 11.9% • Machine Learning: Used Random Forest to achieve 1970 rmse on predicted sales value where the benchmark was 5050 • LTFS Data Science FinHack 3 February 2021 Accomplished rank 11 out of 34415 registered in the competition; got a score of (0.62), highest (0.83) in F1 scale • JantaHack: Healthcare Analytics II July 2020 Accomplished rank 7 out of 19419 registered in the competition; got a score of (0.4364), highest (0.4390) in Accuracy scale • JantaHack: Machine Learning in Agriculture September 2020 Accomplished rank 2 out of 15381 registered in the competition; got a score of (0.9605), highest (0.9610) in Accuracy scale • JantaHack: Machine Learning for Banking May 2020 Accomplished rank 6 out of 8341 registered in the competition; got a score of (0.5365), highest (0.5399) in F1 scale • Gartner HackElite September 2019\n",
      "9 •Developed an analytics model that would help buyers and sellers predict the sales success of a set of eBay listings for iPads •Developed Logistic Regression Model for the binary output to predict the sales of the ipads •Random Forest model with Bag of Words, among various other models was found to be the optimal model SKILLS AND EXPERTISE Programming Languages : SQL, Visual Basic for Application, R Programming, C Software Packages : MS Office, SolidWorks, AutoCAD, Staad.\n",
      "10 Oct 2015 Feb 2017 SKILLS AND EXPERTISE HITEC City, Hyderabad, Telangana 500081, India ABHIROOP KUMAR +91 9932549291 | abhiroopkumar.iitkgp@gmail.com ACADEMIC QUALIFICATIONS Year Examination/Degree Institution/Board Performance 2019 Bachelor of Technology in Mining Engineering Indian Institute of Technology, Kharagpur 7.64/10 2014 Central Board of Secondary Education (CBSE) (Class XII) D.A.V. Public School, B.S.E.B Colony, Patna 84.60% 2012 Central Board of Secondary Education (CBSE) (Class X) D.A.V. Public School, B.S.E.B Colony, Patna 9.2/10 AWARDS AND ACHIEVEMENTS ▪ Published a Research paper in IEEE 16th India Council International Conference 2019 on the topic “Emotion Recognition from EEG Signal” ▪ Earned 3 Winner, 4 Top 10, and 3 Top 25 badges for excellent performance in various Data Science hackathons on Analytics Vidhya ▪ Secured 3rd position in Analyze This 2018 organized by American Express and 5th Rank in the ZS Data Science Challenge by ZS Associates ▪ Bagged 2nd and 5th Position among 34K+ data enthusiasts in Talent Hunt Hackathon by L&T Financial Services for two consecutive years ▪ Identified patients who will be on the brink of a significant increase in health care expenditures to help care management programs ▪ Predicted individual‘s medication state and Parkinson's Disease severity using raw sensor(accelerometer and gyroscope) time series data ▪ Identifying Risk and Stratifying members based on their historical clinical data and taking actionable decisions to reduce ER costs ▪ Predicted estimated claim process time and check issue date and deployed it on the provider’s dashboard for Realtime prediction ▪ Improved ZestMoney credit fraud detection model by 6% by analyzing member behavior and integrating socio economic features ▪ Analyzed the spending pattern of the account holder to discover fraudulent activities▪ ▪ Developed an ensemble sale forecasting model employing Holt Winters, ARIMAX, and TBATS achieving an accuracy of 91.58% ▪ Effectuated by the company, the model is presently being used to forecast sale for PSP of over 400 stores in 31 states in the USA ▪ Raised Gross Margin of a company by 2.6% by optimizing the car rental pricing model incorporating competitor price information ▪ Developed a customer retention model to identify the potential customers who are likely to book a car in future months ▪ Applied k Nearest Neighbors algorithm for efficient selection of 100 stores for running promotional campaigns to maximize revenue ▪ Integrated Apriori algorithm of association rule learning incorporating Market Basket Analysis for promotional marketing strategy ▪ Leading a project titled ‘Classification of EEG Motor Imagery (MI) Multi Class Signals used in BCI’ under Prof. Debasis Samanta ▪ Improving the classification accuracy by noise reduction of MI signals, optimizing features and, implementing an ensemble classifier ▪ Secured 1st position among 1200+ participants in a two month long Hackathon predicting the performance of an enrollee on tests ▪ Built a 3 level stacked model with 8 different classification algorithms as base classifier and trained GLM & GBM as meta classifier ▪ Improved model accuracy by incorporating Matrix Factorization via Singular Value Decomposition (SVD) in the stacked model ▪ Predicted the click probability of links inside a mailer for email campaigns of 1.68M unique users with an imbalance ratio of 80:1 ▪ Optimized model predicting power by applying CatBoost, Light GBM, XGBoost algorithms getting an AUC ROC score of 0.646 ▪ Ranked 5th among 10000+ competitors in Pan India contest, forecasting sales of 5 different products in 6 countries ▪ Applied Ensemble forecasting technique employing multivariate ARIMA model, linear regression, SVR, holt’s winter and TBATS ▪ Individually garnered funds worth INR 3.2 Lakhs through corporate deals and alumni contributions ▪ Coordinated the publicity drive in Northeast India leading to a 68% YoY increase in participation and 35% increase in media outreach ▪ Conducted the nationwide prelims of 4 events (140% participants increase) in Mumbai with Nukkad taking place for the first time ▪ Conceptualized and edited the promo for Spring Fest 2017 telecasted on a popular music channel VH1 with total views of 2 million ▪ Directed and edited the Official After movie and the Social initiative of Spring Fest 2017, Masoomiyat which has total views of 50000+ Software: Python | RStudio | Power BI | Tableau | Adobe Premiere Pro || Languages: C | C++ | R | Python | SQL Relevant Coursework: Econometric Analysis | Probability and Statistics | Machine Learning | Programming and Data Structure ▪ EXTRA CURRICULAR ACHIEVEMENTS Cultural ▪ Part of the bronze winning Inter Hall Eastern Vocals team of Patel Hall of Residence in General Championship 2016 17 Sports ▪ Represented Patel Hall as part of its Athletics team in the 800 meter race for 2 years in General Championship Sports Mentorship ▪ Mentored 6 freshmen students on campus under the purview of the Dean of Student Affairs, IIT Kharagpur WORK EXPERIENCE/PROJECTS Optum, United Health Group | Hyderabad | Data Scientist July 2019 Present ZestMoney | Bangalore | Data Science Intern May July 2019 Impact Analytics | Bangalore | Data Science Intern May July 2018 Drivezy | Bangalore | Business Analyst Intern Dec Jan 2018 Peel Works | Mumbai | Data Science Intern May July 2017 Bachelor's Thesis Project | Guide:\n",
      "11  Salim Shaikh Manager, HDFC Bank Ltd. EDUCATION Degree Institution CGPA Year of Passing M.Sc (STATISTICS) University Of Mumbai 6.83/7 2017 B.Sc (STATISTICS) University Of Mumbai 91.6% 2015 SKILLS Languages ML/DL Tools Big Data/ETL Python Pandas / Dask PySpark SQL OpenCv H2o\n",
      "12 Gurugram, India +91 9728427702 linkedin.com/in/dhruvdeepbishnoi EDUCATION SKILLS MBA in Information Technology & Marketing IIT\n",
      "13 None\n",
      "14 https://github.com/G Slient https://www.kaggle.com/marison CAREER OBJECTIVE Recently Graduated (2020) Computer Engineering student at Army Institute of Technology seeking a position in Data Science where I can utilize my Analytical and Visualization skills for modelling the data with the goal of discovering significant information, suggesting conclusions and support decision making.\n",
      "15 None\n",
      "16 None\n",
      "17 Great management and People skills.\n",
      "18 Coursera Data Science and Software Skills  Python, R, Java, Scala, HTML and CSS Big Data Hadoop, Pig, Hive, and Spark Deep Learning, Machine Learning, Statistics, Probability, and Data Analysis Positions of Responsibility  Team Lead – Airpush India Pvt. Ltd., Xangars Solutions Pvt.\n",
      "19 Problem Statement: Probability of Ronaldo scoring a goal, Classification of tweets (Healthcare Professional or not)\n",
      "20 o Growth Driver Analysis for a leading CPG firm ▪ Identified the Key Performance Indicators (KPIs) ▪ Used Bayesian Belief Networks to leverage direct and indirect relationships, capturing lead lag relationships, objectively making trade off decisions between markets, functions and categories and making informed decisions at the correct granular level o Financial data forecasting for a leading CPG firm ▪ Forecasted key indicators such as GSV, TTS, Turnover ▪ Built a scalable machine forecasting architecture with approximately 25 models achieving accuracies of greater than 90% on a consistent basis ▪ Used techniques like ARIMA, GARCH, ETS, Prophet, XGBoost, Bayesian etc o Automation of model generation for Growth Driver Analysis ▪ Automated the model generation process using Bayesian Belief Networks ▪ Increased efficiency, eliminated manual modelling errors and achieved a significant decrease in the time taken to generate models (From 3 5 hours to approximately 1 hour) Skills & Certifications •\n",
      "21 None\n",
      "22 Data Science at theDepartment of Applied Mathematics and Computational Sciences at PSG College of Technology.\n",
      "23 None\n",
      "24 None\n",
      "25 rajat.ranjan24@gmail.com Current Location: Melbourne AU Objective: To pursue a challenging career and be a part of progressive organization that gives a scope to enhance my knowledge and utilizing my skills towards the growth of the organization and self.\n",
      "26 None\n",
      "27 SACHDEVA sidharth19.sachdeva@gmail.com 09650302606 www.linkedin.com/in/sidharth sachdeva 5035b598/ sidharth19s H 173, DLF Skycourt, Sector 86, GurgaonSUMMARYExperienced Data Scientist with hands on skills in Python, SQL, Statistical Analysis, Machine Learning, Deep Learning & Model Building with goodvisualization and presentation skills.\n",
      "28 PROFESSIONAL SKILLS Machine Learning Algorithms Statistical Modelling Python Hive PostgreSQL Azure DataBricks Program Management Stakeholder Management EDUCATION Experienced and Data Driven Data Scientist with approx.\n",
      "29 None\n",
      "30 The Analytics Edge • Data Analysis and Statistical Inference • Introduction to Probability and Data SKILLS AND EXPERTISE • R • SQL & Hive • Python • Tableau\n",
      "31  An effective Communicator & Coordinator with Excellent Networking & Interpersonal skills.\n",
      "32 Data Science at theDepartment of Applied Mathematics and Computational Sciences at PSG College of Technology.\n",
      "33 None\n",
      "34 None\n",
      "35 None\n",
      "36 None\n",
      "37 None\n",
      "38 None\n",
      "39 Extremely motivated to constantlydevelop my skills.\n",
      "40 Excellent interpersonal and customer relationship management skills.\n",
      "41 Hiring hackathon; organized by Analytics Vidhya 2019 • Landed at Rank 34 out of 5000 in Affine Analytics ML Hackathon; organized by Hackerearth 2018 SKILLS Technical – Python, Machine Learning, NLP, Time Series, Pytorch, Deep Learning Soft Skills – Leadership, Conflict Resolution, Teamwork, Professional communication, Adaptability EXTRA CURRICULAR ACTIVITIES SRM Organized workshop on Machine Learning/Data Science for juniors to introduce them with these new technologies and how they can learn and enhance their skills in machine learning; Got 2nd/6 in VolleyBall CA Won 2 times Bronze medal in Maths Olympiad held across India;\n",
      "42  VINCY SAGAR paulsagar13vincy@gmail.com |+91 8800858216 | LinkedIn: https://www.linkedin.com/in/vincy sagar 326099138/ GitHub: www.github.com/Vincy13 | Blog: https://easyplacementguide.wordpress.com/ PROFILE Organized and ambitious budding professional with experience in data science and business analysis with a thirst for learning top notch skills, collaborating with great teams, sharing new ideas and leveraging my skills for the growth of the company.\n",
      "43 Data Science at Department of Applied Mathematics and Computational Sciences at PSG College of Technology.\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(spacy_doc)):\n",
    "    try:\n",
    "        for sent in spacy_doc[i].sents:\n",
    "            if found_matches[i][0][1] < sent.end:  # this is the fifth match, that starts at doc3[673]\n",
    "                print(i,sent)\n",
    "                break\n",
    "    except:\n",
    "        print(i,None)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "df29f93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wayal Wayal compound NNP PROPN \n",
      "Rushikesh|LinkedIn|:Wayal Rushikesh|LinkedIn|:Wayal compound NNP PROPN \n",
      "Rushikeshwayalrushi5@gmail.com rushikeshwayalrushi5@gmail.com nummod CD NUM \n",
      "| | compound NN NOUN \n",
      "8597165877EDUCATIONIIT 8597165877educationiit punct CD NUM \n",
      "KHARAGPURDUAL KHARAGPURDUAL compound NNP PROPN ORG\n",
      "DEGREEELECTRONICS DEGREEELECTRONICS compound NNP PROPN ORG\n",
      "ANDELECTRICAL ANDELECTRICAL compound NNP PROPN ORG\n",
      "COMMUNICATIONSENGINEERING(AIR COMMUNICATIONSENGINEERING(AIR ROOT NNP PROPN ORG\n",
      "690)2017 690)2017 compound CD NUM \n",
      "2021CGPA 2021cgpa appos CD NUM \n",
      ": : punct : PUNCT \n",
      "8.76/10AKLANK 8.76/10aklank punct CD NUM CARDINAL\n",
      "PUBLIC public amod JJ ADJ \n",
      "SCHOOLCLASS SCHOOLCLASS appos NNP PROPN \n",
      "XII2016 XII2016 appos NNP PROPN \n",
      "2017Percentage 2017percentage nummod CD NUM \n",
      ": : punct : PUNCT \n",
      "84.6PODAR 84.6podar compound CD NUM \n",
      "INT INT appos NNP PROPN \n",
      ". . punct . PUNCT \n",
      "SCHOOLCLASS SCHOOLCLASS nsubj NNP PROPN \n",
      "XNTSE XNTSE ROOT NNP PROPN PERSON\n",
      "Qualified2014 Qualified2014 dep NNP PROPN \n",
      "2015CGPA 2015cgpa npadvmod CD NUM \n",
      ": : punct : PUNCT \n",
      "10/10SKILLSPROGRAMMING• 10/10skillsprogramming• nsubj CD NUM CARDINAL\n",
      "Python• Python• ROOT NNP PROPN \n",
      "C C dobj NNP PROPN \n",
      "and and cc CC CCONJ \n",
      "C++LIBRARIES• c++libraries• conj JJ ADJ \n",
      "Pytorch• Pytorch• compound NNP PROPN \n",
      "Keras• Keras• compound NNP PROPN \n",
      "Scikit Scikit compound NNP PROPN \n",
      "learn• learn• ROOT NN NOUN \n",
      "Numpy• Numpy• punct NNP PROPN PERSON\n",
      "PandasUTILITIES• PandasUTILITIES• compound NNP PROPN \n",
      "Tableau• Tableau• ROOT NNP PROPN \n",
      "Solid Solid amod NNP PROPN \n",
      "WorksCOURSEWORK•Machine WorksCOURSEWORK•Machine compound NNP PROPN \n",
      "Learning•Neural learning•neural amod JJ ADJ \n",
      "Networksand Networksand nsubj NNP PROPN \n",
      "its its poss PRP$ PRON \n",
      "Application•Probability application•probability nsubj NN NOUN \n",
      "and and cc CC CCONJ \n",
      "Stochastic•Mathematical stochastic•mathematical amod JJ ADJ \n",
      "Methods•Algorithms(Theory Methods•Algorithms(Theory conj NNP PROPN \n",
      "and and cc CC CCONJ \n",
      "Lab)•Engineering lab)•engineere conj VBG VERB \n",
      "Mathematics•Programming mathematics•programming dobj NN NOUN \n",
      "and and cc CC CCONJ \n",
      "Data Data conj NNP PROPN \n",
      "Structures•Matrix structures•matrix ROOT VBP VERB \n",
      "AlgebraEXTRA algebraextra compound CD NUM ORG\n",
      "CURRICULARNational curricularnational det JJ ADJ ORG\n",
      "Sports Sports compound NNPS PROPN ORG\n",
      "Organisation•Taught organisation•taught npadvmod NN NOUN \n",
      "the the det DT DET \n",
      "students student dobj NNS NOUN \n",
      "of of prep IN ADP \n",
      "primary primary amod JJ ADJ \n",
      "school•Part school•part pobj NN NOUN \n",
      "of of prep IN ADP \n",
      "the the det DT DET \n",
      "NSO NSO compound NNP PROPN ORG\n",
      "YogaInter YogaInter compound NNP PROPN ORG\n",
      "Hall Hall pobj NNP PROPN ORG\n",
      "Events•Data Events•Data punct NNP PROPN \n",
      "Analytics•Open Analytics•Open compound NNP PROPN \n",
      "SoftINTERNSHIPSHSBC| softinternshipshsbc| ROOT NN NOUN \n",
      "JUNIOR junior compound NN NOUN \n",
      "ANALYST•Study analyst•study dobj NN NOUN \n",
      "of of prep IN ADP \n",
      "order order compound NN NOUN \n",
      "book book pobj NN NOUN \n",
      "resilience.•Developed resilience.•developed appos NN NOUN \n",
      "an an det DT DET \n",
      "algorithm algorithm appos NN NOUN \n",
      "to to aux TO PART \n",
      "classify classify acl VB VERB \n",
      "the the det DT DET \n",
      "aggressiveness aggressiveness dobj NN NOUN \n",
      "of of prep IN ADP \n",
      "orders.•Quantiﬁed orders.•quantiﬁed pobj DT PRON \n",
      "the the det DT DET \n",
      "changes change dobj NNS NOUN \n",
      "in in prep IN ADP \n",
      "behaviours behaviour pobj NNS NOUN \n",
      "of of prep IN ADP \n",
      "liquidity liquidity compound NN NOUN \n",
      "measures measure pobj NNS NOUN \n",
      "aroundaggressive aroundaggressive punct JJ ADJ \n",
      "orders.•Quantiﬁed orders.•quantiﬁed nsubj DT PRON \n",
      "the the det DT DET \n",
      "differences difference ROOT NNS NOUN \n",
      "in in prep IN ADP \n",
      "market market compound NN NOUN \n",
      "impact impact pobj NN NOUN \n",
      "left leave acl VBN VERB \n",
      "by by agent IN ADP \n",
      "various various amod JJ ADJ \n",
      "orders order pobj NNS NOUN \n",
      "and and cc CC CCONJ \n",
      "invarious invarious conj JJ ADJ \n",
      "markets.•Extended markets.•Extended amod NNP PROPN \n",
      "the the det DT DET \n",
      "created create amod VBN VERB \n",
      "mechanism mechanism dobj NN NOUN \n",
      "to to aux TO PART \n",
      "ﬁnd ﬁnd relcl VB VERB \n",
      "optimal optimal amod JJ ADJ \n",
      "combination combination dobj NN NOUN \n",
      "of of prep IN ADP \n",
      "aggressiveorders aggressiveorder pobj NNS NOUN \n",
      "to to aux TO PART \n",
      "give give advcl VB VERB \n",
      "low low amod JJ ADJ \n",
      "cost cost dobj NN NOUN \n",
      "and and cc CC CCONJ \n",
      "minimal minimal amod JJ ADJ \n",
      "market market compound NN NOUN \n",
      "impact impact conj NN NOUN \n",
      ". . punct . PUNCT \n",
      "COMPETITIONSINSTANT COMPETITIONSINSTANT compound NNP PROPN ORG\n",
      "GRATIFICATION gratification ROOT NN NOUN \n",
      "| | nummod NN NOUN \n",
      "KAGGLE•Binary kaggle•binary amod JJ ADJ \n",
      "prediction prediction compound NN NOUN \n",
      "challenge challenge nsubj NN NOUN \n",
      "having have acl VBG VERB \n",
      "a a det DT DET \n",
      "puzzling puzzle amod VBG VERB \n",
      "data datum compound NNS NOUN \n",
      "set.•The set.•the compound NN NOUN \n",
      "activation activation dobj NN NOUN \n",
      "of of prep IN ADP \n",
      "certain certain amod JJ ADJ \n",
      "columns column pobj NNS NOUN \n",
      "was be ROOT VBD AUX \n",
      "dependent dependent acomp JJ ADJ \n",
      "on on prep IN ADP \n",
      "certain certain amod JJ ADJ \n",
      "factors factor pobj NNS NOUN \n",
      "andhugely andhugely advmod RB ADV \n",
      "affected affect conj VBD VERB \n",
      "the the det DT DET \n",
      "prediction•Found prediction•found dobj NN NOUN \n",
      "out out prt RP ADP \n",
      "the the det DT DET \n",
      "best good amod JJS ADJ \n",
      "model model dobj NN NOUN \n",
      "and and cc CC CCONJ \n",
      "hence hence advmod RB ADV \n",
      "data datum compound NNS NOUN \n",
      "distribution distribution compound NN NOUN \n",
      "type type conj NN NOUN \n",
      ". . punct . PUNCT \n",
      "Rank:244ML rank:244ml nmod ADD X \n",
      "/ / punct SYM SYM \n",
      "AI AI nmod NNP PROPN \n",
      "CHALLENGE| challenge| nummod CD NUM \n",
      "FLIPKART FLIPKART compound NNP PROPN \n",
      "GRID grid npadvmod NN NOUN \n",
      "– – punct : PUNCT \n",
      "TE[A]CH TE[A]CH appos NNP PROPN \n",
      "THE the det DT DET \n",
      "MACHINES•Trained machines•traine amod VBN VERB \n",
      "A a compound DT PRON \n",
      "transfer transfer compound NN NOUN \n",
      "learning learning compound NN NOUN \n",
      "algorithm algorithm appos NN NOUN \n",
      "to to aux TO PART \n",
      "predict predict relcl VB VERB \n",
      "the the det DT DET \n",
      "bounding bounding compound NN NOUN \n",
      "box box dobj NN NOUN \n",
      "of of prep IN ADP \n",
      "objectsNETAPP objectsNETAPP compound NNP PROPN \n",
      "DATA DATA compound NNP PROPN \n",
      "CHALLENGE CHALLENGE compound NNP PROPN \n",
      "| | pobj NN NOUN \n",
      "KSHITIJ KSHITIJ pobj NNP PROPN DATE\n",
      "2019 2019 nummod CD NUM DATE\n",
      ", , punct , PUNCT \n",
      "IIT IIT nsubj NNP PROPN ORG\n",
      "KHARAGPUR•Used kharagpur•use ROOT VBD VERB \n",
      "Tf Tf compound NNP PROPN ORG\n",
      "idf idf dobj NNP PROPN ORG\n",
      "and and cc CC CCONJ \n",
      "word2vec word2vec conj NNP PROPN \n",
      "to to aux TO PART \n",
      "build build xcomp VB VERB \n",
      "a a det DT DET \n",
      "news news compound NN NOUN \n",
      "classiﬁer classiﬁer dobj NN NOUN \n",
      "based base prep VBN VERB \n",
      "on on prep IN ADP \n",
      "headlines headline compound NNS NOUN \n",
      "andshort andshort pobj NN NOUN \n",
      "description•Stood description•stood compound CD NUM \n",
      "2nd 2nd dobj NN NOUN ORDINAL\n",
      "among among prep IN ADP \n",
      "more more amod JJR ADJ CARDINAL\n",
      "than than quantmod IN ADP CARDINAL\n",
      "150 150 nummod CD NUM CARDINAL\n",
      "teams team pobj NNS NOUN \n",
      ", , punct , PUNCT \n",
      "held hold acl VBN VERB \n",
      "at at prep IN ADP \n",
      "Data Data compound NNP PROPN ORG\n",
      "Analytics Analytics compound NNP PROPN ORG\n",
      "Event Event pobj NNP PROPN ORG\n",
      ". . punct . PUNCT \n",
      "INTEL INTEL nmod NNP PROPN \n",
      "SCENE SCENE compound NNP PROPN \n",
      "CLASSIFICATION CLASSIFICATION compound NNP PROPN \n",
      "CHALLENGE CHALLENGE compound NNP PROPN \n",
      "| | ROOT NN NOUN \n",
      "ANALYTICS analytics npadvmod NN NOUN \n",
      "VIDHYA•Used vidhya•use amod VBN VERB \n",
      "Fastai fastai compound NN NOUN GPE\n",
      "libraby libraby nsubj NN NOUN \n",
      "( ( punct -LRB- PUNCT \n",
      "resnet152 resnet152 appos NNP PROPN \n",
      ") ) punct -RRB- PUNCT \n",
      "to to aux TO PART \n",
      "build build relcl VB VERB \n",
      "image image dobj NN NOUN \n",
      "classiﬁer•Final classiﬁer•final det JJ ADJ \n",
      "rank rank dobj NN NOUN \n",
      "82 82 nummod CD NUM CARDINAL\n",
      "with with prep IN ADP \n",
      "accuracy accuracy pobj NN NOUN \n",
      "0.94MOOCSMACHINE 0.94moocsmachine nummod CD NUM WORK_OF_ART\n",
      "LEARNING learn ccomp VBG VERB WORK_OF_ART\n",
      "A a det DT DET WORK_OF_ART\n",
      "Z z dobj NN NOUN WORK_OF_ART\n",
      "™ ™ punct VBP VERB \n",
      "•Study •study dobj NN NOUN \n",
      "of of prep IN ADP \n",
      "regression regression pobj NN NOUN \n",
      ", , punct , PUNCT \n",
      "CART cart compound NN NOUN \n",
      "models model appos NNS NOUN \n",
      ", , punct , PUNCT \n",
      "Text text nsubj NN NOUN \n",
      "parsing parse ROOT VBG VERB \n",
      "Regression regression compound NN NOUN \n",
      "trees tree dobj NNS NOUN \n",
      ", , punct , PUNCT \n",
      "Clustering clustering conj NN NOUN \n",
      ", , punct , PUNCT \n",
      "Data Data compound NNP PROPN ORG\n",
      "Visualization Visualization conj NNP PROPN ORG\n",
      "and and cc CC CCONJ \n",
      "its its poss PRP$ PRON \n",
      "implementation implementation conj NN NOUN \n",
      ". . punct . PUNCT \n",
      "DEEP deep amod JJ ADJ \n",
      "LEARNING LEARNING compound NNP PROPN \n",
      "SPECIALIZATION•Study specialization•study nsubj NN NOUN \n",
      "of of prep IN ADP \n",
      "CNN CNN pobj NNP PROPN ORG\n",
      ", , punct , PUNCT \n",
      "RCNN RCNN appos NNS NOUN \n",
      "for for prep IN ADP \n",
      "image image compound NN NOUN \n",
      "processing processing pobj NN NOUN \n",
      "and and cc CC CCONJ \n",
      "LSTM LSTM conj NNP PROPN \n",
      "for for prep IN ADP \n",
      "text text compound NN NOUN \n",
      "processingMACHINE processingMACHINE compound NNP PROPN \n",
      "LEARNING(CS239 learning(cs239 pobj CD NUM \n",
      ") ) punct -RRB- PUNCT \n",
      "| | ROOT NN NOUN \n",
      "OFFERED offer acl VBN VERB \n",
      "BY by agent IN ADP \n",
      "STANFORD STANFORD compound NNP PROPN ORG\n",
      "UNIVERSITY•Study UNIVERSITY•Study pobj NNP PROPN \n",
      "of of prep IN ADP \n",
      "CNN CNN pobj NNP PROPN ORG\n",
      ", , punct , PUNCT \n",
      "RCNN RCNN conj NNS NOUN ORG\n",
      "and and cc CC CCONJ \n",
      "ResNetGROUPSKHARAGPUR ResNetGROUPSKHARAGPUR compound NNP PROPN ORG\n",
      "DATA DATA compound NNP PROPN ORG\n",
      "ANALYTICS ANALYTICS compound NNP PROPN ORG\n",
      "GROUP GROUP compound NNP PROPN ORG\n",
      "| | compound NN NOUN \n",
      "IIT IIT conj NNP PROPN ORG\n",
      "KHARAGPUR• kharagpur• advmod RP ADP \n",
      "Conducting conduct ROOT VBG VERB \n",
      "of of prep IN ADP \n",
      "data datum compound NNS NOUN \n",
      "antics antic compound NNS NOUN \n",
      "camps camp pobj NNS NOUN \n",
      ". . punct . PUNCT \n",
      "POSITION position nsubj NN NOUN \n",
      "OF of prep IN ADP \n",
      "RESPONSIBILITYCAPTAIN RESPONSIBILITYCAPTAIN pobj NNP PROPN ORG\n",
      ": : punct : PUNCT \n",
      "DATA DATA compound NNP PROPN \n",
      "ANALYTICS ANALYTICS compound NNP PROPN \n",
      "| | compound NNP PROPN \n",
      "MEGHNAD MEGHNAD compound NNP PROPN ORG\n",
      "SAHA SAHA compound NNP PROPN ORG\n",
      "HALL HALL appos NNP PROPN ORG\n",
      "OF of prep IN ADP ORG\n",
      "RESIDENCE• RESIDENCE• pobj NNP PROPN ORG\n",
      "Headed head ROOT VBD VERB ORG\n",
      "the the det DT DET \n",
      "gold gold amod NN NOUN \n",
      "wining wine amod VBG VERB \n",
      "team team dobj NN NOUN \n",
      "in in prep IN ADP \n",
      "Open Open compound NNP PROPN \n",
      "IIT IIT compound NNP PROPN ORG\n",
      "Data Data compound NNP PROPN ORG\n",
      "Analytics Analytics pobj NNPS PROPN ORG\n",
      "competition.• competition.• punct NNP PROPN ORG\n",
      "Trained train ROOT VBN VERB \n",
      "new new amod JJ ADJ \n",
      "batch batch dobj NN NOUN \n",
      "in in prep IN ADP \n",
      "Data Data compound NNP PROPN ORG\n",
      "Science• Science• compound NNP PROPN ORG\n",
      "Responsibility Responsibility pobj NNP PROPN ORG\n",
      "of of prep IN ADP \n",
      "conducting conduct pcomp VBG VERB \n",
      "meetings meeting dobj NNS NOUN \n",
      "on on prep IN ADP \n",
      "regular regular amod JJ ADJ \n",
      "intervals interval pobj NNS NOUN \n",
      "for for prep IN ADP \n",
      "preparationof preparationof amod VBN VERB \n",
      "Inter Inter compound NNP PROPN NORP\n",
      "hall hall compound NN NOUN \n",
      "event event pobj NN NOUN \n",
      "competitions.1 competitions.1 punct NN NOUN \n"
     ]
    }
   ],
   "source": [
    "for i in tokens[33]:\n",
    "    print(i,i.lemma_,i.dep_,i.tag_,i.pos_,i.ent_type_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aed02a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
