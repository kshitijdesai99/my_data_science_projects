{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPWH43tGvdw5ZRpHjo9JZd1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kshitijdesai99/my_data_science_projects/blob/main/DNN_implementation_using_torch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import sys\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader"
      ],
      "metadata": {
        "id": "D9VTtfFJTe0r"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "YuC0rYgvTC3a"
      },
      "outputs": [],
      "source": [
        "# parameters\n",
        "\n",
        "# random.rand() - Generates random number from a uniform distribution over [0,1)\n",
        "# 1000 rows and 10 columns\n",
        "X = np.random.rand(1000, 10)\n",
        "\n",
        "# random.randint() - Picks a random int between 0 and 1 of 1000 columns\n",
        "y = np.random.randint(0,2,1000)\n",
        "\n",
        "# W1, W2, W3 here are the weights in the 1st hidden layer, 2nd hidden layer and output layer\n",
        "\n",
        "# pick a random number between 0 and 1 and create 2*10 matrix\n",
        "W1 = torch.tensor(np.random.uniform(0,1,(2,10)),requires_grad=True)\n",
        "\n",
        "# pick a random number between 0 and 1 and create 3*2 matrix\n",
        "W2 = torch.tensor(np.random.uniform(0,1,(3,2)),requires_grad=True)\n",
        "\n",
        "# pick a random number betwee 0 and 1 and create 1*3 marix\n",
        "W3 = torch.tensor(np.random.uniform(0,1,3),requires_grad=True)\n",
        "\n",
        "# add these weights to a list W_list\n",
        "W_list = [W1, W2, W3]\n",
        "\n",
        "#No of epochs\n",
        "nepochs = 100\n",
        "\n",
        "#learning rate\n",
        "lr = 0.0001\n",
        "\n",
        "# define the loss function here\n",
        "loss_fn = nn.BCELoss()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_x = torch.Tensor(X)\n",
        "tensor_y = torch.Tensor(y)\n",
        "\n",
        "Xy = TensorDataset(tensor_x, tensor_y)\n",
        "Xy_loader = DataLoader(Xy, batch_size = 16, shuffle=True, drop_last=True)"
      ],
      "metadata": {
        "id": "CekXFl5mTaSh"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.Sequential(\n",
        "    nn.Linear(10, 200),\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm1d(num_features = 200),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(200, 100),\n",
        "    nn.Tanh(),\n",
        "    nn.BatchNorm1d(num_features = 100),\n",
        "    nn.Linear(100,1),\n",
        "    nn.Sigmoid()\n",
        ")"
      ],
      "metadata": {
        "id": "SCrFCiL1T2eG"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "azY_AJfnUc1N"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.BCELoss()"
      ],
      "metadata": {
        "id": "V5ZcMepjUp-s"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nepochs = 100\n",
        "for epoch in range(nepochs):\n",
        "  for X,y in Xy_loader:\n",
        "    batch_size = X.shape[0]\n",
        "\n",
        "    #X.view is like np.reshape\n",
        "    y_hat = model(X.view(batch_size,-1))\n",
        "\n",
        "    y = y.unsqueeze(1)\n",
        "\n",
        "    loss = loss_fn(y_hat, y)\n",
        "\n",
        "    # setting the gradients to zero\n",
        "    # clear the accumulated gradients from previous iteration before\n",
        "    # before computing gradients for the current iteration\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # backpropogating the loss\n",
        "    loss.backward()\n",
        "\n",
        "    # updating weights\n",
        "    optimizer.step()\n",
        "  \n",
        "  print(float(loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqaBumCOUya6",
        "outputId": "4a662dab-5567-4bc0-9728-6e09d82cc43b"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6871651411056519\n",
            "0.7486498355865479\n",
            "0.7270989418029785\n",
            "0.6660298705101013\n",
            "0.7590755224227905\n",
            "0.6290777921676636\n",
            "0.6355878114700317\n",
            "0.6318148374557495\n",
            "0.6988677382469177\n",
            "0.7291333675384521\n",
            "0.7342350482940674\n",
            "0.712933361530304\n",
            "0.6091144680976868\n",
            "0.761988639831543\n",
            "0.6325455904006958\n",
            "0.6821422576904297\n",
            "0.7162585854530334\n",
            "0.5818670988082886\n",
            "0.6500377058982849\n",
            "0.7358464598655701\n",
            "0.6971769332885742\n",
            "0.7597783803939819\n",
            "0.5958792567253113\n",
            "0.6317734122276306\n",
            "0.7369711995124817\n",
            "0.6422520279884338\n",
            "0.7863887548446655\n",
            "0.6178295612335205\n",
            "0.7119387984275818\n",
            "0.6932079195976257\n",
            "0.5641357898712158\n",
            "0.6188817024230957\n",
            "0.6410239934921265\n",
            "0.614646315574646\n",
            "0.5725829601287842\n",
            "0.6432338953018188\n",
            "0.49076464772224426\n",
            "0.624797523021698\n",
            "0.7015645503997803\n",
            "0.5183502435684204\n",
            "0.5333356261253357\n",
            "0.6872117519378662\n",
            "0.6293474435806274\n",
            "0.6202964186668396\n",
            "0.4709894061088562\n",
            "0.62205570936203\n",
            "0.6394284963607788\n",
            "0.4925467073917389\n",
            "0.6915730834007263\n",
            "0.4942721128463745\n",
            "0.7240598201751709\n",
            "0.631935715675354\n",
            "0.6644836664199829\n",
            "0.5125154256820679\n",
            "0.594194769859314\n",
            "0.52049320936203\n",
            "0.5718055963516235\n",
            "0.6042579412460327\n",
            "0.6490140557289124\n",
            "0.5033105611801147\n",
            "0.527855634689331\n",
            "0.6037653684616089\n",
            "0.5568998456001282\n",
            "0.5310148596763611\n",
            "0.5487459897994995\n",
            "0.3958999514579773\n",
            "0.6038866639137268\n",
            "0.5903739333152771\n",
            "0.5309581160545349\n",
            "0.5718071460723877\n",
            "0.47886747121810913\n",
            "0.4778997600078583\n",
            "0.6375041007995605\n",
            "0.6785897612571716\n",
            "0.4680776596069336\n",
            "0.4697643518447876\n",
            "0.36695367097854614\n",
            "0.3976701498031616\n",
            "0.4871806800365448\n",
            "0.4617393910884857\n",
            "0.3419336974620819\n",
            "0.5943402051925659\n",
            "0.3715691566467285\n",
            "0.629112958908081\n",
            "0.5134928822517395\n",
            "0.43777942657470703\n",
            "0.4991084337234497\n",
            "0.5325546860694885\n",
            "0.6848218441009521\n",
            "0.603217363357544\n",
            "0.6867027282714844\n",
            "0.3487520217895508\n",
            "0.4490886926651001\n",
            "0.6000080704689026\n",
            "0.38688403367996216\n",
            "0.35999661684036255\n",
            "0.5731773972511292\n",
            "0.4162779450416565\n",
            "0.558535635471344\n",
            "0.3876088857650757\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  xt = torch.tensor(np.random.rand(1, 10))\n",
        "  model.eval()\n",
        "  y2 = model(xt.float())\n",
        "  print(y2.detach().numpy()[0][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3Z10RyUW-5u",
        "outputId": "c6cf6423-f8a3-435c-9b9b-080b65021e04"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.91654146\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Xavier initialization - can be used for weight initializations"
      ],
      "metadata": {
        "id": "6eVZCu-KacCm"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# input data and output data can have different distribuitons"
      ],
      "metadata": {
        "id": "1INsjdzUec2j"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Covariate shift  - overcome by batch Normalization"
      ],
      "metadata": {
        "id": "oXLug5Owbdtm"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generalizations - Dropout and Early stopping"
      ],
      "metadata": {
        "id": "YGg10uHFeTsx"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Early stopping - point where validation error goes up and training goes down"
      ],
      "metadata": {
        "id": "4B8_40b9hMt4"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparamters\n",
        "\n",
        "# 1. No of layers\n",
        "# 2. No of units in each layer\n",
        "# 3. Activation function\n",
        "# 4. Initiatial learning rate\n",
        "# 5. Batch size\n",
        "# 6. Drop out ratio\n",
        "# 7. Weight initialization\n",
        "# 8. Learning rate decay policy\n",
        "# 9. Early stopping patience"
      ],
      "metadata": {
        "id": "Ae801_ach8MA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}