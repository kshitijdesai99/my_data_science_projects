{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNnn2aLZGwfrEZjJPtiNe5D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kshitijdesai99/my_data_science_projects/blob/main/NN_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import sys\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "5-O99ZwbK27A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stochiastic gradient descent from scratch"
      ],
      "metadata": {
        "id": "jiyrWJqwXOmy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# parameters\n",
        "\n",
        "# random.rand() - Generates random number from a uniform distribution over [0,1)\n",
        "# 1000 rows and 10 columns\n",
        "X = np.random.rand(1000, 10)\n",
        "\n",
        "# random.randint() - Picks a random int between 0 and 1 of 1000 columns\n",
        "y = np.random.randint(0,2,1000)\n",
        "\n",
        "# W1, W2, W3 here are the weights in the 1st hidden layer, 2nd hidden layer and output layer\n",
        "\n",
        "# pick a random number between 0 and 1 and create 2*10 matrix\n",
        "W1 = torch.tensor(np.random.uniform(0,1,(2,10)),requires_grad=True)\n",
        "\n",
        "# pick a random number between 0 and 1 and create 3*2 matrix\n",
        "W2 = torch.tensor(np.random.uniform(0,1,(3,2)),requires_grad=True)\n",
        "\n",
        "# pick a random number betwee 0 and 1 and create 1*3 marix\n",
        "W3 = torch.tensor(np.random.uniform(0,1,3),requires_grad=True)\n",
        "\n",
        "# add these weights to a list W_list\n",
        "W_list = [W1, W2, W3]\n",
        "\n",
        "#No of epochs\n",
        "nepochs = 100\n",
        "\n",
        "#learning rate\n",
        "lr = 0.0001\n",
        "\n",
        "# define the loss function here\n",
        "loss_fn = nn.BCELoss()\n"
      ],
      "metadata": {
        "id": "YrGQAGgPKcvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper functions\n",
        "\n",
        "# Activation function\n",
        "def activate(X):\n",
        "\n",
        "  #sigmoid function\n",
        "  return 1/(1+torch.exp(-X))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Moving forward\n",
        "def forwardStep(X, W_list):\n",
        "\n",
        "  # converting numpy to torch data type\n",
        "  h = torch.from_numpy(X)\n",
        "\n",
        "  # Iterating through every weights of every layer\n",
        "  for W in W_list:\n",
        "    # (1*10) * (10*2) --> (1*2) * (2*3) --> (1*3) * (3*1) --> (1*1)\n",
        "    z = torch.matmul(W, h)\n",
        "\n",
        "    #applying non linear activation function\n",
        "    h = activate(z)\n",
        "\n",
        "  return h\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Update params - Based on gradients and learning rates weights are updated\n",
        "def updateParams(W_list, dW_list, lr):\n",
        "\n",
        "  # torch.no_grad() -> loop where requires_grad is set to false\n",
        "  with torch.no_grad():\n",
        "    for i in range(len(W_list)):\n",
        "      W_list[i] -= lr*dW_list[i]\n",
        "  return W_list\n"
      ],
      "metadata": {
        "id": "mwElSRnWNVGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wup-v2pCKF4z"
      },
      "outputs": [],
      "source": [
        "def trainNN_sgd(X, y, W_list, loss_fn, lr, nepochs):\n",
        "\n",
        "  # Iterating every epoch\n",
        "  for epoch in range(nepochs):\n",
        "\n",
        "    # storing batch loss in a list for batch gradient\n",
        "    avgLoss = []\n",
        "\n",
        "    # Iterating through every output\n",
        "    for i in range(len(y)):\n",
        "\n",
        "      #passing input\n",
        "      X_in = X[i, :]\n",
        "\n",
        "      #passing output\n",
        "      y_True = y[i]\n",
        "\n",
        "      #predicting new output\n",
        "      y_hat = forwardStep(X_in, W_list)\n",
        "\n",
        "      #checking the loss\n",
        "      loss = loss_fn(y_hat, torch.tensor(y_True, dtype=torch.double))\n",
        "\n",
        "      #Backpropogating the gradients wrt current loss\n",
        "      loss.backward()\n",
        "\n",
        "      #Appending loss to averageLoss list\n",
        "      avgLoss.append(loss.item())\n",
        "\n",
        "      #Flushing the buffer to the terminal\n",
        "      sys.stdout.flush()\n",
        "\n",
        "      # Storing all the gradients in a list\n",
        "      dW_list = []\n",
        "\n",
        "      for j in range(len(W_list)):\n",
        "        dW_list.append(W_list[j].grad.data)\n",
        "\n",
        "      # Updating weights to reduce the loss      \n",
        "      W_list = updateParams(W_list, dW_list, lr)\n",
        "\n",
        "      # Setting the gradients to 0 since we are starting new epoch\n",
        "      for j in range(len(W_list)):\n",
        "        W_list[j].grad.data.zero_()\n",
        "\n",
        "    # printing the loss and epoch name\n",
        "    print(\"Loss after epoch = %d: %f\"%(epoch, np.mean(np.array(avgLoss))))\n",
        "\n",
        "  # finally return the last weights\n",
        "  return W_list"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calling the function\n",
        "trainNN_sgd(X,y,W_list, loss_fn, lr, nepochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NWyW-mYWgW5",
        "outputId": "e0c8a0bd-b094-4071-8705-92a41fb9801c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss after epoch = 0: 0.957474\n",
            "Loss after epoch = 1: 0.939600\n",
            "Loss after epoch = 2: 0.922793\n",
            "Loss after epoch = 3: 0.907008\n",
            "Loss after epoch = 4: 0.892200\n",
            "Loss after epoch = 5: 0.878321\n",
            "Loss after epoch = 6: 0.865326\n",
            "Loss after epoch = 7: 0.853170\n",
            "Loss after epoch = 8: 0.841809\n",
            "Loss after epoch = 9: 0.831199\n",
            "Loss after epoch = 10: 0.821298\n",
            "Loss after epoch = 11: 0.812065\n",
            "Loss after epoch = 12: 0.803461\n",
            "Loss after epoch = 13: 0.795448\n",
            "Loss after epoch = 14: 0.787990\n",
            "Loss after epoch = 15: 0.781051\n",
            "Loss after epoch = 16: 0.774600\n",
            "Loss after epoch = 17: 0.768604\n",
            "Loss after epoch = 18: 0.763034\n",
            "Loss after epoch = 19: 0.757862\n",
            "Loss after epoch = 20: 0.753061\n",
            "Loss after epoch = 21: 0.748605\n",
            "Loss after epoch = 22: 0.744472\n",
            "Loss after epoch = 23: 0.740638\n",
            "Loss after epoch = 24: 0.737084\n",
            "Loss after epoch = 25: 0.733790\n",
            "Loss after epoch = 26: 0.730736\n",
            "Loss after epoch = 27: 0.727907\n",
            "Loss after epoch = 28: 0.725286\n",
            "Loss after epoch = 29: 0.722859\n",
            "Loss after epoch = 30: 0.720611\n",
            "Loss after epoch = 31: 0.718529\n",
            "Loss after epoch = 32: 0.716601\n",
            "Loss after epoch = 33: 0.714817\n",
            "Loss after epoch = 34: 0.713165\n",
            "Loss after epoch = 35: 0.711637\n",
            "Loss after epoch = 36: 0.710222\n",
            "Loss after epoch = 37: 0.708913\n",
            "Loss after epoch = 38: 0.707701\n",
            "Loss after epoch = 39: 0.706580\n",
            "Loss after epoch = 40: 0.705542\n",
            "Loss after epoch = 41: 0.704583\n",
            "Loss after epoch = 42: 0.703695\n",
            "Loss after epoch = 43: 0.702873\n",
            "Loss after epoch = 44: 0.702113\n",
            "Loss after epoch = 45: 0.701410\n",
            "Loss after epoch = 46: 0.700760\n",
            "Loss after epoch = 47: 0.700158\n",
            "Loss after epoch = 48: 0.699602\n",
            "Loss after epoch = 49: 0.699087\n",
            "Loss after epoch = 50: 0.698611\n",
            "Loss after epoch = 51: 0.698170\n",
            "Loss after epoch = 52: 0.697763\n",
            "Loss after epoch = 53: 0.697386\n",
            "Loss after epoch = 54: 0.697037\n",
            "Loss after epoch = 55: 0.696715\n",
            "Loss after epoch = 56: 0.696417\n",
            "Loss after epoch = 57: 0.696141\n",
            "Loss after epoch = 58: 0.695886\n",
            "Loss after epoch = 59: 0.695650\n",
            "Loss after epoch = 60: 0.695431\n",
            "Loss after epoch = 61: 0.695229\n",
            "Loss after epoch = 62: 0.695043\n",
            "Loss after epoch = 63: 0.694870\n",
            "Loss after epoch = 64: 0.694710\n",
            "Loss after epoch = 65: 0.694562\n",
            "Loss after epoch = 66: 0.694425\n",
            "Loss after epoch = 67: 0.694299\n",
            "Loss after epoch = 68: 0.694182\n",
            "Loss after epoch = 69: 0.694074\n",
            "Loss after epoch = 70: 0.693973\n",
            "Loss after epoch = 71: 0.693881\n",
            "Loss after epoch = 72: 0.693795\n",
            "Loss after epoch = 73: 0.693716\n",
            "Loss after epoch = 74: 0.693643\n",
            "Loss after epoch = 75: 0.693575\n",
            "Loss after epoch = 76: 0.693512\n",
            "Loss after epoch = 77: 0.693454\n",
            "Loss after epoch = 78: 0.693400\n",
            "Loss after epoch = 79: 0.693351\n",
            "Loss after epoch = 80: 0.693305\n",
            "Loss after epoch = 81: 0.693262\n",
            "Loss after epoch = 82: 0.693223\n",
            "Loss after epoch = 83: 0.693186\n",
            "Loss after epoch = 84: 0.693153\n",
            "Loss after epoch = 85: 0.693121\n",
            "Loss after epoch = 86: 0.693093\n",
            "Loss after epoch = 87: 0.693066\n",
            "Loss after epoch = 88: 0.693041\n",
            "Loss after epoch = 89: 0.693018\n",
            "Loss after epoch = 90: 0.692997\n",
            "Loss after epoch = 91: 0.692978\n",
            "Loss after epoch = 92: 0.692960\n",
            "Loss after epoch = 93: 0.692943\n",
            "Loss after epoch = 94: 0.692928\n",
            "Loss after epoch = 95: 0.692913\n",
            "Loss after epoch = 96: 0.692900\n",
            "Loss after epoch = 97: 0.692888\n",
            "Loss after epoch = 98: 0.692876\n",
            "Loss after epoch = 99: 0.692866\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([[0.2994, 0.0401, 0.9599, 0.4630, 0.2344, 0.6865, 0.0861, 0.9256, 0.9877,\n",
              "          0.0893],\n",
              "         [0.8692, 0.5110, 0.9354, 0.3875, 0.1877, 0.8678, 0.6743, 0.5333, 0.5718,\n",
              "          0.3638]], dtype=torch.float64, requires_grad=True),\n",
              " tensor([[0.4095, 0.9779],\n",
              "         [0.6838, 0.4340],\n",
              "         [0.2007, 0.3007]], dtype=torch.float64, requires_grad=True),\n",
              " tensor([-0.4641,  0.2157,  0.2898], dtype=torch.float64, requires_grad=True)]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batch Gradient Descent"
      ],
      "metadata": {
        "id": "wHGiEl4ZZG7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# parameters\n",
        "\n",
        "# random.rand() - Generates random number from a uniform distribution over [0,1)\n",
        "# 1000 rows and 10 columns\n",
        "X = np.random.rand(1000, 10)\n",
        "\n",
        "# random.randint() - Picks a random int between 0 and 1 of 1000 columns\n",
        "y = np.random.randint(0,2,1000)\n",
        "\n",
        "# W1, W2, W3 here are the weights in the 1st hidden layer, 2nd hidden layer and output layer\n",
        "\n",
        "# pick a random number between 0 and 1 and create 2*10 matrix\n",
        "W1 = torch.tensor(np.random.uniform(0,1,(2,10)),requires_grad=True)\n",
        "\n",
        "# pick a random number between 0 and 1 and create 3*2 matrix\n",
        "W2 = torch.tensor(np.random.uniform(0,1,(3,2)),requires_grad=True)\n",
        "\n",
        "# pick a random number betwee 0 and 1 and create 1*3 marix\n",
        "W3 = torch.tensor(np.random.uniform(0,1,3),requires_grad=True)\n",
        "\n",
        "# add these weights to a list W_list\n",
        "W_list = [W1, W2, W3]\n",
        "\n",
        "#No of epochs\n",
        "nepochs = 100\n",
        "\n",
        "#learning rate\n",
        "lr = 0.0001\n",
        "\n",
        "# define the loss function here\n",
        "loss_fn = nn.BCELoss()"
      ],
      "metadata": {
        "id": "Mlp0X0ABIyVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trainNN_batch(X, y, W_list, loss_fn, lr, nepochs):\n",
        "  n = len(y)\n",
        "  # Iterating every epoch\n",
        "  for epoch in range(nepochs):\n",
        "\n",
        "    # storing batch loss in a list for batch gradient\n",
        "    loss = 0\n",
        "\n",
        "    # Iterating through every output\n",
        "    for i in range(n):\n",
        "\n",
        "      #passing input\n",
        "      X_in = X[i, :]\n",
        "\n",
        "      #passing output\n",
        "      y_True = y[i]\n",
        "\n",
        "      #predicting new output\n",
        "      y_hat = forwardStep(X_in, W_list)\n",
        "\n",
        "      #checking the loss\n",
        "      loss += loss_fn(y_hat, torch.tensor(y_True, dtype=torch.double))\n",
        "\n",
        "    loss = loss/n\n",
        "\n",
        "    #Backpropogating the gradients wrt current loss\n",
        "    loss.backward()\n",
        "\n",
        "    #Flushing the buffer to the terminal\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    # Storing all the gradients in a list\n",
        "    dW_list = []\n",
        "\n",
        "    for j in range(len(W_list)):\n",
        "      dW_list.append(W_list[j].grad.data)\n",
        "\n",
        "    # Updating weights to reduce the loss      \n",
        "    W_list = updateParams(W_list, dW_list, lr)\n",
        "\n",
        "    # Setting the gradients to 0 since we are starting new epoch\n",
        "    for j in range(len(W_list)):\n",
        "      W_list[j].grad.data.zero_()\n",
        "\n",
        "    # printing the loss and epoch name\n",
        "    print(\"Loss after epoch = %d: %f\"%(epoch, loss))\n",
        "\n",
        "  # finally return the last weights\n",
        "  return W_list"
      ],
      "metadata": {
        "id": "ozX1RZiQY4qN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calling the function\n",
        "trainNN_batch(X,y,W_list, loss_fn, lr, nepochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdjNlcBTaSDG",
        "outputId": "039d1b98-752a-46c8-c0f1-e374a2ec0308"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss after epoch = 0: 0.692841\n",
            "Loss after epoch = 1: 0.692841\n",
            "Loss after epoch = 2: 0.692841\n",
            "Loss after epoch = 3: 0.692841\n",
            "Loss after epoch = 4: 0.692841\n",
            "Loss after epoch = 5: 0.692841\n",
            "Loss after epoch = 6: 0.692841\n",
            "Loss after epoch = 7: 0.692841\n",
            "Loss after epoch = 8: 0.692841\n",
            "Loss after epoch = 9: 0.692841\n",
            "Loss after epoch = 10: 0.692841\n",
            "Loss after epoch = 11: 0.692841\n",
            "Loss after epoch = 12: 0.692841\n",
            "Loss after epoch = 13: 0.692841\n",
            "Loss after epoch = 14: 0.692841\n",
            "Loss after epoch = 15: 0.692841\n",
            "Loss after epoch = 16: 0.692841\n",
            "Loss after epoch = 17: 0.692841\n",
            "Loss after epoch = 18: 0.692841\n",
            "Loss after epoch = 19: 0.692841\n",
            "Loss after epoch = 20: 0.692841\n",
            "Loss after epoch = 21: 0.692841\n",
            "Loss after epoch = 22: 0.692841\n",
            "Loss after epoch = 23: 0.692841\n",
            "Loss after epoch = 24: 0.692841\n",
            "Loss after epoch = 25: 0.692841\n",
            "Loss after epoch = 26: 0.692841\n",
            "Loss after epoch = 27: 0.692841\n",
            "Loss after epoch = 28: 0.692841\n",
            "Loss after epoch = 29: 0.692841\n",
            "Loss after epoch = 30: 0.692841\n",
            "Loss after epoch = 31: 0.692841\n",
            "Loss after epoch = 32: 0.692841\n",
            "Loss after epoch = 33: 0.692841\n",
            "Loss after epoch = 34: 0.692841\n",
            "Loss after epoch = 35: 0.692841\n",
            "Loss after epoch = 36: 0.692841\n",
            "Loss after epoch = 37: 0.692841\n",
            "Loss after epoch = 38: 0.692841\n",
            "Loss after epoch = 39: 0.692841\n",
            "Loss after epoch = 40: 0.692841\n",
            "Loss after epoch = 41: 0.692841\n",
            "Loss after epoch = 42: 0.692841\n",
            "Loss after epoch = 43: 0.692841\n",
            "Loss after epoch = 44: 0.692841\n",
            "Loss after epoch = 45: 0.692841\n",
            "Loss after epoch = 46: 0.692841\n",
            "Loss after epoch = 47: 0.692841\n",
            "Loss after epoch = 48: 0.692841\n",
            "Loss after epoch = 49: 0.692841\n",
            "Loss after epoch = 50: 0.692841\n",
            "Loss after epoch = 51: 0.692841\n",
            "Loss after epoch = 52: 0.692841\n",
            "Loss after epoch = 53: 0.692841\n",
            "Loss after epoch = 54: 0.692841\n",
            "Loss after epoch = 55: 0.692841\n",
            "Loss after epoch = 56: 0.692841\n",
            "Loss after epoch = 57: 0.692841\n",
            "Loss after epoch = 58: 0.692841\n",
            "Loss after epoch = 59: 0.692841\n",
            "Loss after epoch = 60: 0.692841\n",
            "Loss after epoch = 61: 0.692841\n",
            "Loss after epoch = 62: 0.692841\n",
            "Loss after epoch = 63: 0.692841\n",
            "Loss after epoch = 64: 0.692841\n",
            "Loss after epoch = 65: 0.692841\n",
            "Loss after epoch = 66: 0.692841\n",
            "Loss after epoch = 67: 0.692841\n",
            "Loss after epoch = 68: 0.692841\n",
            "Loss after epoch = 69: 0.692841\n",
            "Loss after epoch = 70: 0.692841\n",
            "Loss after epoch = 71: 0.692841\n",
            "Loss after epoch = 72: 0.692841\n",
            "Loss after epoch = 73: 0.692841\n",
            "Loss after epoch = 74: 0.692841\n",
            "Loss after epoch = 75: 0.692841\n",
            "Loss after epoch = 76: 0.692841\n",
            "Loss after epoch = 77: 0.692841\n",
            "Loss after epoch = 78: 0.692841\n",
            "Loss after epoch = 79: 0.692841\n",
            "Loss after epoch = 80: 0.692841\n",
            "Loss after epoch = 81: 0.692841\n",
            "Loss after epoch = 82: 0.692841\n",
            "Loss after epoch = 83: 0.692841\n",
            "Loss after epoch = 84: 0.692841\n",
            "Loss after epoch = 85: 0.692841\n",
            "Loss after epoch = 86: 0.692841\n",
            "Loss after epoch = 87: 0.692841\n",
            "Loss after epoch = 88: 0.692841\n",
            "Loss after epoch = 89: 0.692841\n",
            "Loss after epoch = 90: 0.692841\n",
            "Loss after epoch = 91: 0.692841\n",
            "Loss after epoch = 92: 0.692841\n",
            "Loss after epoch = 93: 0.692841\n",
            "Loss after epoch = 94: 0.692840\n",
            "Loss after epoch = 95: 0.692840\n",
            "Loss after epoch = 96: 0.692840\n",
            "Loss after epoch = 97: 0.692840\n",
            "Loss after epoch = 98: 0.692840\n",
            "Loss after epoch = 99: 0.692840\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([[0.2994, 0.0401, 0.9599, 0.4630, 0.2344, 0.6865, 0.0861, 0.9256, 0.9877,\n",
              "          0.0893],\n",
              "         [0.8692, 0.5110, 0.9354, 0.3875, 0.1877, 0.8678, 0.6743, 0.5333, 0.5718,\n",
              "          0.3638]], dtype=torch.float64, requires_grad=True),\n",
              " tensor([[0.4095, 0.9779],\n",
              "         [0.6838, 0.4340],\n",
              "         [0.2007, 0.3007]], dtype=torch.float64, requires_grad=True),\n",
              " tensor([-0.4641,  0.2156,  0.2898], dtype=torch.float64, requires_grad=True)]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mini Batch Gradient Descent non vectorized"
      ],
      "metadata": {
        "id": "FOFhl5O-a7PZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# parameters\n",
        "\n",
        "# random.rand() - Generates random number from a uniform distribution over [0,1)\n",
        "# 1000 rows and 10 columns\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(1000, 10)\n",
        "\n",
        "# random.randint() - Picks a random int between 0 and 1 of 1000 columns\n",
        "y = np.random.randint(0,2,1000)\n",
        "\n",
        "# W1, W2, W3 here are the weights in the 1st hidden layer, 2nd hidden layer and output layer\n",
        "\n",
        "# pick a random number between 0 and 1 and create 2*10 matrix\n",
        "W1 = torch.tensor(np.random.uniform(0,1,(2,10)),requires_grad=True)\n",
        "\n",
        "# pick a random number between 0 and 1 and create 3*2 matrix\n",
        "W2 = torch.tensor(np.random.uniform(0,1,(3,2)),requires_grad=True)\n",
        "\n",
        "# pick a random number betwee 0 and 1 and create 1*3 marix\n",
        "W3 = torch.tensor(np.random.uniform(0,1,3),requires_grad=True)\n",
        "\n",
        "# add these weights to a list W_list\n",
        "W_list = [W1, W2, W3]\n",
        "\n",
        "#No of epochs\n",
        "nepochs = 100\n",
        "\n",
        "#learning rate\n",
        "lr = 0.0001\n",
        "\n",
        "# define the loss function here\n",
        "loss_fn = nn.BCELoss()"
      ],
      "metadata": {
        "id": "IRKHAHUgItAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16"
      ],
      "metadata": {
        "id": "Q4ZXqBr1bQkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trainNN_batch_non_vectorized(X, y, W_list, loss_fn, lr, nepochs, batch_size):\n",
        "  n = len(y)\n",
        "\n",
        "  numBatches = n//batch_size\n",
        "\n",
        "  # Iterating every epoch\n",
        "  for epoch in range(nepochs):\n",
        "\n",
        "      for batch in range(numBatches):\n",
        "        X_batch = X[batch*batch_size : (batch+1)*batch_size, :] \n",
        "        y_batch = y[batch*batch_size : (batch+1)*batch_size]\n",
        "\n",
        "        # storing batch loss in a list for batch gradient\n",
        "        loss = 0\n",
        "\n",
        "        # Iterating through every output\n",
        "        for i in range(batch_size):\n",
        "\n",
        "          #passing input\n",
        "          X_in = X_batch[i, :]\n",
        "\n",
        "          #passing output\n",
        "          y_True = y_batch[i]\n",
        "\n",
        "          #predicting new output\n",
        "          y_hat = forwardStep(X_in, W_list)\n",
        "\n",
        "          #checking the loss\n",
        "          loss += loss_fn(y_hat, torch.tensor(y_True, dtype=torch.double))\n",
        "\n",
        "\n",
        "        loss = loss/batch_size\n",
        "\n",
        "        #Backpropogating the gradients wrt current loss\n",
        "        loss.backward()\n",
        "\n",
        "        #Flushing the buffer to the terminal\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        # Storing all the gradients in a list\n",
        "        dW_list = []\n",
        "\n",
        "        for j in range(len(W_list)):\n",
        "          dW_list.append(W_list[j].grad.data)\n",
        "\n",
        "        # Updating weights to reduce the loss      \n",
        "        W_list = updateParams(W_list, dW_list, lr)\n",
        "\n",
        "        # Setting the gradients to 0 since we are starting new epoch\n",
        "        for j in range(len(W_list)):\n",
        "          W_list[j].grad.data.zero_()\n",
        "    \n",
        "      # printing the loss and epoch name\n",
        "      print(\"Loss after epoch = %d: %f\"%(epoch, loss))\n",
        "\n",
        "  # finally return the last weights\n",
        "  return W_list"
      ],
      "metadata": {
        "id": "18fSr0GFa-OH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calling the function\n",
        "trainNN_batch_non_vectorized(X,y,W_list, loss_fn, lr, nepochs, batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sunAHOyNdIxF",
        "outputId": "b1dd2ab2-0443-4282-fe9a-7d4ad49bfd45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss after epoch = 0: 0.778973\n",
            "Loss after epoch = 1: 0.778440\n",
            "Loss after epoch = 2: 0.777909\n",
            "Loss after epoch = 3: 0.777381\n",
            "Loss after epoch = 4: 0.776856\n",
            "Loss after epoch = 5: 0.776332\n",
            "Loss after epoch = 6: 0.775812\n",
            "Loss after epoch = 7: 0.775294\n",
            "Loss after epoch = 8: 0.774778\n",
            "Loss after epoch = 9: 0.774264\n",
            "Loss after epoch = 10: 0.773754\n",
            "Loss after epoch = 11: 0.773245\n",
            "Loss after epoch = 12: 0.772739\n",
            "Loss after epoch = 13: 0.772236\n",
            "Loss after epoch = 14: 0.771734\n",
            "Loss after epoch = 15: 0.771235\n",
            "Loss after epoch = 16: 0.770739\n",
            "Loss after epoch = 17: 0.770245\n",
            "Loss after epoch = 18: 0.769753\n",
            "Loss after epoch = 19: 0.769264\n",
            "Loss after epoch = 20: 0.768777\n",
            "Loss after epoch = 21: 0.768292\n",
            "Loss after epoch = 22: 0.767810\n",
            "Loss after epoch = 23: 0.767330\n",
            "Loss after epoch = 24: 0.766852\n",
            "Loss after epoch = 25: 0.766377\n",
            "Loss after epoch = 26: 0.765904\n",
            "Loss after epoch = 27: 0.765433\n",
            "Loss after epoch = 28: 0.764965\n",
            "Loss after epoch = 29: 0.764499\n",
            "Loss after epoch = 30: 0.764035\n",
            "Loss after epoch = 31: 0.763573\n",
            "Loss after epoch = 32: 0.763113\n",
            "Loss after epoch = 33: 0.762656\n",
            "Loss after epoch = 34: 0.762201\n",
            "Loss after epoch = 35: 0.761749\n",
            "Loss after epoch = 36: 0.761298\n",
            "Loss after epoch = 37: 0.760850\n",
            "Loss after epoch = 38: 0.760404\n",
            "Loss after epoch = 39: 0.759960\n",
            "Loss after epoch = 40: 0.759518\n",
            "Loss after epoch = 41: 0.759078\n",
            "Loss after epoch = 42: 0.758641\n",
            "Loss after epoch = 43: 0.758206\n",
            "Loss after epoch = 44: 0.757773\n",
            "Loss after epoch = 45: 0.757342\n",
            "Loss after epoch = 46: 0.756913\n",
            "Loss after epoch = 47: 0.756486\n",
            "Loss after epoch = 48: 0.756062\n",
            "Loss after epoch = 49: 0.755639\n",
            "Loss after epoch = 50: 0.755219\n",
            "Loss after epoch = 51: 0.754800\n",
            "Loss after epoch = 52: 0.754384\n",
            "Loss after epoch = 53: 0.753970\n",
            "Loss after epoch = 54: 0.753558\n",
            "Loss after epoch = 55: 0.753148\n",
            "Loss after epoch = 56: 0.752740\n",
            "Loss after epoch = 57: 0.752334\n",
            "Loss after epoch = 58: 0.751930\n",
            "Loss after epoch = 59: 0.751528\n",
            "Loss after epoch = 60: 0.751129\n",
            "Loss after epoch = 61: 0.750731\n",
            "Loss after epoch = 62: 0.750335\n",
            "Loss after epoch = 63: 0.749941\n",
            "Loss after epoch = 64: 0.749549\n",
            "Loss after epoch = 65: 0.749159\n",
            "Loss after epoch = 66: 0.748771\n",
            "Loss after epoch = 67: 0.748386\n",
            "Loss after epoch = 68: 0.748002\n",
            "Loss after epoch = 69: 0.747620\n",
            "Loss after epoch = 70: 0.747239\n",
            "Loss after epoch = 71: 0.746861\n",
            "Loss after epoch = 72: 0.746485\n",
            "Loss after epoch = 73: 0.746111\n",
            "Loss after epoch = 74: 0.745738\n",
            "Loss after epoch = 75: 0.745368\n",
            "Loss after epoch = 76: 0.744999\n",
            "Loss after epoch = 77: 0.744633\n",
            "Loss after epoch = 78: 0.744268\n",
            "Loss after epoch = 79: 0.743905\n",
            "Loss after epoch = 80: 0.743544\n",
            "Loss after epoch = 81: 0.743184\n",
            "Loss after epoch = 82: 0.742827\n",
            "Loss after epoch = 83: 0.742471\n",
            "Loss after epoch = 84: 0.742118\n",
            "Loss after epoch = 85: 0.741766\n",
            "Loss after epoch = 86: 0.741416\n",
            "Loss after epoch = 87: 0.741067\n",
            "Loss after epoch = 88: 0.740721\n",
            "Loss after epoch = 89: 0.740376\n",
            "Loss after epoch = 90: 0.740033\n",
            "Loss after epoch = 91: 0.739692\n",
            "Loss after epoch = 92: 0.739353\n",
            "Loss after epoch = 93: 0.739015\n",
            "Loss after epoch = 94: 0.738679\n",
            "Loss after epoch = 95: 0.738345\n",
            "Loss after epoch = 96: 0.738013\n",
            "Loss after epoch = 97: 0.737682\n",
            "Loss after epoch = 98: 0.737354\n",
            "Loss after epoch = 99: 0.737026\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([[0.0365, 0.0255, 0.7383, 0.3173, 0.7956, 0.4310, 0.4523, 0.5121, 0.7123,\n",
              "          0.2727],\n",
              "         [0.1208, 0.5552, 0.3260, 0.4416, 0.6462, 0.4933, 0.6195, 0.5433, 0.7390,\n",
              "          0.7051]], dtype=torch.float64, requires_grad=True),\n",
              " tensor([[0.5220, 0.6662],\n",
              "         [0.8732, 0.4319],\n",
              "         [0.3930, 0.7437]], dtype=torch.float64, requires_grad=True),\n",
              " tensor([ 0.4793, -0.0344,  0.7898], dtype=torch.float64, requires_grad=True)]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mini Batch Gradient Descent vectorized [Faster]"
      ],
      "metadata": {
        "id": "jtD6y4vmIe3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# parameters\n",
        "\n",
        "# random.rand() - Generates random number from a uniform distribution over [0,1)\n",
        "# 1000 rows and 10 columns\n",
        "np.random.seed(42)\n",
        "\n",
        "X = np.random.rand(1000, 10)\n",
        "\n",
        "# random.randint() - Picks a random int between 0 and 1 of 1000 columns\n",
        "y = np.random.randint(0,2,1000)\n",
        "\n",
        "# W1, W2, W3 here are the weights in the 1st hidden layer, 2nd hidden layer and output layer\n",
        "\n",
        "# pick a random number between 0 and 1 and create 2*10 matrix\n",
        "W1 = torch.tensor(np.random.uniform(0,1,(2,10)),requires_grad=True)\n",
        "\n",
        "# pick a random number between 0 and 1 and create 3*2 matrix\n",
        "W2 = torch.tensor(np.random.uniform(0,1,(3,2)),requires_grad=True)\n",
        "\n",
        "# pick a random number betwee 0 and 1 and create 1*3 marix\n",
        "W3 = torch.tensor(np.random.uniform(0,1,3),requires_grad=True)\n",
        "\n",
        "# add these weights to a list W_list\n",
        "W_list = [W1, W2, W3]\n",
        "\n",
        "#No of epochs\n",
        "nepochs = 100\n",
        "\n",
        "#learning rate\n",
        "lr = 0.0001\n",
        "\n",
        "# define the loss function here\n",
        "loss_fn = nn.BCELoss()\n"
      ],
      "metadata": {
        "id": "9KxYXgfmISNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper functions\n",
        "\n",
        "# Activation function\n",
        "def activate(X):\n",
        "\n",
        "  #sigmoid function\n",
        "  return 1/(1+torch.exp(-X))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Moving forward\n",
        "def forwardStep_batch(X, W_list):\n",
        "\n",
        "  # converting numpy to torch data type\n",
        "  h = torch.from_numpy(X.T)\n",
        "\n",
        "  # Iterating through every weights of every layer\n",
        "  for W in W_list:\n",
        "    # (1*10) * (10*2) --> (1*2) * (2*3) --> (1*3) * (3*1) --> (1*1)\n",
        "    z = torch.matmul(W,h)\n",
        "\n",
        "    #applying non linear activation function\n",
        "    h = activate(z)\n",
        "\n",
        "  return h\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Update params - Based on gradients and learning rates weights are updated\n",
        "def updateParams(W_list, dW_list, lr):\n",
        "\n",
        "  # torch.no_grad() -> loop where requires_grad is set to false\n",
        "  with torch.no_grad():\n",
        "    for i in range(len(W_list)):\n",
        "      W_list[i] -= lr*dW_list[i]\n",
        "  return W_list"
      ],
      "metadata": {
        "id": "cKBSdOHHCill"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trainNN_batch_vectorized(X, y, W_list, loss_fn, lr, nepochs, batch_size):\n",
        "  n = len(y)\n",
        "\n",
        "  numBatches = n//batch_size\n",
        "\n",
        "  # Iterating every epoch\n",
        "  for epoch in range(nepochs):\n",
        "\n",
        "      for batch in range(numBatches):\n",
        "        X_batch = X[batch*batch_size : (batch+1)*batch_size, :] \n",
        "        y_batch = y[batch*batch_size : (batch+1)*batch_size]\n",
        "\n",
        "        # storing batch loss in a list for batch gradient\n",
        "        loss = 0\n",
        "\n",
        "        #predicting new output\n",
        "        y_hat = forwardStep_batch(X_batch, W_list)\n",
        "\n",
        "        #checking the loss\n",
        "        loss = torch.sum(loss_fn(y_hat, torch.tensor(y_batch, dtype=torch.double)))\n",
        "\n",
        "        # loss = loss/batch_size\n",
        "\n",
        "        #Backpropogating the gradients wrt current loss\n",
        "        loss.backward()\n",
        "\n",
        "        #Flushing the buffer to the terminal\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        # Storing all the gradients in a list\n",
        "        dW_list = []\n",
        "\n",
        "        for j in range(len(W_list)):\n",
        "          dW_list.append(W_list[j].grad.data)\n",
        "\n",
        "        # Updating weights to reduce the loss      \n",
        "        W_list = updateParams(W_list, dW_list, lr)\n",
        "\n",
        "        # Setting the gradients to 0 since we are starting new epoch\n",
        "        for j in range(len(W_list)):\n",
        "          W_list[j].grad.data.zero_()\n",
        "    \n",
        "      # printing the loss and epoch name\n",
        "      print(\"Loss after epoch = %d: %f\"%(epoch, loss))\n",
        "\n",
        "  # finally return the last weights\n",
        "  return W_list"
      ],
      "metadata": {
        "id": "DVSuoqRAC-A4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calling the function\n",
        "trainNN_batch_vectorized(X,y,W_list, loss_fn, lr, 100, 16)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ubVl9n-F9vc",
        "outputId": "134a66ed-6255-4db9-adf3-1f20c9247e51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss after epoch = 0: 0.778973\n",
            "Loss after epoch = 1: 0.778440\n",
            "Loss after epoch = 2: 0.777909\n",
            "Loss after epoch = 3: 0.777381\n",
            "Loss after epoch = 4: 0.776856\n",
            "Loss after epoch = 5: 0.776332\n",
            "Loss after epoch = 6: 0.775812\n",
            "Loss after epoch = 7: 0.775294\n",
            "Loss after epoch = 8: 0.774778\n",
            "Loss after epoch = 9: 0.774264\n",
            "Loss after epoch = 10: 0.773754\n",
            "Loss after epoch = 11: 0.773245\n",
            "Loss after epoch = 12: 0.772739\n",
            "Loss after epoch = 13: 0.772236\n",
            "Loss after epoch = 14: 0.771734\n",
            "Loss after epoch = 15: 0.771235\n",
            "Loss after epoch = 16: 0.770739\n",
            "Loss after epoch = 17: 0.770245\n",
            "Loss after epoch = 18: 0.769753\n",
            "Loss after epoch = 19: 0.769264\n",
            "Loss after epoch = 20: 0.768777\n",
            "Loss after epoch = 21: 0.768292\n",
            "Loss after epoch = 22: 0.767810\n",
            "Loss after epoch = 23: 0.767330\n",
            "Loss after epoch = 24: 0.766852\n",
            "Loss after epoch = 25: 0.766377\n",
            "Loss after epoch = 26: 0.765904\n",
            "Loss after epoch = 27: 0.765433\n",
            "Loss after epoch = 28: 0.764965\n",
            "Loss after epoch = 29: 0.764499\n",
            "Loss after epoch = 30: 0.764035\n",
            "Loss after epoch = 31: 0.763573\n",
            "Loss after epoch = 32: 0.763113\n",
            "Loss after epoch = 33: 0.762656\n",
            "Loss after epoch = 34: 0.762201\n",
            "Loss after epoch = 35: 0.761749\n",
            "Loss after epoch = 36: 0.761298\n",
            "Loss after epoch = 37: 0.760850\n",
            "Loss after epoch = 38: 0.760404\n",
            "Loss after epoch = 39: 0.759960\n",
            "Loss after epoch = 40: 0.759518\n",
            "Loss after epoch = 41: 0.759078\n",
            "Loss after epoch = 42: 0.758641\n",
            "Loss after epoch = 43: 0.758206\n",
            "Loss after epoch = 44: 0.757773\n",
            "Loss after epoch = 45: 0.757342\n",
            "Loss after epoch = 46: 0.756913\n",
            "Loss after epoch = 47: 0.756486\n",
            "Loss after epoch = 48: 0.756062\n",
            "Loss after epoch = 49: 0.755639\n",
            "Loss after epoch = 50: 0.755219\n",
            "Loss after epoch = 51: 0.754800\n",
            "Loss after epoch = 52: 0.754384\n",
            "Loss after epoch = 53: 0.753970\n",
            "Loss after epoch = 54: 0.753558\n",
            "Loss after epoch = 55: 0.753148\n",
            "Loss after epoch = 56: 0.752740\n",
            "Loss after epoch = 57: 0.752334\n",
            "Loss after epoch = 58: 0.751930\n",
            "Loss after epoch = 59: 0.751528\n",
            "Loss after epoch = 60: 0.751129\n",
            "Loss after epoch = 61: 0.750731\n",
            "Loss after epoch = 62: 0.750335\n",
            "Loss after epoch = 63: 0.749941\n",
            "Loss after epoch = 64: 0.749549\n",
            "Loss after epoch = 65: 0.749159\n",
            "Loss after epoch = 66: 0.748771\n",
            "Loss after epoch = 67: 0.748386\n",
            "Loss after epoch = 68: 0.748002\n",
            "Loss after epoch = 69: 0.747620\n",
            "Loss after epoch = 70: 0.747239\n",
            "Loss after epoch = 71: 0.746861\n",
            "Loss after epoch = 72: 0.746485\n",
            "Loss after epoch = 73: 0.746111\n",
            "Loss after epoch = 74: 0.745738\n",
            "Loss after epoch = 75: 0.745368\n",
            "Loss after epoch = 76: 0.744999\n",
            "Loss after epoch = 77: 0.744633\n",
            "Loss after epoch = 78: 0.744268\n",
            "Loss after epoch = 79: 0.743905\n",
            "Loss after epoch = 80: 0.743544\n",
            "Loss after epoch = 81: 0.743184\n",
            "Loss after epoch = 82: 0.742827\n",
            "Loss after epoch = 83: 0.742471\n",
            "Loss after epoch = 84: 0.742118\n",
            "Loss after epoch = 85: 0.741766\n",
            "Loss after epoch = 86: 0.741416\n",
            "Loss after epoch = 87: 0.741067\n",
            "Loss after epoch = 88: 0.740721\n",
            "Loss after epoch = 89: 0.740376\n",
            "Loss after epoch = 90: 0.740033\n",
            "Loss after epoch = 91: 0.739692\n",
            "Loss after epoch = 92: 0.739353\n",
            "Loss after epoch = 93: 0.739015\n",
            "Loss after epoch = 94: 0.738679\n",
            "Loss after epoch = 95: 0.738345\n",
            "Loss after epoch = 96: 0.738013\n",
            "Loss after epoch = 97: 0.737682\n",
            "Loss after epoch = 98: 0.737354\n",
            "Loss after epoch = 99: 0.737026\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([[0.0365, 0.0255, 0.7383, 0.3173, 0.7956, 0.4310, 0.4523, 0.5121, 0.7123,\n",
              "          0.2727],\n",
              "         [0.1208, 0.5552, 0.3260, 0.4416, 0.6462, 0.4933, 0.6195, 0.5433, 0.7390,\n",
              "          0.7051]], dtype=torch.float64, requires_grad=True),\n",
              " tensor([[0.5220, 0.6662],\n",
              "         [0.8732, 0.4319],\n",
              "         [0.3930, 0.7437]], dtype=torch.float64, requires_grad=True),\n",
              " tensor([ 0.4793, -0.0344,  0.7898], dtype=torch.float64, requires_grad=True)]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qfqUvjhuJHh9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}