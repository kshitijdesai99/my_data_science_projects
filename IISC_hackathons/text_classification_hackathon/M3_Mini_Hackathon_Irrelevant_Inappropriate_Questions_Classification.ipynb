{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNgLag1Euy3H"
      },
      "source": [
        "# Advanced Programme in Deep Learning (Foundations and Applications)\n",
        "## A Program by IISc and TalentSprint\n",
        "\n",
        "### Mini Project Notebook: Irrelevant/inappropriate Questions Classification using Deep Neural Networks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maritime-miami"
      },
      "source": [
        "## Learning Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nljJR6CwfZN_"
      },
      "source": [
        "At the end of the mini-hackathon, you will be able to :\n",
        "\n",
        "* perform data preprocessing/preprocess the text\n",
        "* represent the text/words using the pretrained word embeddings - Word2Vec/Glove\n",
        "* build the deep neural networks to classify the questions as Irrelevant/inappropriate or not\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FL0Ve1abn6YJ"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "The challenge in this competition is to predict whether a question asked on a well known public forum/platform is irrelevant/inappropriate or not.\n",
        "\n",
        "A irrelevant/inappropriate question is defined as a question intended to make a statement and not with a purpose of looking for helpful/meaningful answers. The following are some of the characteristics that can signify that a question is irrelevant/inappropriate:\n",
        "\n",
        "* Based on false information, or contains absurd assumptions\n",
        "* Does not have a non-neutral tone\n",
        "* Has an exaggerated tone to underscore a point about a group of people\n",
        "* Is rhetorical and meant to imply a statement about a group of people\n",
        "* Is disparaging or inflammatory against an individual or a group of people\n",
        "* Uses sexual content (such as incest, pedophilia), and not to seek genuine answers\n",
        "* Suggests a discriminatory idea against a protected class of people, or seeks confirmation of a stereotype\n",
        "* Based on an unrealistic premise about a group of people\n",
        "* Is not grounded in reality\n",
        "\n",
        "The training dataset includes the questions 1044897 that was asked, and whether it was identified as irrelevant/inappropriate (target = 1) or as relevant/appropriate (target = 0). The test dataset consists of approximately 261000 questions.\n",
        "\n",
        "The training data might be imbalanced or noisy. They are not guaranteed to be perfect. Please take the necessary actions/steps while building the model.\n",
        "\n",
        "\n",
        "## Description\n",
        "\n",
        "This dataset has the following information:\n",
        "\n",
        "1. **qid** - unique question identifier\n",
        "2. **question_text** - the text of the question asked in the well known public forum/platform\n",
        "3. **target** - a question labeled \"irrelevant/inappropriate\" has a value of 1, otherwise 0\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ih-oasWmdZul"
      },
      "source": [
        "## Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfWGmjNHdZul"
      },
      "source": [
        "To perform classification of approximately 261000 questions asked on a well known public form using Deep Neural Networks such as RNN/CNN/BERT/LSTM as 'irrelevant/inappropriate' questions or 'relevant/appropriate' questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BQEA97zTlTa"
      },
      "source": [
        "## Grading = 10 Marks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdYmy-tJgURN"
      },
      "source": [
        "Here is a handy link to Kaggle's competition documentation (https://www.kaggle.com/docs/competitions), which includes, among other things, instructions on submitting predictions (https://www.kaggle.com/docs/competitions#making-a-submission)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8OapRtHgLnU"
      },
      "source": [
        "## Instructions for downloading train and test dataset from Kaggle API are as follows:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DO2jS73oLnCR"
      },
      "source": [
        "### 1. Create an API key in Kaggle.\n",
        "\n",
        "To do this, go to the competition site on Kaggle at (https://www.kaggle.com/t/bde6f23028154933a99e4b4ca8a3dff2) and click on user then click on your profile as shown below. Click Account.\n",
        "\n",
        "![alt text](https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/Capture-NLP.PNG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkzGffHdbwX2"
      },
      "source": [
        "### 2. Next, scroll down to the API access section and click on **Create New Token** to download an API key (kaggle.json).\n",
        "\n",
        "![alt text](https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/Capture-NLP_1.PNG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtETuXx8b-OC"
      },
      "source": [
        "### 3. Upload your kaggle.json file using the following snippet in a code cell:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1pfXBDxWl0Y"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCV_T6MMW4eX"
      },
      "source": [
        "#If successfully uploaded in the above step, the 'ls' command here should display the kaggle.json file.\n",
        "%ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbukdzJ6cE32"
      },
      "source": [
        "### 4. Install the Kaggle API using the following command\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMj1n1MJcqzN"
      },
      "source": [
        "!pip install -U -q kaggle==1.5.8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Vpy9P1nchhd"
      },
      "source": [
        "### 5. Move the kaggle.json file into ~/.kaggle, which is where the API client expects your token to be located:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQbPsDOLZ0b4"
      },
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BenAWlpI73sm"
      },
      "source": [
        "# Execute the following command to verify whether the kaggle.json is stored in the appropriate location: ~/.kaggle/kaggle.json\n",
        "!ls ~/.kaggle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vm2jGsCradOS"
      },
      "source": [
        "!chmod 600 /root/.kaggle/kaggle.json # run this command to ensure your Kaggle API token is secure on colab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32unPZKzdI72"
      },
      "source": [
        "### 6. Now download the Test Data from Kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppuy5gRKHFwv"
      },
      "source": [
        "**NOTE: If you get a '404 - Not Found' error after running the cell below, it is most likely that the user (whose kaggle.json is uploaded above) has not 'accepted' the rules of the competition and therefore has 'not joined' the competition.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41-ETZCE_A1j"
      },
      "source": [
        "If you encounter **401-unauthorised** download latest **kaggle.json** by repeating steps 1 & 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TY40TmgfHq0s"
      },
      "source": [
        "#If you get a forbidden link, you have most likely not joined the competition.\n",
        "!kaggle competitions download -c toxic-questions-classification"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/toxic-questions-classification.zip"
      ],
      "metadata": {
        "id": "mvKRiFNglvpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## YOUR CODING STARTS FROM HERE"
      ],
      "metadata": {
        "id": "QeKon2vruI_c"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abstract-stocks"
      },
      "source": [
        "## Import required packages"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required packages"
      ],
      "metadata": {
        "id": "O5RcxwQUku6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from gensim.utils import simple_preprocess\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from gensim.parsing.preprocessing import preprocess_string, strip_tags, strip_punctuation, strip_multiple_whitespaces, strip_numeric, remove_stopwords, strip_short\n",
        "nltk.download('wordnet')\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "from IPython.display import HTML,display\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import itertools\n",
        "\n",
        "\n",
        "from keras.layers import Input, Embedding, Dense, Bidirectional, Dropout, GRU\n",
        "from keras.models import Sequential   # the model\n",
        "from keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "GmL6byt1Nxzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53g0zVbjRV7K"
      },
      "source": [
        "##   **Stage 1**:  Data Loading and Perform Exploratory Data Analysis (1 Points)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv(\"/content/train_dataset.csv\")\n",
        "train"
      ],
      "metadata": {
        "id": "Xff3evqbOUrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = pd.read_csv(\"/content/test_dataset.csv\")\n",
        "test"
      ],
      "metadata": {
        "id": "mtc4i9CnOf7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combo = pd.concat([train, test], axis=0)"
      ],
      "metadata": {
        "id": "L350TDdROZfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#let's see how data is looklike\n",
        "random_index=random.randint(0,train.shape[0]-3)\n",
        "for row in train[['question_text','target']][random_index:random_index+3].itertuples():\n",
        "    _,text,label=row\n",
        "    class_name=0\n",
        "    if label==1:\n",
        "        class_name=\"1\"\n",
        "    display(HTML(f\"<h5><b style='color:red'>question_text: </b>{text}</h5>\"))\n",
        "    display(HTML(f\"<h5><b style='color:red'>target: </b>{class_name}<br><hr></h5>\"))\n",
        "#data contain so much garbage needs to be cleaned"
      ],
      "metadata": {
        "id": "A2CCIyAQO9bB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "colors=['#AB47BC','#6495ED']\n",
        "plt.pie(train['target'].value_counts(),labels=['0','1'],autopct='%.2f%%',explode=[0.01,0.01],colors=colors);\n",
        "plt.title('Distribution of target')\n",
        "plt.ylabel('target');"
      ],
      "metadata": {
        "id": "QdjM2KpZPVFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "5TgrK6vhP1CM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positivedata = train[train['target']== 0]\n",
        "positivedata =positivedata['question_text']\n",
        "negdata = train[train['target']== 1]\n",
        "negdata= negdata['question_text']\n",
        "\n",
        "def wordcloud_draw(data, color, s):\n",
        "    words = ' '.join(data)\n",
        "    cleaned_word = \" \".join([word for word in words.split() if(word not in ['would','get','like','people','think','take'])])\n",
        "    wordcloud = WordCloud(stopwords=stopwords.words('english'),background_color=color,width=2500,height=2000).generate(cleaned_word)\n",
        "    plt.imshow(wordcloud)\n",
        "    plt.title(s)\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.figure(figsize=[20,10])\n",
        "plt.subplot(1,2,1)\n",
        "wordcloud_draw(positivedata,'white','Most-common Positive words')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "wordcloud_draw(negdata, 'white','Most-common Negative words')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "S2e9OaDlPdS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['text_word_count']=train['question_text'].apply(lambda x:len(x.split()))\n",
        "\n",
        "numerical_feature_cols=['text_word_count']"
      ],
      "metadata": {
        "id": "-cPMGOY_QP-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20,3))\n",
        "for i,col in enumerate(numerical_feature_cols):\n",
        "    plt.subplot(1,3,i+1)\n",
        "    sns.histplot(data=train,x=col,hue='target',bins=50)\n",
        "    plt.title(f\"Distribution of Various word counts with respect to target\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JJywV93rPk1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oLyIb19KcdL"
      },
      "source": [
        "##   **Stage 2**: Data Pre-Processing  (1 Points)\n",
        "\n",
        "####  Clean and Transform the data into a specified format\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "combo['question_text'] = combo['question_text'].apply(lambda x:simple_preprocess(x))"
      ],
      "metadata": {
        "id": "GpmA5ZPJObUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combo['question_text'] = combo['question_text'].apply(lambda tokens: ' '.join(tokens))"
      ],
      "metadata": {
        "id": "ZygqgmfSOnws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_preprocess(s):\n",
        "    # Initialize WordNetLemmatizer\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    # Define your own preprocessing steps\n",
        "    filters = [lambda x: x.lower(),  # Convert to lowercase\n",
        "               strip_tags,  # Remove HTML tags\n",
        "               strip_punctuation,  # Remove punctuation\n",
        "               strip_multiple_whitespaces,  # Remove multiple whitespaces\n",
        "               strip_numeric,  # Remove numbers\n",
        "               remove_stopwords,  # Remove stopwords\n",
        "               strip_short,  # Remove short words\n",
        "               lambda x: lemmatizer.lemmatize(x)]  # Lemmatization step\n",
        "\n",
        "    return preprocess_string(s, filters)"
      ],
      "metadata": {
        "id": "91y9Cx7bOpYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combo['question_text'] = combo['question_text'].apply(lambda x:custom_preprocess(x))"
      ],
      "metadata": {
        "id": "dY6r7tXUQEw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combo['question_text'] = combo['question_text'].apply(lambda tokens: ' '.join(tokens))"
      ],
      "metadata": {
        "id": "9HL0fdBCQY8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_text_length(text):\n",
        "    tokens = simple_preprocess(text)\n",
        "    return len(tokens)"
      ],
      "metadata": {
        "id": "t2kq2YOrRAe0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combo['text_length'] = combo['question_text'].apply(get_text_length)"
      ],
      "metadata": {
        "id": "x7sUeuAUQ5DG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(10,6))\n",
        "# Add axes to the figure. Create the first main window\n",
        "ax1 = fig.add_axes([0, 0, 0.95, 0.95])\n",
        "ax1.hist(np.array(combo.text_length), bins=50, label='length', alpha=0.6, color='blue');"
      ],
      "metadata": {
        "id": "JOkpkQ6MQczs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combo['text_length'].quantile(0.995)"
      ],
      "metadata": {
        "id": "FWWj58fvQdVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combo['question_text'] = combo['question_text'].apply(lambda x:simple_preprocess(x, max_len=17))"
      ],
      "metadata": {
        "id": "io3Jy5hhQdpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combo['question_text'] = combo['question_text'].apply(lambda tokens: ' '.join(tokens))"
      ],
      "metadata": {
        "id": "wJ2fyN3xQk9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_tokens = list(itertools.chain.from_iterable([i.split() for i in combo.question_text]))\n",
        "token_counts = Counter(all_tokens)\n",
        "sorted_token_counts = dict(sorted(token_counts.items(), key=lambda item: item[1], reverse=True))\n",
        "len(sorted_token_counts)\n",
        "count = 0\n",
        "for key, value in sorted_token_counts.items():\n",
        "    if value > 11:\n",
        "        count+=1\n",
        "print(count)\n",
        "sorted_token_counts.get('approach', 0)"
      ],
      "metadata": {
        "id": "I_ecaEkBRMpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vocab_limiter(x):\n",
        "    x = x.split()\n",
        "    y = []\n",
        "    for i in x:\n",
        "        if sorted_token_counts.get(i, 0) > 11:\n",
        "            y.append(i)\n",
        "    return ' '.join(y)"
      ],
      "metadata": {
        "id": "tT8wYw50RM-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combo['question_text'] = combo['question_text'].apply(vocab_limiter)"
      ],
      "metadata": {
        "id": "Q6Z8nImjRwEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jZg7yL2TtTM"
      },
      "source": [
        "##   **Stage 3**: Build the Word Embeddings using pretrained Word2vec/Glove (Text Representation) (1 Point)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "Fc3P_XuOSIIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_SENT_LEN = 17   # Number of words to consider from each review\n",
        "MAX_VOCAB_SIZE = 29581 # Max vocabulary size\n",
        "BATCH_SIZE = 32\n",
        "N_EPOCHS = 5"
      ],
      "metadata": {
        "id": "8tCkBVTLRMPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts(combo['question_text'])\n",
        "print(\"Number of words in vocabulary:\", len(tokenizer.word_index))\n",
        "X = tokenizer.texts_to_sequences(combo['question_text'])\n",
        "X = tf.keras.preprocessing.sequence.pad_sequences(X, maxlen=MAX_SENT_LEN, padding='post', truncating='post')\n",
        "y = combo['target']\n",
        "testX = X[1044897:]\n",
        "X = X[:1044897]\n",
        "y = y[:1044897]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42, test_size = 0.1)\n",
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape\n",
        "embeddings_index = {}"
      ],
      "metadata": {
        "id": "JcsOhtmpR4yf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://nlp.stanford.edu/data/glove.42B.300d.zip"
      ],
      "metadata": {
        "id": "V3jZVgLwSrk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip glove*.zip"
      ],
      "metadata": {
        "id": "MdjtL640S5Kq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = open('/content/glove.42B.300d.txt')"
      ],
      "metadata": {
        "id": "vM4oPILcTLbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for line in tqdm(f):\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "metadata": {
        "id": "zN0EEjX8TL0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding 1 because of reversed 0 index\n",
        "words_not_found = []\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Loaded %s word vectors.' % len(embeddings_index))"
      ],
      "metadata": {
        "id": "z755Dg_9TaMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 300\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "len(tokenizer.word_index.items())"
      ],
      "metadata": {
        "id": "ZW8B3_RFTcCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word, i in tokenizer.word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "    else:\n",
        "        words_not_found.append(word)"
      ],
      "metadata": {
        "id": "ci-K5cq_TiyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(words_not_found)"
      ],
      "metadata": {
        "id": "Zg4dcJsoTlOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(tokenizer.word_index))"
      ],
      "metadata": {
        "id": "QVyDtsOITmhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6jfm3YFUL7i"
      },
      "source": [
        "##   **Stage 4**: Build and Train the Deep networks model using Pytorch/Keras (5 Points)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3)"
      ],
      "metadata": {
        "id": "7tbALVb-TIBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights_assigned={0:1,1:20}"
      ],
      "metadata": {
        "id": "AC25wofnS4bG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a sequential model by stacking neural net units\n",
        "model = Sequential()\n",
        "embedding_layer = Embedding(vocab_size,\n",
        "                            embedding_dim,\n",
        "                            weights = [embedding_matrix],\n",
        "                            trainable=False)\n",
        "model.add(embedding_layer)\n",
        "model.add(Bidirectional(GRU(128, return_sequences=True, dropout=0.50, name='first_gru_layer')))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Bidirectional(GRU(64, name='second_gru_layer')))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(32, activation='tanh'))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(1, activation='sigmoid', name='output_layer'))"
      ],
      "metadata": {
        "id": "Mfv05pNxT7SL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Summary of the built model...')\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "u0JHNQLcT_oO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['AUC'])"
      ],
      "metadata": {
        "id": "S7KBGzQoUBI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          epochs=2,\n",
        "          validation_data=(X_test, y_test),\n",
        "          callbacks=[early_stopping],\n",
        "          class_weight=weights_assigned)"
      ],
      "metadata": {
        "id": "LoO_LH4GUCit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-O0Jx99UhmI"
      },
      "source": [
        "##   **Stage 5**: Evaluate the Model and get model predictions on the test dataset (2 Points)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Testing...')\n",
        "model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "id": "Ki8U25qHUIOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predz = model.predict(X_test)"
      ],
      "metadata": {
        "id": "xSqdE5OpUK3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(10,6))\n",
        "# Add axes to the figure. Create the first main window\n",
        "ax1 = fig.add_axes([0, 0, 0.95, 0.95])\n",
        "ax1.hist(np.array(predz), bins=50, label='length', alpha=0.6, color='blue');"
      ],
      "metadata": {
        "id": "B4jc34rHUMF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_predz1 = [1 if i >= 0.92 else 0 for i in predz]"
      ],
      "metadata": {
        "id": "5hdoX1_wUPlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test.to_numpy(), y_predz1, target_names=['0','1']))"
      ],
      "metadata": {
        "id": "mhWzmo2WUSdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model.predict(testX)\n"
      ],
      "metadata": {
        "id": "gFBuud6vUS2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test['target'] = preds"
      ],
      "metadata": {
        "id": "RMw8xuCmUWYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds1 = [1 if i >= 0.92 else 0 for i in preds]"
      ],
      "metadata": {
        "id": "13vkOslvUYyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test['target'] = preds1"
      ],
      "metadata": {
        "id": "PWu8W9yOUbmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_submit = test[['qid','target']]"
      ],
      "metadata": {
        "id": "27Z_2V3TUdL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_submit['qid'] = test['qid']"
      ],
      "metadata": {
        "id": "FP7KeykyUe7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_submit.to_csv('upload1.csv',index=False)"
      ],
      "metadata": {
        "id": "tcGICSOSUgBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_submit"
      ],
      "metadata": {
        "id": "MIA4yJNGUhnp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}